{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# New Student Smart Replay Small Maze Experiments in Curriculum Learning\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "26/07/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeGNMcHDj1vL"
   },
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mT38a_chiRWz"
   },
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False):\n",
    "    # 10: Key\n",
    "    # -2: Door\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    \n",
    "    self._wall = -1\n",
    "    self._door = -2\n",
    "    self._key = 10\n",
    "    \n",
    "#     self._layout = np.array([\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       ])\n",
    "    \n",
    "    \n",
    "    \n",
    "#      ROOMS\n",
    "\n",
    "    self._layout = np.array([\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -2, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0,  0, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      ])\n",
    "  \n",
    "  #     self._layout = np.array([\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1,  0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1,  0,  0,  0, -1, -1,  0,  0,  0,  0, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       ])\n",
    "    \n",
    "    #self._goal = value\n",
    "    \n",
    "    # row, col format\n",
    "    # row, col format\n",
    "    self._start_state = (11, 8)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "    \n",
    "    self._goals = set()\n",
    "    self._goal_loc = []\n",
    "    self._distances_to_goal = []\n",
    "    \n",
    "    self._dist = {}\n",
    "    for r,c in zip(np.where(self._layout != self._wall)[0],np.where(self._layout != self._wall)[1]):\n",
    "      self._dist[(r, c)] = self.dijkstra(r, c)\n",
    "      \n",
    "    for e in goal_loc:\n",
    "      self._layout[e[0],e[1]] = e[2]\n",
    "      self._goal_loc.append((e[0],e[1]))\n",
    "      self._goals.add(e[2])\n",
    "      self._distances_to_goal.append(self.minDistanceTwoPoints(self._start_state[0], self._start_state[1], e[0],e[1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    #self.distanceToGoal()\n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "  \n",
    "  def distanceLeft(self, teacher_action):\n",
    "    goal_y = self._goal_loc[teacher_action][0]\n",
    "    goal_x = self._goal_loc[teacher_action][1]\n",
    "    return self.minDistanceTwoPoints(self._state[0], self._state[1], goal_y, goal_x)\n",
    "  \n",
    "  def distanceToGoal(self, teacher_action):\n",
    "    return np.prod(self._layout.shape)\n",
    "  \n",
    "  def dijkstra(self, cy, cx):\n",
    "    dist = {}\n",
    "    prev = {}\n",
    "    Q = set()\n",
    "    \n",
    "    for r,c in zip(np.where(self._layout != self._wall)[0],np.where(self._layout != self._wall)[1]):\n",
    "      dist[(r,c)] = np.inf\n",
    "      prev[(r,c)] = None\n",
    "      Q.add((r,c))\n",
    "    \n",
    "    dist[(cy,cx)] = 0.\n",
    "    \n",
    "    while len(Q) != 0:\n",
    "      ud = np.inf\n",
    "      u = None\n",
    "      for e in Q:\n",
    "        if dist[e] < ud:\n",
    "          ud = dist[e]\n",
    "          u = e\n",
    "      Q.remove(u)\n",
    "      \n",
    "      neighbors_u = []\n",
    "      if u[0]+1 < self._layout.shape[0] and self._layout[u[0]+1, u[1]] != self._wall and (u[0]+1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]+1, u[1]))\n",
    "      \n",
    "      if u[0]-1 > -1 and self._layout[u[0]-1, u[1]] != self._wall and (u[0]-1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]-1, u[1]))\n",
    "      \n",
    "      if u[1]+1 < self._layout.shape[1] and self._layout[u[0], u[1]+1] != self._wall and (u[0], u[1]+1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]+1))\n",
    "      \n",
    "      if u[1]-1 > -1 and self._layout[u[0], u[1]-1] != self._wall and (u[0], u[1]-1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]-1))\n",
    "        \n",
    "      for neighbor in neighbors_u:\n",
    "        alt = dist[u] + 1.\n",
    "        if alt < dist[neighbor]:\n",
    "          dist[neighbor] = alt\n",
    "          prev[neighbor] = u\n",
    "    \n",
    "    return dist\n",
    "  \n",
    "  def minDistanceTwoPoints(self, cy, cx, dy, dx):\n",
    "    return self._dist[(cy,cx)][(dy,dx)]\n",
    "    \n",
    "  def handleDoor(self):\n",
    "    pass\n",
    "  \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self, title=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if title != None:\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      plt.title(\"The Grid\")\n",
    "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    \n",
    "    for e in self._goals:\n",
    "      if e == self._key:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{K}$\", ha='center', va='center')\n",
    "      elif e > 0:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    y = np.where(self._layout==self._door)[0]\n",
    "    x = np.where(self._layout==self._door)[1]\n",
    "    for i in range(y.shape[0]): \n",
    "      plt.text(x[i], y[i], r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    \n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
    "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
    "      return location\n",
    "\n",
    "  def step(self, action, teacher_action, agent_inventory):\n",
    "    item = None\n",
    "    goal_y = self._goal_loc[teacher_action][0]\n",
    "    goal_x = self._goal_loc[teacher_action][1]\n",
    "    y, x = self._state\n",
    "        \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    # DO I REALLY WANT THIS\n",
    "    elif self._layout[new_y, new_x] == self._key and (new_y, new_x) == (goal_y, goal_x): # a key\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      item = 'KEY'\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "      print('PICKED UP KEY')\n",
    "    elif self._layout[new_y, new_x] == self._door: # a door\n",
    "      reward = 5\n",
    "      if 'KEY' not in agent_inventory:\n",
    "        reward = self._layout[new_y, new_x]\n",
    "        new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] > 0 and (new_y, new_x) == (goal_y, goal_x): # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      minDistanceStart = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1], goal_y, goal_x)\n",
    "      distToGoal = self.minDistanceTwoPoints(new_y, new_x, goal_y, goal_x)\n",
    "      distToGoal = float(distToGoal)\n",
    "      minDistanceStart = float(minDistanceStart)\n",
    "      # works well with door -2?\n",
    "#       reward = self._layout[goal_y, goal_x]*(np.exp(-distToGoal/minDistanceStart) - 1.)\n",
    "      reward = self._layout[goal_y, goal_x]*(np.exp(-distToGoal) - 1.)\n",
    "      if self._layout[goal_y, goal_x] == 100. and 'KEY' not in agent_inventory:\n",
    "        reward = 0.\n",
    "#       elif self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' in agent_inventory:\n",
    "#         print agent_inventory\n",
    "        \n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs(), item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxFTKIfFj1vP"
   },
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2723
    },
    "colab_type": "code",
    "id": "Hd1tV95Gj1vQ",
    "outputId": "b08857a7-5596-4e30-c70f-5210197ba216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: 0 (8, 2)\n",
      "Task: 1 (2, 2)\n",
      "Task: 2 (2, 14)\n",
      "Task: 3 (8, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(8,2,10),(2,2,1),(2,14,1),(8,14,100)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "\n",
    "for idx, goal_loc in enumerate(tasks[0]._goal_loc):\n",
    "  print('Task: '+str(idx), goal_loc)\n",
    "\n",
    "for task in tasks:\n",
    "    task.plot_grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwHC3_5hj1vb"
   },
   "outputs": [],
   "source": [
    "class NEURAL_TEACHER_STUDENT(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, number_of_features_teacher,\n",
    "                number_of_features_student,\n",
    "                number_of_features_substitute,\n",
    "                number_of_hidden_teacher,\n",
    "                number_of_hidden_student,\n",
    "                number_of_hidden_substitute,\n",
    "                number_of_actions_teacher,\n",
    "                number_of_actions_student,\n",
    "                number_of_actions_substitute,\n",
    "                initial_state_teacher,\n",
    "                initial_state_student, \n",
    "                initial_state_substitute,\n",
    "                rl_alg_teacher='DQN',\n",
    "                rl_alg_student='DQN', \n",
    "                rl_alg_substitute='DQN',\n",
    "                num_offline_updates_teacher=20, \n",
    "                num_offline_updates_student=25,\n",
    "                num_offline_updates_substitute=25,\n",
    "                step_size_teacher=0.01,\n",
    "                step_size_student=0.01,\n",
    "                step_size_substitute=0.01): \n",
    "    # HMMM?\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    self._prev_action_student = 0\n",
    "    self._step_student = step_size_student\n",
    "    self._num_features_student = number_of_features_student\n",
    "    self._num_action_student = number_of_actions_student\n",
    "    self._num_hidden_student = number_of_hidden_student\n",
    "    self._initial_state_student = initial_state_student\n",
    "    self._s_student = initial_state_student\n",
    "    self._s_student = np.reshape(self._s_student, (1,-1))\n",
    "    self._times_trained_student = 0\n",
    "    self._inventory_student = set()\n",
    "    self._replayBuffer_student = []\n",
    "    self._num_offline_updates_student = num_offline_updates_student\n",
    "    self._rl_alg_student = rl_alg_student\n",
    "\n",
    "\n",
    "    self._prev_action_substitute = 0\n",
    "    self._step_substitute = step_size_substitute\n",
    "    self._num_features_substitute = number_of_features_substitute\n",
    "    self._num_action_substitute = number_of_actions_substitute\n",
    "    self._num_hidden_substitute = number_of_hidden_substitute\n",
    "    self._initial_state_substitute = initial_state_substitute\n",
    "    self._s_substitute = initial_state_substitute\n",
    "    self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
    "    self._times_trained_substitute = 0\n",
    "    self._inventory_substitute = set()\n",
    "    self._replayBuffer_substitute = []\n",
    "    self._num_offline_updates_substitute = num_offline_updates_substitute\n",
    "    self._rl_alg_substitute = rl_alg_substitute\n",
    "    \n",
    "    self._eps = 1.0\n",
    "\n",
    "    \n",
    "    self._prev_action_teacher = 0\n",
    "    self._step_teacher = step_size_teacher\n",
    "    self._num_features_teacher = number_of_features_teacher\n",
    "    self._num_action_teacher = number_of_actions_teacher\n",
    "    self._num_hidden_teacher = number_of_hidden_teacher\n",
    "    self._initial_state_teacher = initial_state_teacher\n",
    "    self._s_teacher = initial_state_teacher\n",
    "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
    "    self._times_trained_teacher = 0\n",
    "    self._replayBuffer_teacher = []\n",
    "    self._num_offline_updates_teacher = num_offline_updates_teacher\n",
    "    self._rl_alg_teacher = rl_alg_teacher\n",
    "    self.name = 'HYPER '+self._rl_alg_teacher\n",
    "    \n",
    "    self._eps_teacher = 1.0\n",
    "  \n",
    "    # ?????????? should it be the number of tasks\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    \n",
    "    \n",
    "    self._times_used = 0.\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_teacher()\n",
    "    self.resetReplayBuffer_teacher()\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    self._times_trained_teacher = 0\n",
    "    self._prev_action_teacher = 0\n",
    "\n",
    "    self.resetReplayBuffer('TRAIN_STUDENT')\n",
    "    self.resetState('TRAIN_STUDENT')\n",
    "    self._times_trained_student = 0\n",
    "    self._prev_action_student = 0\n",
    "    self._inventory_student = set()\n",
    "\n",
    "    self.resetReplayBuffer('TEST_SUBSTITUTE')\n",
    "    self.resetState('TEST_SUBSTITUTE')\n",
    "    self._times_trained_substitute = 0\n",
    "    self._prev_action_substitute = 0\n",
    "    self._inventory_substitute = set()\n",
    "    \n",
    "    self._eps_teacher = 1.0\n",
    "    self._eps = 1.0\n",
    "    \n",
    "    self._times_used = 0\n",
    "\n",
    "\n",
    "\n",
    "  def resetReplayBuffer_teacher(self):\n",
    "    self._replayBuffer_teacher = []\n",
    "    \n",
    "  def resetState_teacher(self):\n",
    "    self._s_teacher = self._initial_state_teacher \n",
    "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.disTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_teacher = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_teacher = tf.placeholder(tf.float64, shape=(1,self._num_features_teacher))\n",
    "    self._dense_1_teacher = tf.layers.dense(self.stateTensor_teacher,\n",
    "                                    self._num_hidden_teacher, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_teacher = tf.layers.dense(self._dense_1_teacher,\n",
    "                                    self._num_action_teacher, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_teacher = tf.reshape(self._dense_2_teacher, (self._num_action_teacher,))    \n",
    "    self._softmx_teacher = tf.nn.softmax(self._q_teacher)\n",
    "    self._cost_teacher = tf.losses.mean_squared_error(self.rewTensor_teacher + self.disTensor_teacher*self.nqTensor_teacher, self._q_teacher[self.actionTensor_teacher])\n",
    "    self._opt_teacher = tf.train.RMSPropOptimizer(self._step_teacher).minimize(self._cost_teacher) \n",
    "    \n",
    "\n",
    "    self.rewTensor_student = tf.placeholder(tf.float64)\n",
    "    self.disTensor_student = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_student = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_student = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_student = tf.placeholder(tf.float64, shape=(1,self._num_features_student))\n",
    "    self._dense_1_student = tf.layers.dense(self.stateTensor_student,\n",
    "                                    self._num_hidden_student, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_student = tf.layers.dense(self._dense_1_student,\n",
    "                                    self._num_action_student, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_student = tf.reshape(self._dense_2_student, (self._num_action_student,))    \n",
    "    self._cost_student = tf.losses.mean_squared_error(self.rewTensor_student+ self.disTensor_student*self.nqTensor_student, self._q_student[self.actionTensor_student])\n",
    "    self._opt_student = tf.train.RMSPropOptimizer(self._step_student).minimize(self._cost_student)\n",
    "\n",
    "\n",
    "    self.rewTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.disTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_substitute = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_substitute = tf.placeholder(tf.float64, shape=(1,self._num_features_substitute))\n",
    "    self._dense_0_substitute = tf.layers.dense(self.stateTensor_substitute,\n",
    "                                    self._num_hidden_substitute, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_1_substitute = tf.layers.dense(self._dense_0_substitute,\n",
    "                                    self._num_hidden_substitute, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_substitute = tf.layers.dense(self._dense_1_substitute,\n",
    "                                    self._num_action_substitute, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_substitute = tf.reshape(self._dense_2_substitute, (self._num_action_substitute,))    \n",
    "    self._cost_substitute = tf.losses.mean_squared_error(self.rewTensor_substitute+ self.disTensor_substitute*self.nqTensor_substitute, self._q_substitute[self.actionTensor_substitute])\n",
    "    self._opt_substitute = tf.train.RMSPropOptimizer(self._step_substitute).minimize(self._cost_substitute)\n",
    "    \n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy_teacher(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_teacher(self, q):    \n",
    "    return epsilon_greedy(q, self._eps_teacher)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "\n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs_teacher\n",
    "\n",
    "  def q_teacher(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t, probs = self._sess.run([self._q_teacher, self._softmx_teacher], {self.stateTensor_teacher: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step_teacher(self, r, g, s, train):\n",
    "    self._times_used += 1\n",
    "    #print self._times_used\n",
    "    qvs, probs = self.q_teacher(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs_teacher = probs\n",
    "    next_action = self._behaviour_policy_teacher(q_nxtState)\n",
    "    \n",
    "    if r != None and train == True:\n",
    "      if self._rl_alg_teacher == 'NEURALSARSA':\n",
    "        target = self._target_policy_teacher(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "        #print vob\n",
    "        self._sess.run(self._opt_teacher,{\n",
    "            self.nqTensor_teacher: vob,\n",
    "            self.rewTensor_teacher: r,\n",
    "            self.disTensor_teacher: g,\n",
    "            self.actionTensor_teacher: self._prev_action_teacher,\n",
    "            self.stateTensor_teacher: self._s_teacher})\n",
    "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_teacher):\n",
    "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
    "          self._sess.run(self._opt_teacher,{\n",
    "              self.nqTensor_teacher: replay[4],\n",
    "              self.rewTensor_teacher: replay[2],\n",
    "              self.disTensor_teacher: replay[3],\n",
    "              self.actionTensor_teacher: replay[1],\n",
    "              self.stateTensor_teacher: replay[0]})\n",
    "      elif self._rl_alg_teacher == 'DQN':\n",
    "        # This function should return an action\n",
    "        # Optimiser\n",
    "        vob = np.max(q_nxtState)\n",
    "        self._sess.run(self._opt_teacher,{\n",
    "            self.nqTensor_teacher: vob,\n",
    "            self.rewTensor_teacher: r,\n",
    "            self.disTensor_teacher: g,\n",
    "            self.actionTensor_teacher: self._prev_action_teacher,\n",
    "            self.stateTensor_teacher: self._s_teacher})\n",
    "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_teacher):\n",
    "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
    "          self._sess.run(self._opt_teacher,{\n",
    "              self.nqTensor_teacher: replay[4],\n",
    "              self.rewTensor_teacher: replay[2],\n",
    "              self.disTensor_teacher: replay[3],\n",
    "              self.actionTensor_teacher: replay[1],\n",
    "              self.stateTensor_teacher: replay[0]})\n",
    "\n",
    "    self._s_teacher = np.reshape(s, (1,-1))\n",
    "    self._prev_action_teacher = next_action\n",
    "    \n",
    "    return next_action\n",
    "\n",
    "  def reset_teacher(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_teacher()\n",
    "    self.resetReplayBuffer_teacher()\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    self._times_trained_teacher = 0\n",
    "    self._prev_action_teacher = 0\n",
    "\n",
    "\n",
    "\n",
    "    # resetReplayBuffer_student\n",
    "  def resetReplayBuffer(self, STUDENT_TYPE):\n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      self._replayBuffer_student = []\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      self._replayBuffer_substitute = []\n",
    "\n",
    "    # resetState_student\n",
    "  def resetState(self, STUDENT_TYPE):\n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      self._s_student = self._initial_state_student \n",
    "      self._s_student = np.reshape(self._s_student, (1,-1))\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      self._s_substitute = self._initial_state_substitute\n",
    "      self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
    "\n",
    "\n",
    "  def _target_policy_student(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_student(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, self._eps) if train == True else epsilon_greedy(q, 0.05)\n",
    "\n",
    "  def _target_policy_substitute(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_substitute(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, self._eps) if train == True else epsilon_greedy(q, 0.05)\n",
    "  \n",
    "  def q_student(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t = self._sess.run(self._q_student, {self.stateTensor_student: obs})\n",
    "    return t\n",
    "  \n",
    "  def q_substitute(self, obs):\n",
    "    obs = np.reshape(obs, (1,-1))\n",
    "    t = self._sess.run(self._q_substitute, {self.stateTensor_substitute: obs})\n",
    "    return t\n",
    "\n",
    "  # step_student\n",
    "  def step(self, r, g, s, item, train, STUDENT_TYPE):\n",
    "    cost = None\n",
    "    \n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      if item != None:\n",
    "        self._inventory_student.add(item)\n",
    "      \n",
    "      # This function should return an action\n",
    "      q_nxtState = np.reshape(self.q_student(s), (-1,))\n",
    "      next_action = self._behaviour_policy_student(q_nxtState, train)\n",
    "      \n",
    "\n",
    "      if self._rl_alg_student == 'NEURALSARSA':\n",
    "        target = self._target_policy_student(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        \n",
    "        # Optimiser\n",
    "        vob = q_nxtState[target]\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_student,{\n",
    "              self.nqTensor_student: vob,\n",
    "              self.rewTensor_student: r,\n",
    "              self.disTensor_student: g,\n",
    "              self.actionTensor_student: self._prev_action_student,\n",
    "              self.stateTensor_student: self._s_student})\n",
    "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_student):\n",
    "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
    "            self._sess.run(self._opt_student,{\n",
    "                self.nqTensor_student: replay[4],\n",
    "                self.rewTensor_student: replay[2],\n",
    "                self.disTensor_student: replay[3],\n",
    "                self.actionTensor_student: replay[1],\n",
    "                self.stateTensor_student: replay[0]})\n",
    "      elif self._rl_alg_student == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_student,{\n",
    "              self.nqTensor_student: vob,\n",
    "              self.rewTensor_student: r,\n",
    "              self.disTensor_student: g,\n",
    "              self.actionTensor_student: self._prev_action_student,\n",
    "              self.stateTensor_student: self._s_student})\n",
    "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_student):\n",
    "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
    "            self._sess.run(self._opt_student,{\n",
    "                self.nqTensor_student: replay[4],\n",
    "                self.rewTensor_student: replay[2],\n",
    "                self.disTensor_student: replay[3],\n",
    "                self.actionTensor_student: replay[1],\n",
    "                self.stateTensor_student: replay[0]})\n",
    "\n",
    "      \n",
    "          \n",
    "      self._s_student = np.reshape(s, (1,-1))\n",
    "      self._prev_action_student = next_action\n",
    "      return next_action, self._inventory_student, cost\n",
    "\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      if item != None:\n",
    "        self._inventory_substitute.add(item)\n",
    "      \n",
    "      # This function should return an action\n",
    "      q_nxtState = np.reshape(self.q_substitute(s), (-1,))\n",
    "      next_action = self._behaviour_policy_substitute(q_nxtState, train)\n",
    "      \n",
    "\n",
    "      if self._rl_alg_substitute == 'NEURALSARSA':\n",
    "        target = self._target_policy_substitute(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        \n",
    "        # Optimiser\n",
    "        vob = q_nxtState[target]\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_substitute,{\n",
    "              self.nqTensor_substitute: vob,\n",
    "              self.rewTensor_substitute: r,\n",
    "              self.disTensor_substitute: g,\n",
    "              self.actionTensor_substitute: self._prev_action_substitute,\n",
    "              self.stateTensor_substitute: self._s_substitute})\n",
    "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_substitute):\n",
    "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
    "            self._sess.run(self._opt_substitute,{\n",
    "                self.nqTensor_substitute: replay[4],\n",
    "                self.rewTensor_substitute: replay[2],\n",
    "                self.disTensor_substitute: replay[3],\n",
    "                self.actionTensor_substitute: replay[1],\n",
    "                self.stateTensor_substitute: replay[0]})\n",
    "      elif self._rl_alg_substitute == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_substitute,{\n",
    "              self.nqTensor_substitute: vob,\n",
    "              self.rewTensor_substitute: r,\n",
    "              self.disTensor_substitute: g,\n",
    "              self.actionTensor_substitute: self._prev_action_substitute,\n",
    "              self.stateTensor_substitute: self._s_substitute})\n",
    "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_substitute):\n",
    "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
    "            self._sess.run(self._opt_substitute,{\n",
    "                self.nqTensor_substitute: replay[4],\n",
    "                self.rewTensor_substitute: replay[2],\n",
    "                self.disTensor_substitute: replay[3],\n",
    "                self.actionTensor_substitute: replay[1],\n",
    "                self.stateTensor_substitute: replay[0]})\n",
    "          \n",
    "      self._s_substitute = np.reshape(s, (1,-1))\n",
    "      self._prev_action_substitute = next_action\n",
    "      return next_action, self._inventory_substitute, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixZUk41Zj1v6"
   },
   "source": [
    "## Task Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS1RGyMGj1v7"
   },
   "outputs": [],
   "source": [
    "class TaskSelector(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, reward_signal, number_of_tasks_selection_steps, student_type):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._unscaled_env_reward_history = []\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "    self._FIFO_Length = 5\n",
    "    self._tasks_env_buffer = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_buffer_scaled = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_env_buffer_scaled = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_episodes_completed_train = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._tasks_episodes_completed_test = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._time = 0\n",
    "    self._number_of_tasks_selection_steps = number_of_tasks_selection_steps\n",
    "    \n",
    "    self._train_tasks_times_selected = np.zeros((len(tasks[0]._goal_loc)))\n",
    "    self._test_tasks_times_selected = np.zeros((len(tasks[0]._goal_loc)))\n",
    "    self._train_task_accuracy = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._test_task_accuracy = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    \n",
    "    self._tasks_slopes = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    \n",
    "    self._student_type = student_type\n",
    "                                   \n",
    "  def resetReplayBuffer(self):\n",
    "    self._rl_agent.resetReplayBuffer(self._student_type)    \n",
    "  \n",
    "  def step(self, action_task_id):\n",
    "    if np.all(self._train_tasks_times_selected == 0.):\n",
    "      for i in range(len(tasks[0]._goal_loc)):\n",
    "        for j in range(self._FIFO_Length):\n",
    "          reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], i, self._rl_agent, False, self._student_type)\n",
    "          self._tasks_env_buffer[:,i] = np.roll(self._tasks_env_buffer[:,i], 1)\n",
    "          self._tasks_env_buffer[0,i] = reward_after\n",
    "          \n",
    "    eps = min(1.0, max(0.1, 1.0 - (self._time)/(self._number_of_tasks_selection_steps/8.)))\n",
    "    self._rl_agent._eps = eps\n",
    "    \n",
    "    ep_comp_train = False\n",
    "    if self._reward_signal == 'SPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[action_task_id] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], action_task_id, self._rl_agent, False, self._student_type)\n",
    "      if not ep_comp_test:\n",
    "#         print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True, self._student_type)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "       \n",
    "    elif self._reward_signal == 'TPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[-1] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], -1, self._rl_agent, False, self._student_type)\n",
    "      if not ep_comp_test:\n",
    "#         print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True, self._student_type)\n",
    "      \n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[-1, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[-1, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[-1, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected       \n",
    "    \n",
    "    elif self._reward_signal == 'MPG':\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      \n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[uniform_sampled_task_id] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], uniform_sampled_task_id, self._rl_agent, False, self._student_type)\n",
    "      if not ep_comp_test:\n",
    "#         print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True, self._student_type)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "    \n",
    "    self._tasks_env_buffer[:,action_task_id] = np.roll(self._tasks_env_buffer[:,action_task_id], 1)\n",
    "    self._tasks_env_buffer[0,action_task_id] = reward_after\n",
    "    X = np.arange(self._tasks_env_buffer.shape[0])\n",
    "    slope, _, _, _, _ = stats.linregress(X, self._tasks_env_buffer[:,action_task_id])\n",
    "    rhat = np.abs(slope)\n",
    "    \n",
    "    for i in range(self._tasks_env_buffer.shape[1]):\n",
    "      slope, _, _, _, _ = stats.linregress(X, self._tasks_env_buffer[:,i])\n",
    "      self._tasks_slopes[i, self._time] = np.abs(slope)\n",
    "#       print(\"Task: \", i, slope)\n",
    "    \n",
    "    \n",
    "    self._time += 1\n",
    "#     self._unscaled_reward_history.append(rhat)\n",
    "#     temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "#     p_20 = np.percentile(temp_history, 20)\n",
    "#     p_80 = np.percentile(temp_history, 80)        \n",
    "\n",
    "#     if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
    "#       raise ValueError('Action {} is out of bounds for a '\n",
    "#                        '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
    "    \n",
    "#     r = None\n",
    "#     if rhat <= p_20:\n",
    "#       r = -1.\n",
    "#     elif rhat > p_80:\n",
    "#       r = 1.\n",
    "#     else:\n",
    "#       r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "      \n",
    "#     self._tasks_buffer_scaled[:,action_task_id] = np.roll(self._tasks_buffer_scaled[:,action_task_id], 1)\n",
    "#     self._tasks_buffer_scaled[0,action_task_id] = r\n",
    "#     print('Task: '+str(action_task_id), rhat, reward_after, self._tasks_env_buffer[:,action_task_id])\n",
    "\n",
    "    #print reward_steps_after\n",
    "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
    "    return rhat, reward_after, np.reshape(self._tasks_env_buffer.T,(-1,)), (ep_comp_train or ep_comp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrXBQ9NZj1v-"
   },
   "outputs": [],
   "source": [
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(24, 24))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  #print vmin, vmax\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
    "#   plt.savefig('./action_values_{}'.format(title))\n",
    "#   plt.close()\n",
    "\n",
    "def plot_greedy_policy(grid, title, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid(title)\n",
    "  plt.hold('on')\n",
    "  for i in range(grid._layout.shape[0]):\n",
    "    for j in range(grid._layout.shape[1]):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_slopes(slopes, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Slopes of Tasks; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(slopes.shape[0]):    \n",
    "    plot = plt.plot(slopes[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48UmChiQ_zZa"
   },
   "outputs": [],
   "source": [
    "def plot_task_accuracy(accuracy_train, accuracy_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Train Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_train.shape[0]):    \n",
    "    plot = plt.plot(accuracy_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Test Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_test.shape[0]):    \n",
    "    plot = plt.plot(accuracy_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6Y1XQzF_zZd"
   },
   "outputs": [],
   "source": [
    "def plot_completed_episodes(completed_train, completed_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Train Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_train.shape[0]):    \n",
    "    plot = plt.plot(completed_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Test Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_test.shape[0]):    \n",
    "    plot = plt.plot(completed_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGZIM_Gcj1wA"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  reward_delta_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  episode_dict = {}\n",
    "  \n",
    "  student_type = ['TRAIN_STUDENT', 'TEST_SUBSTITUTE']\n",
    "  alg = algs[0]\n",
    "  \n",
    "  for typ in student_type:\n",
    "    print('Running:', alg.name + ' ' + typ)\n",
    "    reward_dict[alg.name + ' ' + typ] = []\n",
    "    reward_delta_dict[alg.name + ' ' + typ] = []\n",
    "    action_dict[alg.name + ' ' + typ] = []\n",
    "    prob_dict[alg.name + ' ' + typ] = []\n",
    "    entropy_dict[alg.name + ' ' + typ] = []\n",
    "    episode_dict[alg.name + ' ' + typ] = []\n",
    "    \n",
    "    qs = None\n",
    "    completed_train_episodes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    completed_test_episodes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    train_task_accuracy = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    test_task_accuracy = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks)) \n",
    "    task_slopes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "\n",
    "#     alg.reset()\n",
    "    bandit = TaskSelector(alg, tasks, reward_signal, number_of_steps_of_selecting_tasks, typ)\n",
    "\n",
    "    reward_dict[alg.name + ' ' + typ].append([0.])\n",
    "    episode_dict[alg.name + ' ' + typ].append([0.])\n",
    "    reward_delta_dict[alg.name + ' ' + typ].append([])\n",
    "    action_dict[alg.name + ' ' + typ].append([])\n",
    "    prob_dict[alg.name + ' ' + typ].append([])\n",
    "    entropy_dict[alg.name + ' ' + typ].append([])\n",
    "    action = None\n",
    "    reward = None\n",
    "    prob = None\n",
    "    entropy = None\n",
    "    reward_delta = None\n",
    "    success_student_episode = False\n",
    "    capability = alg._initial_state_teacher\n",
    "\n",
    "    for i in range(number_of_steps_of_selecting_tasks):\n",
    "      print('Step: ', i)\n",
    "      if typ == 'TRAIN_STUDENT':\n",
    "#         action = alg.step_teacher(reward, 0., capability, True) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, True)\n",
    "        action = alg.step_teacher(reward, 0.98, capability, True)\n",
    "        eps = min(1.0, max(0.1, 1.0 - (i - 0.)/(number_of_steps_of_selecting_tasks/8.)))\n",
    "        alg._eps_teacher = eps\n",
    "      elif typ == 'TEST_SUBSTITUTE':\n",
    "#         action = alg.step_teacher(reward, 0., capability, False) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, False)\n",
    "        action = alg.step_teacher(reward, 0.98, capability, False)  \n",
    "      alg._eps_teacher = 0.\n",
    "        \n",
    "      prob = alg.getProbs()\n",
    "      entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "      reward, reward_from_environment, capability, success_student_episode = bandit.step(action)\n",
    "      bandit.resetReplayBuffer()\n",
    "      \n",
    "      reward_dict[alg.name + ' ' + typ][-1].append(reward_from_environment+reward_dict[alg.name + ' ' + typ][-1][-1])\n",
    "      episode_dict[alg.name + ' ' + typ][-1].append(success_student_episode+episode_dict[alg.name + ' ' + typ][-1][-1])\n",
    "      reward_delta_dict[alg.name + ' ' + typ][-1].append(reward)\n",
    "      action_dict[alg.name + ' ' + typ][-1].append(action)\n",
    "      prob_dict[alg.name + ' ' + typ][-1].append(prob.copy())\n",
    "      entropy_dict[alg.name + ' ' + typ][-1].append(entropy)\n",
    "      \n",
    "    if typ == 'TRAIN_STUDENT':\n",
    "      h, w = tasks[-1]._layout.shape\n",
    "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "      qs = np.array([[[alg.q_student(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "    elif typ == 'TEST_SUBSTITUTE':\n",
    "      h, w = tasks[-1]._layout.shape\n",
    "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "      qs = np.array([[[alg.q_substitute(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "\n",
    "    completed_train_episodes = bandit._tasks_episodes_completed_train\n",
    "    completed_test_episodes = bandit._tasks_episodes_completed_test\n",
    "    train_task_accuracy = bandit._train_task_accuracy\n",
    "    test_task_accuracy = bandit._test_task_accuracy\n",
    "    task_slopes += bandit._tasks_slopes\n",
    "    \n",
    "    if typ == 'TRAIN_STUDENT':\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_slopes(task_slopes, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "    else:\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_slopes(task_slopes, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "\n",
    "    plot_action_values(qs, alg.name + ' ' + typ)\n",
    "    print('Train: ', bandit._train_tasks_times_selected)\n",
    "    print('Test: ', bandit._test_tasks_times_selected)\n",
    "    \n",
    "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
    "  smoothed_rewards = {}\n",
    "  smoothed_rewards_stds = {}\n",
    "  smoothed_reward_deltas = {}\n",
    "  smoothed_reward_deltas_stds = {}\n",
    "  smoothed_actions = {}\n",
    "  smoothed_probs = {}\n",
    "  smoothed_entropies = {}\n",
    "  smoothed_entropies_stds = {}\n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, rewards in reward_delta_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    \n",
    "    plt.figure(figsize=(44,40))\n",
    "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
    "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Reward')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Delta')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_reward_deltas[agent] /= repetitions    \n",
    "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTRJuSa_j1wC"
   },
   "outputs": [],
   "source": [
    "def run_step(env, teacher_action, agent, train, student_type):     \n",
    "    env.resetState()\n",
    "    agent.resetState(student_type)\n",
    "    number_of_steps = env.distanceToGoal(teacher_action)\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
    "    steps_completed = 0\n",
    "    total_reward = 0.\n",
    "    while steps_completed != number_of_steps:\n",
    "      reward, discount, next_state, item = env.step(action, teacher_action, agent_inventory)\n",
    "      \n",
    "      if item != None:\n",
    "        if student_type == 'TRAIN_STUDENT':\n",
    "          agent._inventory_student.add(item)\n",
    "        else:\n",
    "          agent._inventory_substitute.add(item)\n",
    "        \n",
    "      total_reward += reward\n",
    "      \n",
    "      # Dont want to remove the key on train==True, cuz then cant get reward on train==false, where we record the reward      \n",
    "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train, student_type)\n",
    "      \n",
    "      if discount == 0:\n",
    "        #print(total_reward)\n",
    "        print('EPISODE COMPLETED')\n",
    "        goal_y = env._goal_loc[teacher_action][0]\n",
    "        goal_x = env._goal_loc[teacher_action][1]\n",
    "        if env._layout[goal_y,goal_x] == 100.:\n",
    "          print('REMOVED KEY')\n",
    "          if student_type == 'TRAIN_STUDENT':\n",
    "            agent._inventory_student.remove('KEY')\n",
    "          else:\n",
    "            agent._inventory_substitute.remove('KEY')\n",
    "        return total_reward, total_reward/steps_completed, True, 0\n",
    "      \n",
    "      steps_completed += 1.\n",
    "    \n",
    "    mean_reward = total_reward/number_of_steps\n",
    "\n",
    "    return total_reward, mean_reward, False, float(env.distanceLeft(teacher_action))\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1672I5Z7j1wE"
   },
   "source": [
    "# Reward as Reward Signal for Bandit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK8BVplBj1wE"
   },
   "source": [
    "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
    "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
    "#### NeuralRL Controller does not have inventory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SsYpc2SRPe4"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 500\n",
    "reps = 1\n",
    "\n",
    "reward_signals=['TPG','MPG','SPG']\n",
    "rl_teaching_algs = ['NEURALSARSA','DQN']\n",
    "\n",
    "# reward_signals=['SPG']\n",
    "# rl_teaching_algs = ['DQN']\n",
    "\n",
    "hidden_units_teacher_net = 30\n",
    "hidden_units_student_net = 30\n",
    "hidden_units_substitute_net = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "colab_type": "code",
    "id": "dDzz94z_iRXz",
    "outputId": "bce2d9b1-e97f-44ed-95e4-9c8281c3a93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: HYPER NEURALSARSA TRAIN_STUDENT\n",
      "Step:  0\n",
      "EPISODE COMPLETED\n",
      "Step:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:72: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:76: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  2\n",
      "Step:  3\n",
      "Step:  4\n",
      "EPISODE COMPLETED\n",
      "Step:  5\n",
      "Step:  6\n",
      "Step:  7\n",
      "EPISODE COMPLETED\n",
      "Step:  8\n",
      "Step:  9\n",
      "Step:  10\n",
      "Step:  11\n",
      "Step:  12\n",
      "Step:  13\n",
      "Step:  14\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  15\n",
      "Step:  16\n",
      "Step:  17\n",
      "EPISODE COMPLETED\n",
      "Step:  18\n",
      "Step:  19\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  20\n",
      "Step:  21\n",
      "EPISODE COMPLETED\n",
      "Step:  22\n",
      "Step:  23\n",
      "Step:  24\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  25\n",
      "Step:  26\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  27\n",
      "Step:  28\n",
      "Step:  29\n",
      "EPISODE COMPLETED\n",
      "Step:  30\n",
      "Step:  31\n",
      "Step:  32\n",
      "Step:  33\n",
      "Step:  34\n",
      "Step:  35\n",
      "Step:  36\n",
      "Step:  37\n",
      "Step:  38\n",
      "Step:  39\n",
      "EPISODE COMPLETED\n",
      "Step:  40\n",
      "Step:  41\n",
      "Step:  42\n",
      "Step:  43\n",
      "Step:  44\n",
      "Step:  45\n",
      "Step:  46\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  47\n",
      "Step:  48\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  49\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  50\n",
      "Step:  51\n",
      "Step:  52\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  53\n",
      "Step:  54\n",
      "Step:  55\n",
      "EPISODE COMPLETED\n",
      "Step:  56\n",
      "Step:  57\n",
      "Step:  58\n",
      "Step:  59\n",
      "Step:  60\n",
      "Step:  61\n",
      "Step:  62\n",
      "EPISODE COMPLETED\n",
      "REMOVED KEY\n",
      "Step:  63\n",
      "Step:  64\n",
      "Step:  65\n",
      "Step:  66\n",
      "Step:  67\n",
      "Step:  68\n",
      "Step:  69\n",
      "Step:  70\n",
      "Step:  71\n",
      "Step:  72\n",
      "Step:  73\n",
      "Step:  74\n",
      "Step:  75\n",
      "Step:  76\n",
      "Step:  77\n",
      "Step:  78\n",
      "Step:  79\n",
      "Step:  80\n",
      "Step:  81\n",
      "Step:  82\n",
      "Step:  83\n",
      "Step:  84\n",
      "Step:  85\n",
      "Step:  86\n",
      "Step:  87\n",
      "Step:  88\n",
      "Step:  89\n",
      "Step:  90\n",
      "Step:  91\n",
      "Step:  92\n",
      "Step:  93\n",
      "Step:  94\n",
      "Step:  95\n",
      "Step:  96\n",
      "Step:  97\n",
      "Step:  98\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  99\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  100\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  101\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  102\n",
      "Step:  103\n",
      "Step:  104\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  105\n",
      "Step:  106\n",
      "Step:  107\n",
      "Step:  108\n",
      "Step:  109\n",
      "Step:  110\n",
      "Step:  111\n",
      "Step:  112\n",
      "Step:  113\n",
      "Step:  114\n",
      "Step:  115\n",
      "Step:  116\n",
      "Step:  117\n",
      "Step:  118\n",
      "Step:  119\n",
      "EPISODE COMPLETED\n",
      "REMOVED KEY\n",
      "Step:  120\n",
      "Step:  121\n",
      "Step:  122\n",
      "Step:  123\n",
      "Step:  124\n",
      "Step:  125\n",
      "Step:  126\n",
      "Step:  127\n",
      "Step:  128\n",
      "Step:  129\n",
      "Step:  130\n",
      "Step:  131\n",
      "Step:  132\n",
      "Step:  133\n",
      "Step:  134\n",
      "Step:  135\n",
      "Step:  136\n",
      "Step:  137\n",
      "Step:  138\n",
      "Step:  139\n",
      "Step:  140\n",
      "Step:  141\n",
      "Step:  142\n",
      "Step:  143\n",
      "Step:  144\n",
      "Step:  145\n",
      "Step:  146\n",
      "Step:  147\n",
      "Step:  148\n",
      "Step:  149\n",
      "Step:  150\n",
      "Step:  151\n",
      "Step:  152\n",
      "Step:  153\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  154\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  155\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  156\n",
      "Step:  157\n",
      "Step:  158\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  159\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  160\n",
      "Step:  161\n",
      "Step:  162\n",
      "Step:  163\n",
      "Step:  164\n",
      "Step:  165\n",
      "Step:  166\n",
      "Step:  167\n",
      "Step:  168\n",
      "Step:  169\n",
      "Step:  170\n",
      "Step:  171\n",
      "Step:  172\n",
      "Step:  173\n",
      "Step:  174\n",
      "Step:  175\n",
      "Step:  176\n",
      "Step:  177\n",
      "Step:  178\n",
      "Step:  179\n",
      "Step:  180\n",
      "Step:  181\n",
      "Step:  182\n",
      "Step:  183\n",
      "Step:  184\n",
      "Step:  185\n",
      "Step:  186\n",
      "Step:  187\n",
      "Step:  188\n",
      "Step:  189\n",
      "Step:  190\n",
      "Step:  191\n",
      "Step:  192\n",
      "Step:  193\n",
      "Step:  194\n",
      "Step:  195\n",
      "Step:  196\n",
      "Step:  197\n",
      "Step:  198\n",
      "Step:  199\n",
      "Step:  200\n",
      "Step:  201\n",
      "Step:  202\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  203\n",
      "Step:  204\n",
      "Step:  205\n",
      "Step:  206\n",
      "Step:  207\n",
      "Step:  208\n",
      "Step:  209\n",
      "Step:  210\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  211\n",
      "Step:  212\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  213\n",
      "Step:  214\n",
      "Step:  215\n",
      "Step:  216\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  217\n",
      "Step:  218\n",
      "Step:  219\n",
      "Step:  220\n",
      "Step:  221\n",
      "Step:  222\n",
      "Step:  223\n",
      "Step:  224\n",
      "Step:  225\n",
      "Step:  226\n",
      "Step:  227\n",
      "Step:  228\n",
      "Step:  229\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  230\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  231\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  232\n",
      "Step:  233\n",
      "Step:  234\n",
      "Step:  235\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  236\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  237\n",
      "Step:  238\n",
      "Step:  239\n",
      "Step:  240\n",
      "Step:  241\n",
      "EPISODE COMPLETED\n",
      "REMOVED KEY\n",
      "Step:  242\n",
      "Step:  243\n",
      "Step:  244\n",
      "Step:  245\n",
      "Step:  246\n",
      "Step:  247\n",
      "Step:  248\n",
      "Step:  249\n",
      "Step:  250\n",
      "Step:  251\n",
      "Step:  252\n",
      "Step:  253\n",
      "Step:  254\n",
      "Step:  255\n",
      "Step:  256\n",
      "Step:  257\n",
      "Step:  258\n",
      "Step:  259\n",
      "Step:  260\n",
      "Step:  261\n",
      "Step:  262\n",
      "Step:  263\n",
      "Step:  264\n",
      "Step:  265\n",
      "Step:  266\n",
      "Step:  267\n",
      "Step:  268\n",
      "Step:  269\n",
      "Step:  270\n",
      "Step:  271\n",
      "Step:  272\n",
      "Step:  273\n",
      "Step:  274\n",
      "Step:  275\n",
      "Step:  276\n",
      "Step:  277\n",
      "Step:  278\n",
      "Step:  279\n",
      "Step:  280\n",
      "Step:  281\n",
      "Step:  282\n",
      "Step:  283\n",
      "Step:  284\n",
      "Step:  285\n",
      "Step:  286\n",
      "Step:  287\n",
      "Step:  288\n",
      "Step:  289\n",
      "Step:  290\n",
      "Step:  291\n",
      "Step:  292\n",
      "Step:  293\n",
      "Step:  294\n",
      "Step:  295\n",
      "Step:  296\n",
      "Step:  297\n",
      "Step:  298\n",
      "Step:  299\n",
      "Step:  300\n",
      "Step:  301\n",
      "Step:  302\n",
      "Step:  303\n",
      "Step:  304\n",
      "Step:  305\n",
      "Step:  306\n",
      "Step:  307\n",
      "Step:  308\n",
      "Step:  309\n",
      "Step:  310\n",
      "Step:  311\n",
      "Step:  312\n",
      "Step:  313\n",
      "Step:  314\n",
      "Step:  315\n",
      "Step:  316\n",
      "Step:  317\n",
      "Step:  318\n",
      "Step:  319\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  320\n",
      "Step:  321\n",
      "Step:  322\n",
      "Step:  323\n",
      "Step:  324\n",
      "Step:  325\n",
      "Step:  326\n",
      "Step:  327\n",
      "Step:  328\n",
      "Step:  329\n",
      "Step:  330\n",
      "Step:  331\n",
      "Step:  332\n",
      "Step:  333\n",
      "Step:  334\n",
      "EPISODE COMPLETED\n",
      "REMOVED KEY\n",
      "Step:  335\n",
      "Step:  336\n",
      "Step:  337\n",
      "Step:  338\n",
      "Step:  339\n",
      "Step:  340\n",
      "Step:  341\n",
      "Step:  342\n",
      "Step:  343\n",
      "Step:  344\n",
      "Step:  345\n",
      "Step:  346\n",
      "Step:  347\n",
      "Step:  348\n",
      "Step:  349\n",
      "Step:  350\n",
      "Step:  351\n",
      "Step:  352\n",
      "Step:  353\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  354\n",
      "Step:  355\n",
      "Step:  356\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  357\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  358\n",
      "Step:  359\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  360\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  361\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  362\n",
      "Step:  363\n",
      "Step:  364\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  365\n",
      "Step:  366\n",
      "Step:  367\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  368\n",
      "Step:  369\n",
      "PICKED UP KEY\n",
      "EPISODE COMPLETED\n",
      "Step:  370\n",
      "Step:  371\n",
      "Step:  372\n",
      "Step:  373\n",
      "Step:  374\n",
      "Step:  375\n",
      "Step:  376\n",
      "Step:  377\n",
      "Step:  378\n",
      "Step:  379\n",
      "Step:  380\n",
      "Step:  381\n",
      "Step:  382\n",
      "Step:  383\n",
      "Step:  384\n",
      "Step:  385\n",
      "Step:  386\n",
      "Step:  387\n",
      "EPISODE COMPLETED\n",
      "REMOVED KEY\n",
      "Step:  388\n",
      "Step:  389\n",
      "Step:  390\n"
     ]
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size_teacher = 0.01\n",
    "step_size_student = 0.01\n",
    "step_size_substitute = 0.01\n",
    "\n",
    "  \n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "    for teacher_agent in rl_teaching_algs:\n",
    "        for student_agent in rl_teaching_algs:                \n",
    "            # Intrinsically Motivated Curriculum Learning\n",
    "            number_of_arms_tasks = len(tasks[0]._goal_loc)\n",
    "\n",
    "            agents = [\n",
    "            NEURAL_TEACHER_STUDENT(number_of_arms_tasks*5,\n",
    "                                        (2*vision_size + 1)**2,\n",
    "                                        (2*vision_size + 1)**2,\n",
    "                                        hidden_units_teacher_net,\n",
    "                                        hidden_units_student_net,\n",
    "                                        hidden_units_substitute_net,\n",
    "                                        number_of_arms_tasks,\n",
    "                                        4,\n",
    "                                        4,\n",
    "                                        np.zeros((1,number_of_arms_tasks*5)),\n",
    "                                        tasks[0].get_obs(),\n",
    "                                        tasks[0].get_obs(),\n",
    "                                        teacher_agent,\n",
    "                                        student_agent,\n",
    "                                        student_agent,\n",
    "                                        num_offline_updates_teacher=30, \n",
    "                                        num_offline_updates_student=30,\n",
    "                                        num_offline_updates_substitute=30,\n",
    "                                        step_size_teacher=step_size_teacher,\n",
    "                                        step_size_student=step_size_student,\n",
    "                                        step_size_substitute=step_size_substitute),\n",
    "            ]\n",
    "\n",
    "            agents[0].reset()\n",
    "\n",
    "            train_task_agents(agents,\n",
    "                            number_of_arms_tasks,\n",
    "                            number_of_steps_of_selecting_tasks, \n",
    "                            tasks,\n",
    "                            reward_signal,\n",
    "                            reps,\n",
    "                            vision_size,\n",
    "                            tabular_grid)\n",
    "\n",
    "    \n",
    "for task in tasks:\n",
    "    task.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUmqra3NK3a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWmqCB_D_zZ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "New_Student_Experiments_SMART_REPLAY_SMALL_MAZE_LEAN_REWARD_SHAPED.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
