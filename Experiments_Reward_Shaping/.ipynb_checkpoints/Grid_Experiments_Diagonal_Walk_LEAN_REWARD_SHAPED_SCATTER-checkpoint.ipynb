{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# Door and Key Experiments in Curriculum Learning\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "26/07/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeGNMcHDj1vL"
   },
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mT38a_chiRWz"
   },
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False):\n",
    "    # 10: Key\n",
    "    # -2: Door\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    \n",
    "    self._wall = -1\n",
    "    self._door = -2\n",
    "    self._key = 10\n",
    "    \n",
    "#     self._layout = np.array([\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       ])\n",
    "    \n",
    "    \n",
    "    \n",
    "#      ROOMS\n",
    "  \n",
    "    self._layout = np.array([\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    self._goals = set()\n",
    "    self._goal_loc = []\n",
    "    \n",
    "    for e in goal_loc:\n",
    "      self._layout[e[0],e[1]] = e[2]\n",
    "      self._goal_loc.append((e[0],e[1]))\n",
    "      self._goals.add(e[2])\n",
    "    \n",
    "    #self._goal = value\n",
    "    \n",
    "    # row, col format\n",
    "#     self._start_state = (12, 8)\n",
    "    self._start_state = (11, 8)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "    self._distanceToGoal = None\n",
    "    \n",
    "    self._minDistanceStartGoal = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1],\n",
    "                                                          self._goal_loc[-1][0], self._goal_loc[-1][1])\n",
    "    \n",
    "    #self.distanceToGoal()\n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "  \n",
    "  def distanceToGoal(self):\n",
    "    return np.prod(self._layout.shape)\n",
    "    #return np.count_nonzero(self._layout != -1)*10\n",
    "  \n",
    "  def minDistanceTwoPoints(self, cy, cx, dy, dx):\n",
    "    distances = []\n",
    "    stack = []\n",
    "    visited = set()\n",
    "    stack.append((cy, cx, 0))\n",
    "    while len(stack) != 0:\n",
    "      #print len(stack)\n",
    "      cur_row, cur_col, dist = stack.pop()\n",
    "      if (cur_row, cur_col) in visited:\n",
    "        continue\n",
    "      visited.add((cur_row, cur_col))\n",
    "      \n",
    "      if (cur_row, cur_col) == (dy, dx):\n",
    "        distances.append(dist)\n",
    "\n",
    "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
    "        stack.append((cur_row+1, cur_col, dist+1))\n",
    "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
    "        stack.append((cur_row-1, cur_col, dist+1))\n",
    "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
    "        stack.append((cur_row, cur_col+1, dist+1))\n",
    "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
    "        stack.append((cur_row, cur_col-1, dist+1))\n",
    "    \n",
    "    return np.min(np.array(distances))\n",
    "  \n",
    "  def distanceToNearestGoal(self, new_y, new_x):\n",
    "    distances = []\n",
    "    locations = []\n",
    "    stack = []\n",
    "    visited = set()\n",
    "    stack.append((new_y, new_x, 0))\n",
    "    while len(stack) != 0:\n",
    "      #print len(stack)\n",
    "      cur_row, cur_col, dist = stack.pop()\n",
    "      if (cur_row, cur_col) in visited:\n",
    "        continue\n",
    "      visited.add((cur_row, cur_col))\n",
    "      \n",
    "      for e in self._goal_loc:\n",
    "        if (cur_row, cur_col) == e:\n",
    "          distances.append(dist)\n",
    "          locations.append(e)\n",
    "          \n",
    "\n",
    "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
    "        stack.append((cur_row+1, cur_col, dist+1))\n",
    "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
    "        stack.append((cur_row-1, cur_col, dist+1))\n",
    "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
    "        stack.append((cur_row, cur_col+1, dist+1))\n",
    "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
    "        stack.append((cur_row, cur_col-1, dist+1))\n",
    "    \n",
    "    # Has to be the absolute closest person\n",
    "    argmin_dist = np.argmin(np.array(distances))\n",
    "    #print(distances[argmin_dist], locations[argmin_dist])\n",
    "    return distances[argmin_dist], locations[argmin_dist] \n",
    "    \n",
    "  def handleDoor(self):\n",
    "    pass\n",
    "  \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self, title=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if title != None:\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      plt.title(\"The Grid\")\n",
    "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    \n",
    "    for e in self._goals:\n",
    "      if e == self._key:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{K}$\", ha='center', va='center')\n",
    "      elif e > 0:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    y = np.where(self._layout==self._door)[0]\n",
    "    x = np.where(self._layout==self._door)[1]\n",
    "    for i in range(y.shape[0]): \n",
    "      plt.text(x[i], y[i], r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    \n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
    "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
    "      return location\n",
    "\n",
    "  def step(self, action, agent_inventory):\n",
    "    item = None\n",
    "    y, x = self._state\n",
    "        \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == self._key: # a key\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      item = 'KEY'\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "      print('PICKED UP KEY')\n",
    "    elif self._layout[new_y, new_x] == self._door: # a door\n",
    "      reward = 5\n",
    "      if 'KEY' not in agent_inventory:\n",
    "        reward = self._layout[new_y, new_x]\n",
    "        new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] > 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      \n",
    "#       reward = 0.\n",
    "      distToNearestGoal, nearestGoal = self.distanceToNearestGoal(new_y, new_x)\n",
    "      if len(self._goal_loc) != 1:\n",
    "        minDistance = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1], nearestGoal[0], nearestGoal[1])    \n",
    "      else:\n",
    "        minDistance = self._minDistanceStartGoal\n",
    "      \n",
    "      distToNearestGoal = float(distToNearestGoal)\n",
    "      minDistance = float(minDistance)\n",
    "      \n",
    "      #print(self._layout[nearestGoal[0],nearestGoal[1]], distToNearestGoal, minDistance, distToNearestGoal/minDistance, -distToNearestGoal/minDistance, np.exp(-distToNearestGoal/minDistance))\n",
    "      reward = self._layout[nearestGoal[0],nearestGoal[1]]*np.exp(-distToNearestGoal/minDistance)\n",
    "      if self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' not in agent_inventory:\n",
    "        reward = 0.\n",
    "#       elif self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' in agent_inventory:\n",
    "#         print agent_inventory\n",
    "        \n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs(), item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxFTKIfFj1vP"
   },
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2627
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2799,
     "status": "ok",
     "timestamp": 1533076732154,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "Hd1tV95Gj1vQ",
    "outputId": "719ffc2e-7d10-464b-f7be-59e59d693783"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACpJJREFUeJzt3F9olfcdx/HPtzU6WRZkkcY4Ui+0jhGscTKbwNyi9qIX\n4uo/OlZhaMZ6IUsJbVeGdFuFWkoKoTKINyYUCquzbWrtQC/SXmi7sZGU4tKCNMIMUnRmMX8QkqV9\ndpEcOQ0xOTk5z+/8vsf3CwrmyfM7v2/i7+Pz9Jzv77EkSQTAj/uKXQCAhSG0gDOEFnCG0ALOEFrA\nGUILOENoS4CZ/djMPp/j+51mdjRkTUgPoS0BSZJcTJLkB4t9HTPbYWafm9mYmXWb2YOFqA+FRWid\nM7P7C/Q6lZLelnRE0ncl9Ug6VYjXRmER2kiZ2Q/NrNfMhs3sL2b2ppkdNbOfmtmAmf3WzL6U1JE5\nljV2k5n1TI99U9K3cphyj6R/JUnyTpIkE5L+KGmjma1P5QdE3ghthMysTNI7kjo0ddX7s6TdWaes\nkrRC0oOSfj19LMka2yXp9emxpyXtzWHaWkmfZr5IkuS2pP7p44jIkmIXgFnVS7o/SZI/TX/dZWb/\nyPr+V5L+kCTJ/yTJzLLHNkhakiTJ8emv3zazf+YwZ7mkGzOODUv6zkKLR7q40sZptaRrM44NZP35\nP5nAzqJ6lrH/zmHOMUkVM45VSBrNYSwCIrRx+lLS92Ycq8n681xbs2Ybm8u7wH2S6jJfmNm3Ja2d\nPo6IENo4/U3SV2Z22MzuN7OfSdqS9X27y7jM2Ekz+42ZLTGzPTPG3k2XpFoz221myyT9XtKnSZJc\nzveHQDoIbYSmb333SPqVpCFJv5B0VtJ45pQcxh6UNChpv6Y+yplvzpuaesPqmKT/SvqRpJ/n/UMg\nNcYmeB/M7O+S2pMkeb3YtaC4uNJGysx+YmZV07fHv5S0QdK5YteF4iO08fq+pj43HZLUImlvkiTX\nF/OCZvY7Mxs1s5EZ//21EAUjDG6PAWe40gLOzNkRZWZchoEiSZJk1o/25m1jXMjerKsLHJP2+aUy\nR4w1hZgjxppCzzEbbo8BZwgt4AyhBZwhtIAzhBZwhtACzhBawBlCCzhDaAFn5twwQBsjUDx3a2Pk\nSgs4k0rvca7b/TKP/kzr/OwxnntRY6wpxBwLXU9S6a2p2XClBZwhtIAzhBZwhtACzhBawBlCCzhD\naAFnCC3gDKEFnKH3GIgUvcdAiaD3eIaYe3BjqinEHPQez44rLeDMvFfaxRgdHdWzzz6rGzduaN26\ndSovL9etW7fU1tZWsDH5zAG/QqyP2NdUqlfanTt3amJiQl1dXWptbVVVVZWGh4cLOiafOeBXiPUR\n+5pKLbTd3d26cOGCmpqa7hw7cOCAli5dWrAx+cwBv0KsDw9rKrXQ9vb2ysxUXV1951h5eblOnDhR\nsDH5zAG/QqwPD2sq9TeiysrKUh+TzxzwK8T6iHlNpRbazZs3S5IGBwc1MjKilpYWNTY2ateuXerr\n6yvImHzmgF8h1oeHNZVaaLdv367GxkadPHlSFRUVamtr05UrV7Ry5UrV1tYWZEw+c8CvEOvDw5qa\nt41xMc0VY2Njamlp0dDQkNasWaP77pv6N6K1tTXz+t84f74xCz0/e45SaDKIqaYQc8zWXLHQNVXo\nNZg9Ju3f7d3aGFMN7XxKrXuF0KZz/r3aEZV3aBcwD4ACYsMAUCLYMDBDzLeJMdUUYo57/fb4brjS\nAs4QWsAZQgs4Q2gBZwgt4AyhBZwhtIAzhBZwhtACztB7DESK3mOgRKTSe5xWb+li+kQ9zxFjTSHm\niLGm7DH0HgPICaEFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcILeAMoQWcYcMAECk2DAAlgg0DDuaI\nsaYQc8RYU/YYNgwAyAmhBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDL3HQKToPQZKBL3HDuaI\nsaYQc8RYU/YYeo8B5ITQwo2Ojg7V1NRo06ZNevrpp7V161Y1Nzfr66+/Lsj5+Y4Jbd43org9Lv4c\nMdYUYo7Zzt+2bZs2bNig48ePa3JyUvX19dq/f7+ef/75Wccs9PyFjEn79pg3olBylixZon379qm9\nvT2V8/MdkzZCC9ceeOABDQwM6Pbt26mcn++YNBFauJa5VZ2cnEzl/HzHpInQwrWbN29q1apVqqio\nSOX8fMekidDCrYmJCb311ls6fPhwKufnOyZt8zZXALHo6OhQf3+/hoeH1dzcrE8++USPPPLInXd1\nF3t+vmNCo/cYiBQf+QAlIpXeY5oMSr+mEHPEWFP2GHqPAeSE0MIND33BIRBauHHo0CGtW7dOW7du\n1WuvvaYPP/xQH3/8sVpbW4tdWlCEFm7F2BccAqGFa7H1BYdAaOFabH3BIRBauBZbX3AIhBZuxdgX\nHAK9x3DDQ19wCKk8buZe7ZDh5y79mrLHFOtxM2wYACLFhgGgRLBhwMEcMdYUYo4Ya8oew4YBADmJ\nJrSZZvCamhp1dnZKkhoaGvT444/r8uXLRa4OMchnw8Do6Kieeuop7d69W88995xefPFFtbS0BKy6\n8KIJbaYZ/KGHHtLBgwfV39+vLVu26N1339X69euLXR4ikM+GgZ07d2piYkJdXV1qbW1VVVWVhoeH\nA1ZdeNGENtutW7f00ksv3XO7N7Aw820Y6O7u1oULF9TU1HTn2IEDB7R06dJQJaYiutBOTExoz549\nWrFihftfLtI314aB3t5emZmqq6vvHCsvL9eJEydCllhw0YX2s88+08MPP6z29nYNDAwUuxxELpcN\nA2VlZaHKCSK60NbV1enVV1/V6tWr9cILLxS7HERurg0DmzdvliQNDg5qZGRELS0tamxs1K5du9TX\n1xe61IKJLrTS1P+rHD16VG+88YbrXy7SNd+Gge3bt6uxsVEnT55URUWF2tradOXKFa1cuVK1tbWB\nqy2caEKbaQb/4osv1NnZqWXLlsnM9OSTT+qjjz4qdnmIQGaNXLx4Uc3NzdqxY8e8GwbOnDmj8fFx\n7du3T88884yeeOIJVVZWBqy68Og9BiJF7zFQIug9djBHjDWFmCPGmrLH0HsMICeEFm7wsPIphBZu\n8LDyKYQWbvGwcsAhHlYOOMPDygFneFg54AgPKwcix8PKp/CwcgdzxFhTiDlirCl7DA8rB/ANbBgA\nSkQqGwbSum1YTON1iDm4TZxbqf19s2EAQE4ILeAMoQWcIbSAM4QWcIbQAs4QWsAZQgs4Q2gBZ+g9\nBiJF7zFQIug9LuAc9B7PrdT+vuk9BpATnlzh1Pj4uA4dOqQ1a9ZobGxMy5cv1yuvvFLsshAAV1qn\nzpw5o+rqah07dkxNTU0aGhoqdkkIhNA6dfXqVX3wwQe6du2aNm7cqMcee6zYJSEQQuvUo48+qkuX\nLqmmpkYNDQ2qr68vdkkIhNA6VVdXp/Pnz2vv3r26dOmSjhw5UuySEAihder999/Xtm3bdPr0aZ06\ndUo9PT3FLgmBEFqnenp6dO7cOUlSZWWl1q5dW+SKEAof+Ti1fPlyvffeezp79qyuX7+ul19+udgl\nIRB6j4FI0XsMlAh6jx3MEWNNIeaIsabQc8yGKy3gDKEFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcI\nLeAMoQWcIbSAM+zyASJ1t10+c4YWQHy4PQacIbSAM4QWcIbQAs4QWsCZ/wMZKBiQgsmzXwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c7e92e610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c661bd3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c666696d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEFrAmf8CrEklUtu3XBEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c8160bd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c8309e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c7ca9dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCG0gDOEFnCG0ALO/BcA7ECFgLcinAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c834a2790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AyhBZwhtIAzhBZw5v+pYh5NgzCJ7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c67e51d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAC+CAYAAADZV4jWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzhBawBlCCzjzX2k9MCXXkrzuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c66feae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(8,2,10), (2,1,5), (2,2,5), (2,3,5), (2,13,5), (2,14,5), (2,15,5), (8,14,5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 10)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 15, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 14, 5)], discount=0.98))\n",
    "for idt,task in enumerate(tasks):\n",
    "  task.plot_grid(title=\"grid_{}\".format(idt) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ERMJb-tFj1vW"
   },
   "outputs": [],
   "source": [
    "class GeneralQ(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, num_offline_updates=30, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._replayBuffer_A = []\n",
    "    if double:\n",
    "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
    "      self._replayBuffer_B = []\n",
    "    self._s = initial_state\n",
    "    self._initial_state = initial_state\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._target_policy = target_policy\n",
    "    self._double = double\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._last_action = 0\n",
    "    self._inventory = set()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_A = []\n",
    "    if self._double:\n",
    "      self._replayBuffer_B = []\n",
    "      \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    if self._double:\n",
    "      return (self._q + self._q2)/2\n",
    "    else:\n",
    "      return self._q\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "\n",
    "  def step(self, r, g, s, item, train):\n",
    "    td = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "      \n",
    "    if self._double:\n",
    "      next_action = self._behaviour_policy(self.q_values[s,:], train)\n",
    "      if np.random.random() <= 0.5:\n",
    "        expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q2[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "          #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q2[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "          self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      else:\n",
    "        expectation = np.sum(self._target_policy(self._q2[s,:], next_action) * self._q[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q2[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])   \n",
    "          #self._q2[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q2[s,:], next_action))] - self._q2[self._s,self._last_action])    \n",
    "          self._replayBuffer_B.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_B[np.random.randint(len(self._replayBuffer_B))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      return self._last_action, self._inventory, td\n",
    "    else:\n",
    "      next_action = self._behaviour_policy(self._q[s,:], train)\n",
    "      # This is expected sarsa, but still functions as expected.\n",
    "      expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q[s,:])\n",
    "      td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "      if train == True:\n",
    "        self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "        self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "        for _ in range(self._num_offline_updates):\n",
    "          replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "          expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "          self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      #print(self._inventory)\n",
    "      return self._last_action, self._inventory, td\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8RECPLMKj1vY"
   },
   "outputs": [],
   "source": [
    "def Q_target_policy(q, a):\n",
    "  return np.eye(len(q))[np.argmax(q)]\n",
    "\n",
    "def SARSA_target_policy(q, a):\n",
    "  return np.eye(len(q))[a]\n",
    "\n",
    "def gen_behaviour_policy(q, train):\n",
    "  #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "  return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMr_z0RZsHNj"
   },
   "source": [
    "An agent that uses **Neural-Sarsa/DQN** to learn action values.  The agent should expect a nxn input which it should flatten into a vector, and then pass through a multi-layer perceptron with a single hidden layer with 100 hidden nodes and ReLU activations.  Each weight layer should also have a bias.  Initialize all weights uniformly randomly in $[-0.05, 0.05]$.\n",
    "\n",
    "```\n",
    "NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "            \n",
    "DQN(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "```\n",
    "\n",
    "The number `vision_size` will be either 1 or 2 below.  The input vector will be of size $(2v + 1)^2$, which will correspond to a square local view of the grid, centered on the agent, and of size $(2v + 1) \\times (2v + 1)$ (so either 3x3 or 5x5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qwHC3_5hj1vb"
   },
   "outputs": [],
   "source": [
    "class NEURAL_CONTROLLER_DRIVER(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, number_of_features_controller,\n",
    "                number_of_features_driver,\n",
    "                number_of_hidden_controller,\n",
    "                number_of_hidden_driver,\n",
    "                number_of_actions_controller,\n",
    "                number_of_actions_driver,\n",
    "                initial_state_controller,\n",
    "                initial_state_driver, \n",
    "                rl_alg_controller='DQN',\n",
    "                rl_alg_driver='DQN', \n",
    "                num_offline_updates_controller=20, \n",
    "                num_offline_updates_driver=25,\n",
    "                step_size_controller=0.01,\n",
    "                step_size_driver=0.01): \n",
    "    # HMMM?\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    self._prev_action_driver = 0\n",
    "    self._step_driver = step_size_driver\n",
    "    self._num_features_driver = number_of_features_driver\n",
    "    self._num_action_driver = number_of_actions_driver\n",
    "    self._num_hidden_driver = number_of_hidden_driver\n",
    "    self._initial_state_driver = initial_state_driver\n",
    "    self._s_driver = initial_state_driver\n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "    self._times_trained_driver = 0\n",
    "    self._inventory = set()\n",
    "    self._replayBuffer_driver = []\n",
    "    self._num_offline_updates_driver = num_offline_updates_driver\n",
    "    self._rl_alg_driver = rl_alg_driver\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    self._prev_action_controller = 0\n",
    "    self._step_controller = step_size_controller\n",
    "    self._num_features_controller = number_of_features_controller\n",
    "    self._num_action_controller = number_of_actions_controller\n",
    "    self._num_hidden_controller = number_of_hidden_controller\n",
    "    self._initial_state_controller = initial_state_controller\n",
    "    self._s_controller = initial_state_controller\n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    self._times_trained_controller = 0\n",
    "    self._replayBuffer_controller = []\n",
    "    self._num_offline_updates_controller = num_offline_updates_controller\n",
    "    self._rl_alg_controller = rl_alg_controller\n",
    "    self.name = 'HYPER '+self._rl_alg_controller\n",
    "  \n",
    "    # ?????????? should it be the number of tasks\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    \n",
    "    \n",
    "    self._times_used = 0.\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "    self.resetReplayBuffer()\n",
    "    self.resetState()\n",
    "    self._times_trained_driver = 0\n",
    "    self._prev_action_driver = 0\n",
    "    self._inventory = set()\n",
    "    self._times_used = 0\n",
    "\n",
    "  def resetReplayBuffer_controller(self):\n",
    "    self._replayBuffer_controller = []\n",
    "    \n",
    "  def resetState_controller(self):\n",
    "    self._s_controller = self._initial_state_controller \n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.disTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_controller = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_controller = tf.placeholder(tf.float64, shape=(1,self._num_features_controller))\n",
    "    self._dense_1_controller = tf.layers.dense(self.stateTensor_controller,\n",
    "                                    self._num_hidden_controller, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_controller = tf.layers.dense(self._dense_1_controller,\n",
    "                                    self._num_action_controller, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_controller = tf.reshape(self._dense_2_controller, (self._num_action_controller,))    \n",
    "    self._softmx_controller = tf.nn.softmax(self._q_controller)\n",
    "    self._cost_controller = tf.losses.mean_squared_error(self.rewTensor_controller + self.disTensor_controller*self.nqTensor_controller, self._q_controller[self.actionTensor_controller])\n",
    "    self._opt_controller = tf.train.RMSPropOptimizer(self._step_controller).minimize(self._cost_controller) \n",
    "    \n",
    "\n",
    "    self.rewTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.disTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_driver = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_driver = tf.placeholder(tf.float64, shape=(1,self._num_features_driver))\n",
    "    self._dense_1_driver = tf.layers.dense(self.stateTensor_driver,\n",
    "                                    self._num_hidden_driver, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_driver = tf.layers.dense(self._dense_1_driver,\n",
    "                                    self._num_action_driver, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_driver = tf.reshape(self._dense_2_driver, (self._num_action_driver,))    \n",
    "    self._cost_driver = tf.losses.mean_squared_error(self.rewTensor_driver+ self.disTensor_driver*self.nqTensor_driver, self._q_driver[self.actionTensor_driver])\n",
    "    self._opt_driver = tf.train.RMSPropOptimizer(self._step_driver).minimize(self._cost_driver)\n",
    "\n",
    "\n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy_controller(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_controller(self, q):    \n",
    "    return epsilon_greedy(q, 0.2)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "\n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs_controller\n",
    "\n",
    "  def q_controller(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t, probs = self._sess.run([self._q_controller, self._softmx_controller], {self.stateTensor_controller: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step_controller(self, r, g, s):\n",
    "    self._times_used += 1\n",
    "    #print self._times_used\n",
    "    qvs, probs = self.q_controller(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs_controller = probs\n",
    "    next_action = self._behaviour_policy_controller(q_nxtState)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg_controller == 'NEURALSARSA':\n",
    "        target = self._target_policy_controller(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "        #print vob\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "      elif self._rl_alg_controller == 'DQN':\n",
    "        # This function should return an action\n",
    "        # Optimiser\n",
    "        vob = np.max(q_nxtState)\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "\n",
    "    self._s_controller = np.reshape(s, (1,-1))\n",
    "    self._prev_action_controller = next_action\n",
    "    \n",
    "    return next_action\n",
    "\n",
    "  def reset_controller(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "\n",
    "\n",
    "\n",
    "    # resetReplayBuffer_driver\n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_driver = []\n",
    "    \n",
    "    # resetState_driver\n",
    "  def resetState(self):\n",
    "    self._s_driver = self._initial_state_driver \n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "\n",
    "\n",
    "  def _target_policy_driver(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_driver(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)\n",
    "\n",
    "  def q_driver(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t = self._sess.run(self._q_driver, {self.stateTensor_driver: obs})\n",
    "    return t\n",
    "  \n",
    "  # step_driver\n",
    "  def step(self, r, g, s, item, train):\n",
    "    cost = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "    \n",
    "    # This function should return an action\n",
    "    q_nxtState = np.reshape(self.q_driver(s), (-1,))\n",
    "    next_action = self._behaviour_policy_driver(q_nxtState, train)\n",
    "    \n",
    "\n",
    "    if self._rl_alg_driver == 'NEURALSARSA':\n",
    "      target = self._target_policy_driver(q_nxtState, next_action)\n",
    "      target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "      \n",
    "      # Optimiser\n",
    "      vob = q_nxtState[target]\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#           self.nqTensor_driver: vob,\n",
    "#           self.rewTensor_driver: r,\n",
    "#           self.disTensor_driver: g,\n",
    "#           self.actionTensor_driver: self._prev_action_driver,\n",
    "#           self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "    elif self._rl_alg_driver == 'DQN':\n",
    "      vob = np.max(q_nxtState)\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#               self.nqTensor_driver: vob,\n",
    "#               self.rewTensor_driver: r,\n",
    "#               self.disTensor_driver: g,\n",
    "#               self.actionTensor_driver: self._prev_action_driver,\n",
    "#               self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "\n",
    "    \n",
    "        \n",
    "    self._s_driver = np.reshape(s, (1,-1))\n",
    "    self._prev_action_driver = next_action\n",
    "    return next_action, self._inventory, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCpz-tYIj1vf"
   },
   "source": [
    "## Agent 0: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_ezyxkw3j1vg"
   },
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "  \n",
    "  This agent returns an action between 0 and 'number_of_arms', \n",
    "  uniformly at random. The 'previous_action' argument of 'step'\n",
    "  is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, number_of_arms):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'random'\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return np.ones((self._number_of_arms))/self._number_of_arms\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    " \n",
    "  def __init__(self, number_of_arms, step_size=0.1, baseline=False):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._lr = step_size\n",
    "    self.name = 'reinforce, baseline: {}'.format(baseline)\n",
    "    self._baseline = baseline\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.total_reward = 0;\n",
    "    self.number_rewards = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      self.number_rewards += 1.\n",
    "      self.total_reward += reward\n",
    "      self.action_values[0,previous_action] += reward\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "      self.updatePreferences(previous_action, reward)\n",
    "#    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "#     if unvisited[0].size > 0:\n",
    "#       return unvisited[0][0]\n",
    "#     else:\n",
    "#       return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.number_rewards = 0.\n",
    "    self.total_reward = 0.\n",
    "  \n",
    "  def updatePreferences(self, previous_action, reward):\n",
    "    if not self._baseline: \n",
    "      self.action_preferences[0,previous_action]+=self._lr*reward*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*reward*self.softmax()[i]\n",
    "    else:\n",
    "      self.action_preferences[0,previous_action]+=self._lr*(reward - self.total_reward/self.number_rewards)*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*(reward - self.total_reward/self.number_rewards)*self.softmax()[i]\n",
    "    \n",
    "  def softmax(self):\n",
    "    q = np.sum(np.exp(self.action_preferences),axis=1)\n",
    "    t = np.exp(self.action_preferences)/q\n",
    "    return t.flatten()\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP3(object):\n",
    "\n",
    "  def __init__(self, number_of_arms, gamma):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'exp3 Gamma: ' + str(gamma)\n",
    "    \n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.weights = np.ones((1,self._number_of_arms))\n",
    "    \n",
    "    self.time = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      xhat = np.zeros((1, self._number_of_arms))\n",
    "      xhat[0,previous_action] = reward/self.action_values[0,previous_action]\n",
    "      self.weights = self.weights*np.exp(self.gamma*xhat/self._number_of_arms)\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "    self.action_values[0,:] = (1-self.gamma)*(self.weights)/(np.sum(self.weights)) + self.gamma/self._number_of_arms\n",
    "    action = np.random.choice(self._number_of_arms, p=self.action_values[0,:])\n",
    "    self.time += 1.\n",
    "    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "    return unvisited[0][0] if unvisited[0].size > 0 else action\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.action_values[0,:]\n",
    "  \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2, self._number_of_arms))\n",
    "    self.weights = np.ones((1, self._number_of_arms))\n",
    "    self.time = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA9ryUOej1v3"
   },
   "source": [
    "## Agent 7: Task Selection via RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EVhim-3Rj1v4"
   },
   "outputs": [],
   "source": [
    "class NS_DQN(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, teacher_student, number_of_features, number_of_hidden, number_of_actions, initial_state, rl_alg='DQN', num_offline_updates=30, step_size=0.01):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    self._prev_action = 0\n",
    "    self._step = step_size\n",
    "    self._num_features = number_of_features\n",
    "    self._num_action = number_of_actions\n",
    "    self._num_hidden = number_of_hidden\n",
    "    self._initial_state = initial_state\n",
    "    self._s = initial_state\n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    self._times_trained = 0\n",
    "    self._replayBuffer = []\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._rl_alg = rl_alg\n",
    "    self._teacher_student = teacher_student\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._inventory = set()\n",
    "    \n",
    "    if self._teacher_student == True:\n",
    "      self.name = 'HYPER '+self._rl_alg\n",
    "    else:\n",
    "      self.name = self._rl_alg\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer = []\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor = tf.placeholder(tf.float64)\n",
    "    self.disTensor = tf.placeholder(tf.float64)\n",
    "    self.nqTensor = tf.placeholder(tf.float64)\n",
    "    self.actionTensor = tf.placeholder(tf.int32)\n",
    "    self.stateTensor = tf.placeholder(tf.float64, shape=(1,self._num_features))\n",
    "    self._dense_1 = tf.layers.dense(self.stateTensor,\n",
    "                                    self._num_hidden, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2 = tf.layers.dense(self._dense_1,\n",
    "                                    self._num_action, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q = tf.reshape(self._dense_2, (self._num_action,))    \n",
    "    self._softmx = tf.nn.softmax(self._q)\n",
    "    self._cost = tf.losses.mean_squared_error(self.rewTensor + self.disTensor*self.nqTensor, self._q[self.actionTensor])\n",
    "    self._opt = tf.train.RMSPropOptimizer(self._step).minimize(self._cost) \n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy(self, q, train):\n",
    "    if self._teacher_student == True:   \n",
    "      return epsilon_greedy(q, 0.2)#if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    else:\n",
    "      return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs\n",
    "  \n",
    "  def q_noProbs(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t = self._sess.run(self._q, {self.stateTensor: obs})\n",
    "    return t\n",
    "  \n",
    "  def q(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t, probs = self._sess.run([self._q, self._softmx], {self.stateTensor: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step(self, r, g, s, item=None, train=True):\n",
    "    cost = None\n",
    "\n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "\n",
    "    qvs, probs = self.q(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs = probs\n",
    "    next_action = self._behaviour_policy(q_nxtState, train)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg == 'NEURALSARSA':\n",
    "        target = self._target_policy(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "      elif self._rl_alg == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "\n",
    "    self._s = np.reshape(s, (1,-1))\n",
    "    self._prev_action = next_action\n",
    "    return next_action, self._inventory, cost\n",
    "\n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState()\n",
    "    self.resetReplayBuffer()\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._times_trained = 0\n",
    "    self._prev_action = 0\n",
    "    self._inventory = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixZUk41Zj1v6"
   },
   "source": [
    "## Task Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nS1RGyMGj1v7"
   },
   "outputs": [],
   "source": [
    "class TaskSelector(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, reward_signal, number_of_tasks_selection_steps):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "    self._tasks_buffer = np.zeros((5,len(tasks)))\n",
    "    self._tasks_buffer_scaled = np.zeros((5,len(tasks)))\n",
    "    self._tasks_episodes_completed_train = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    self._tasks_episodes_completed_test = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    self._time = 0\n",
    "    self._number_of_tasks_selection_steps = number_of_tasks_selection_steps\n",
    "    \n",
    "  def resetReplayBuffer(self):\n",
    "    self._rl_agent.resetReplayBuffer()    \n",
    "  \n",
    "  def step(self, action_task_id):\n",
    "    \n",
    "    if self._reward_signal == 'PG':\n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
    "      \n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_test.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
    "          \n",
    "      \n",
    "    elif self._reward_signal == 'GPG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'SPG':\n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
    "      \n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_test.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
    "          \n",
    "       \n",
    "    elif self._reward_signal == 'TPG':\n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[-1], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
    "      \n",
    "      self._tasks_episodes_completed_test[-1, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[-1, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[-1, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_test.shape[0]-1):\n",
    "        self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
    "          \n",
    "        \n",
    "    \n",
    "    elif self._reward_signal == 'MPG':\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
    "        if x != action_task_id:\n",
    "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
    "      \n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1] \n",
    "      for x in range(self._tasks_episodes_completed_test.shape[0]):\n",
    "        if x != uniform_sampled_task_id:\n",
    "          self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
    "          \n",
    "        \n",
    "        \n",
    "    elif self._reward_signal == 'VCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'GVCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'L2G':\n",
    "      pass      \n",
    "    \n",
    "    self._time += 1\n",
    "    self._tasks_buffer[:,action_task_id] = np.roll(self._tasks_buffer[:,action_task_id], 1)\n",
    "    self._tasks_buffer[0,action_task_id] = reward_after\n",
    "    X = np.arange(self._tasks_buffer.shape[0])\n",
    "    slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,action_task_id])\n",
    "    rhat = slope \n",
    "    \n",
    "    self._unscaled_reward_history.append(rhat)\n",
    "    temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "    p_20 = np.percentile(temp_history, 20)\n",
    "    p_80 = np.percentile(temp_history, 80)        \n",
    "\n",
    "    if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
    "      raise ValueError('Action {} is out of bounds for a '\n",
    "                       '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
    "    \n",
    "    r = None\n",
    "    if rhat <= p_20:\n",
    "      r = -1.\n",
    "    elif rhat > p_80:\n",
    "      r = 1.\n",
    "    else:\n",
    "      r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "      \n",
    "    self._tasks_buffer_scaled[:,action_task_id] = np.roll(self._tasks_buffer_scaled[:,action_task_id], 1)\n",
    "    self._tasks_buffer_scaled[0,action_task_id] = r\n",
    "    \n",
    "    #print reward_steps_after\n",
    "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
    "    return r, reward_after, np.reshape(self._tasks_buffer_scaled.T,(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DrXBQ9NZj1v-"
   },
   "outputs": [],
   "source": [
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  #print vmin, vmax\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
    "#   plt.savefig('./action_values_{}'.format(title))\n",
    "#   plt.close()\n",
    "\n",
    "def plot_greedy_policy(grid, title, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid(title)\n",
    "  plt.hold('on')\n",
    "  for i in range(grid._layout.shape[0]):\n",
    "    for j in range(grid._layout.shape[1]):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_completed_episodes(completed_train, completed_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Train Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_train.shape[0]):    \n",
    "    plot = plt.plot(completed_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Test Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_test.shape[0]):    \n",
    "    plot = plt.plot(completed_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QGZIM_Gcj1wA"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  reward_delta_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  \n",
    "  for alg in algs:\n",
    "    print('Running:', alg.name)\n",
    "    reward_dict[alg.name] = []\n",
    "    reward_delta_dict[alg.name] = []\n",
    "    action_dict[alg.name] = []\n",
    "    prob_dict[alg.name] = []\n",
    "    entropy_dict[alg.name] = []\n",
    "    \n",
    "    rl_agent = None\n",
    "    qs = None\n",
    "    completed_train_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "    completed_test_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "    \n",
    "    for qq in range(repetitions):\n",
    "      print('Rep:', qq)\n",
    "      \n",
    "      if isinstance(alg, NEURAL_CONTROLLER_DRIVER):\n",
    "        alg.reset()\n",
    "        bandit = TaskSelector(alg, tasks, reward_signal, number_of_steps_of_selecting_tasks)\n",
    "\n",
    "        reward_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        capability = alg._initial_state_controller\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          try:\n",
    "            action = alg.step_controller(reward, 0.98, capability)\n",
    "            prob = alg.getProbs()\n",
    "            entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "          except:\n",
    "            raise ValueError(\n",
    "                \"The step function of algorithm `{}` failed.\\\n",
    "                Perhaps you have a bug, such as a typo.\\\n",
    "                Or, perhaps your value estimates or policy has diverged.\\\n",
    "                (E.g., internal quantities may have become NaNs.)\\\n",
    "                Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "          reward, reward_from_environment, capability = bandit.step(action)\n",
    "          bandit.resetReplayBuffer()\n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "        \n",
    "        \n",
    "        h, w = tasks[-1]._layout.shape\n",
    "        obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "        if qs is not None:\n",
    "          qs += np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        else:\n",
    "          qs = np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        \n",
    "      else:\n",
    "        alg.reset()\n",
    "        rl_agent = None\n",
    "        \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          rl_agent = NS_DQN(teacher_student=False,\n",
    "                      number_of_features=(2*vision_size + 1)**2,\n",
    "                      number_of_hidden=hidden_units,\n",
    "                      number_of_actions=4,\n",
    "                      initial_state=tasks[0].get_obs(),\n",
    "                      rl_alg=agent_type_driver,\n",
    "                      step_size=step_size)\n",
    "        elif agent_type_driver == 'Q':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=Q_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        elif agent_type_driver == 'SARSA':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=SARSA_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        \n",
    "        bandit = TaskSelector(rl_agent, tasks, reward_signal, number_of_steps_of_selecting_tasks)\n",
    "        \n",
    "        reward_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        capability = None\n",
    "        if 'HYPER' in alg.name: \n",
    "          capability = alg._initial_state\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          try:\n",
    "            # This is for when the teacher is neural and student not\n",
    "            if 'HYPER' in alg.name:\n",
    "              action, _, _ = alg.step(reward, 0.98, capability)\n",
    "            else:\n",
    "              action = alg.step(action, reward)\n",
    "            prob = alg.getProbs()\n",
    "            entropy = -1.*np.sum(prob*np.log(prob))\n",
    "          except:\n",
    "            raise ValueError(\n",
    "                \"The step function of algorithm `{}` failed.\\\n",
    "                Perhaps you have a bug, such as a typo.\\\n",
    "                Or, perhaps your value estimates or policy has diverged.\\\n",
    "                (E.g., internal quantities may have become NaNs.)\\\n",
    "                Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "          reward, reward_from_environment, capability = bandit.step(action)\n",
    "          #print(reward_from_environment)\n",
    "          bandit.resetReplayBuffer()\n",
    "          \n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          #print(reward_dict[alg.name][-1])\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "          \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          h, w = tasks[-1]._layout.shape\n",
    "          obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "          if qs is not None:\n",
    "            qs += np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "          else:\n",
    "            qs = np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        elif agent_type_driver == 'Q' or agent_type_driver == 'SARSA':\n",
    "          if qs is not None:\n",
    "            qs += rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "          else:\n",
    "            qs = rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "      #print('')      \n",
    "      \n",
    "#       print(completed_train_episodes.shape, bandit._tasks_episodes_completed_train.shape)\n",
    "      completed_train_episodes += bandit._tasks_episodes_completed_train\n",
    "      completed_test_episodes += bandit._tasks_episodes_completed_test\n",
    "    \n",
    "    \n",
    "    completed_train_episodes /= repetitions\n",
    "    completed_test_episodes /= repetitions\n",
    "    qs /= repetitions\n",
    "    \n",
    "    if isinstance(alg, NEURAL_CONTROLLER_DRIVER):\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_driver, alg.name, reward_signal)\n",
    "    else:\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, agent_type_driver, alg.name, reward_signal)\n",
    "    plot_action_values(qs, alg.name)\n",
    "        \n",
    "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
    "  \n",
    "  smoothed_rewards = {}\n",
    "  smoothed_rewards_stds = {}\n",
    "  \n",
    "  smoothed_reward_deltas = {}\n",
    "  smoothed_reward_deltas_stds = {}\n",
    "  \n",
    "  smoothed_actions = {}\n",
    "  \n",
    "  smoothed_probs = {}\n",
    "  \n",
    "  smoothed_entropies = {}\n",
    "  smoothed_entropies_stds = {}\n",
    "  \n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, rewards in reward_delta_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    \n",
    "    plt.figure(figsize=(44,40))\n",
    "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
    "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "#     plt.savefig('./'+agent + ', Reward Signal: {}; {}'.format(reward_signal, agent_type_driver_driver))\n",
    "#     plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Reward')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Average Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Delta')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_reward_deltas[agent] /= repetitions    \n",
    "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Delta Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Maximum Likelihood, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wTRJuSa_j1wC"
   },
   "outputs": [],
   "source": [
    "def run_step(env, agent, train):     \n",
    "    env.resetState()\n",
    "    agent.resetState()\n",
    "    number_of_steps = env.distanceToGoal()\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "      agent_inventory = agent._inventory\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      agent_inventory = agent._inventory\n",
    "    steps_completed = 0\n",
    "    total_reward = 0.\n",
    "    while steps_completed != number_of_steps:\n",
    "      reward, discount, next_state, item = env.step(action, agent_inventory)\n",
    "      \n",
    "      if item != None:\n",
    "        agent._inventory.add(item)\n",
    "        \n",
    "      total_reward += reward\n",
    "      \n",
    "      # Dont want to remove the key on train==True, cuz then cant get reward on train==false, where we record the reward\n",
    "      if reward == 100 and train == False:\n",
    "        agent._inventory.remove('KEY')\n",
    "      \n",
    "      if discount == 0:\n",
    "        #print(total_reward)\n",
    "        return total_reward, total_reward/steps_completed, True\n",
    "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train)\n",
    "      steps_completed += 1\n",
    "    \n",
    "    mean_reward = total_reward/number_of_steps\n",
    "\n",
    "    return total_reward, mean_reward, False\n",
    "  \n",
    "def run_episode(env, agent, number_of_episodes, train):\n",
    "    # Mean Reward across all the episodes( aka all steps )\n",
    "    mean_reward = 0.\n",
    "    \n",
    "    # Mean Duration per episode\n",
    "    mean_duration = 0.\n",
    "    \n",
    "    # List of (Total Reward Per Epsiode)/(Duration)\n",
    "    signal_per_episode = np.zeros((1, number_of_episodes))\n",
    "    reward_per_episode = np.zeros((1, number_of_episodes))\n",
    "    duration_per_episode = np.zeros((1, number_of_episodes))\n",
    "    \n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    \n",
    "    episodes_completed = 0\n",
    "    total_reward_per_episode = 0.\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    i = 0.\n",
    "    while episodes_completed != number_of_episodes:\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      total_reward_per_episode += reward\n",
    "      \n",
    "      if discount == 0:\n",
    "        duration = time.time() - start_time\n",
    "        signal_per_episode[0,episodes_completed] = (total_reward_per_episode/duration)\n",
    "        reward_per_episode[0,episodes_completed] = (total_reward_per_episode)\n",
    "        duration_per_episode[0,episodes_completed] = (duration)\n",
    "        \n",
    "        episodes_completed += 1\n",
    "        mean_duration += (duration - mean_duration)/(episodes_completed)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_reward_per_episode = 0.\n",
    "        \n",
    "      action = agent.step(reward, discount, next_state, train)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "      i += 1.\n",
    "    \n",
    "    mean_signal = np.mean(signal_per_episode)\n",
    "    total_reward = np.sum(reward_per_episode)\n",
    "    total_duration = np.sum(duration_per_episode)\n",
    "\n",
    "    return total_reward, total_duration\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1672I5Z7j1wE"
   },
   "source": [
    "# Reward as Reward Signal for Bandit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK8BVplBj1wE"
   },
   "source": [
    "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
    "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
    "#### NeuralRL Controller does not have inventory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iakrzzD7j1wF"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 250\n",
    "reps = 5\n",
    "reward_signals=['MPG','SPG']\n",
    "drivers = ['SARSA','Q']\n",
    "hidden_units_controller_net = 100\n",
    "hidden_units_driver_net = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "DeQu7zIcj1wH",
    "outputId": "1ab5c018-2824-4637-db25-e678f5c60027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Running:', 'random')\n",
      "('Rep:', 0)\n"
     ]
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "#tasks.append(Hallway(goal_loc = [(8,2,-3), (2,2,5), (2,13,5), (8,13,100)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "\n",
    "tasks.append(Hallway(goal_loc = [(8,2,10), (2,1,5), (2,2,5), (2,3,5), (2,13,5), (2,14,5), (2,15,5), (8,14,100)], discount=0.98))\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "      Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "for driver in drivers:\n",
    "  train_task_agents(agents,\n",
    "                    number_of_arms_tasks,\n",
    "                    number_of_steps_of_selecting_tasks, \n",
    "                    tasks,\n",
    "                    'MPG',\n",
    "                    reps,\n",
    "                    vision_size,\n",
    "                    tabular_grid,\n",
    "                    driver,\n",
    "                    hidden_units_driver_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 18332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4653005,
     "status": "ok",
     "timestamp": 1533071261000,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "CYwswTJ2j1wJ",
    "outputId": "906c4def-0340-4196-e301-9f103849351d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 10)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 15, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 14, 100)], discount=0.98))\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "    EXP3(number_of_arms_tasks, 0.2),\n",
    "    REINFORCE(number_of_arms_tasks, baseline=False),\n",
    "    REINFORCE(number_of_arms_tasks, baseline=True),\n",
    "    NS_DQN(teacher_student=True,\n",
    "           number_of_features=number_of_arms_tasks*5,\n",
    "                      number_of_hidden=hidden_units_controller_net,\n",
    "                      number_of_actions=number_of_arms_tasks,\n",
    "                      initial_state=np.zeros((1,number_of_arms_tasks*5)),\n",
    "                      rl_alg='DQN',\n",
    "                      step_size=step_size),\n",
    "    NS_DQN(teacher_student=True,\n",
    "           number_of_features=number_of_arms_tasks*5,\n",
    "                      number_of_hidden=hidden_units_controller_net,\n",
    "                      number_of_actions=number_of_arms_tasks,\n",
    "                      initial_state=np.zeros((1,number_of_arms_tasks*5)),\n",
    "                      rl_alg='NEURALSARSA',\n",
    "                      step_size=step_size),\n",
    "    Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  for driver in drivers:\n",
    "    train_task_agents(agents,\n",
    "                      number_of_arms_tasks,\n",
    "                      number_of_steps_of_selecting_tasks, \n",
    "                      tasks,\n",
    "                      reward_signal,\n",
    "                      reps,\n",
    "                      vision_size,\n",
    "                      tabular_grid,\n",
    "                      driver,\n",
    "                      hidden_units_driver_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2SsYpc2SRPe4"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 250\n",
    "reps = 2\n",
    "\n",
    "# reward_signals=['MPG','SPG']\n",
    "drivers = ['NEURALSARSA','DQN']\n",
    "\n",
    "reward_signals=['SPG']\n",
    "\n",
    "\n",
    "hidden_units_controller_net = 100\n",
    "hidden_units_driver_net = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8426
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4692933,
     "status": "error",
     "timestamp": 1533075954695,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "dDzz94z_iRXz",
    "outputId": "318059f6-c057-4706-c3a6-ddad069e27a4"
   },
   "outputs": [],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size_controller = 0.01\n",
    "step_size_driver = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 10)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 15, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 14, 100)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'NEURALSARSA',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'DQN',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'DQN',\n",
    "                              'NEURALSARSA',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "    NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'NEURALSARSA',\n",
    "                              'NEURALSARSA',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "#     Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  train_task_agents(agents,\n",
    "                    number_of_arms_tasks,\n",
    "                    number_of_steps_of_selecting_tasks, \n",
    "                    tasks,\n",
    "                    reward_signal,\n",
    "                    reps,\n",
    "                    vision_size,\n",
    "                    tabular_grid)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XBBSjyIFRlD7"
   },
   "outputs": [],
   "source": [
    "agents = [\n",
    "    Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  for driver in drivers:\n",
    "    train_task_agents(agents,\n",
    "                      number_of_arms_tasks,\n",
    "                      number_of_steps_of_selecting_tasks, \n",
    "                      tasks,\n",
    "                      reward_signal,\n",
    "                      reps,\n",
    "                      vision_size,\n",
    "                      tabular_grid,\n",
    "                      driver,\n",
    "                      hidden_units_driver_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Grid_Experiments_Door_Key_LEAN.ipynb",
   "provenance": [
    {
     "file_id": "1a1ONHRz5bcd2rJyLUD53OUMR8npSp0QZ",
     "timestamp": 1522325021849
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1521476023411
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1517660129183
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1517174839485
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1515086437469
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1511098107887
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
