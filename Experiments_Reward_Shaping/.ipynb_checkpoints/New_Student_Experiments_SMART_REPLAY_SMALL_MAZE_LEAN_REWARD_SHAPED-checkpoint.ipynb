{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# New Student Smart Replay Small Maze Experiments in Curriculum Learning\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "26/07/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhnQ3GUk6HWk"
   },
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, width, height, complexity, density, num_goals, goal):\n",
    "        self._maze, self._maze_clean, self._goal_locations = self.maze(width, height, complexity, density, num_goals, goal)\n",
    "        \n",
    "    def maze(self, width=81, height=51, complexity=.75, density=.75, num_goals=1, goal=1):\n",
    "        goal_locations = []\n",
    "        \n",
    "        # Only odd shapes\n",
    "        shape = ((height // 2) * 2 + 1, (width // 2) * 2 + 1)\n",
    "        # Adjust complexity and density relative to maze size\n",
    "        complexity = int(complexity * (5 * (shape[0] + shape[1]))) # number of components\n",
    "        density    = int(density * ((shape[0] // 2) * (shape[1] // 2))) # size of components\n",
    "        # Build actual maze\n",
    "        Z = np.zeros(shape)\n",
    "        # Fill borders\n",
    "        Z[0, :] = Z[-1, :] = 1\n",
    "        Z[:, 0] = Z[:, -1] = 1\n",
    "        # Make aisles\n",
    "        for i in range(density):\n",
    "            x, y = np.random.randint(0, (shape[1] // 2)+1) * 2, np.random.randint(0, (shape[0] // 2)+1) * 2 # pick a random position\n",
    "            Z[y, x] = 1\n",
    "            for j in range(complexity):\n",
    "                neighbours = []\n",
    "                if x > 1:             neighbours.append((y, x - 2))\n",
    "                if x < shape[1] - 2:  neighbours.append((y, x + 2))\n",
    "                if y > 1:             neighbours.append((y - 2, x))\n",
    "                if y < shape[0] - 2:  neighbours.append((y + 2, x))\n",
    "                if len(neighbours):\n",
    "                    y_,x_ = neighbours[np.random.randint(0, len(neighbours))]\n",
    "                    if Z[y_, x_] == 0:\n",
    "                        Z[y_, x_] = 1\n",
    "                        Z[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
    "                        x, y = x_, y_\n",
    "        Z[Z == 1] = -1\n",
    "        Y = np.copy(Z)\n",
    "        for x in range(0, num_goals):\n",
    "            idx = np.random.randint(len(np.where(Z == 0)[0]))\n",
    "            Z[np.where(Z == 0)[0][idx],np.where(Z == 0)[1][idx]] = goal\n",
    "        for e in zip(np.where(Z == goal)[0],np.where(Z == goal)[1]):\n",
    "            goal_locations.append((e[0],e[1],goal))\n",
    "        return Z,Y,goal_locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeGNMcHDj1vL"
   },
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mT38a_chiRWz"
   },
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False, layout=None):\n",
    "    # 10: Key\n",
    "    # -2: Door\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    \n",
    "    self._wall = -1\n",
    "    self._door = -2\n",
    "    self._key = 10\n",
    "    \n",
    "    \n",
    "#     self._layout = np.array([\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       ])\n",
    "    if layout.all() != None:\n",
    "        self._layout = layout\n",
    "    else:\n",
    "        self._layout = np.array([\n",
    "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "              [-1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1,  0,  0, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
    "              [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "              [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    self._goals = set()\n",
    "    self._goal_loc = []\n",
    "    \n",
    "    for e in goal_loc:\n",
    "      #print(self._layout.shape)\n",
    "      #print(e)\n",
    "      self._layout[e[0],e[1]] = e[2]\n",
    "      self._goal_loc.append((e[0],e[1]))\n",
    "      self._goals.add(e[2])\n",
    "    \n",
    "    #self._goal = value\n",
    "    \n",
    "    # row, col format\n",
    "#     self._start_state = (12, 8)\n",
    "    self._start_state = (1, 1)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "    self._distanceToGoal = None\n",
    "    \n",
    "    self._minDistanceStartGoal = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1],\n",
    "                                                          self._goal_loc[-1][0], self._goal_loc[-1][1])\n",
    "    \n",
    "    #self.distanceToGoal()\n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "  \n",
    "  def distanceToGoal(self):\n",
    "    #return np.prod(self._layout.shape) if len(self._goal_loc) > 1 else self._minDistanceStartGoal\n",
    "    #return np.count_nonzero(self._layout != self._wall)\n",
    "    #return np.sum(self._layout.shape)\n",
    "    return np.prod(self._layout.shape)\n",
    "    \n",
    "  def dijkstra(self, cy, cx):\n",
    "    dist = {}\n",
    "    prev = {}\n",
    "    Q = set()\n",
    "    \n",
    "    for r,c in zip(np.where(self._layout != self._wall)[0],np.where(self._layout != self._wall)[1]):\n",
    "      dist[(r,c)] = np.inf\n",
    "      prev[(r,c)] = None\n",
    "      Q.add((r,c))\n",
    "    \n",
    "    dist[(cy,cx)] = 0.\n",
    "    \n",
    "    while len(Q) != 0:\n",
    "      ud = np.inf\n",
    "      u = None\n",
    "      for e in Q:\n",
    "        if dist[e] < ud:\n",
    "          ud = dist[e]\n",
    "          u = e\n",
    "      Q.remove(u)\n",
    "      \n",
    "      neighbors_u = []\n",
    "      if u[0]+1 < self._layout.shape[0] and self._layout[u[0]+1, u[1]] != self._wall and (u[0]+1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]+1, u[1]))\n",
    "      \n",
    "      if u[0]-1 > -1 and self._layout[u[0]-1, u[1]] != self._wall and (u[0]-1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]-1, u[1]))\n",
    "      \n",
    "      if u[1]+1 < self._layout.shape[1] and self._layout[u[0], u[1]+1] != self._wall and (u[0], u[1]+1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]+1))\n",
    "      \n",
    "      if u[1]-1 > -1 and self._layout[u[0], u[1]-1] != self._wall and (u[0], u[1]-1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]-1))\n",
    "        \n",
    "      for neighbor in neighbors_u:\n",
    "        alt = dist[u] + 1.\n",
    "        if alt < dist[neighbor]:\n",
    "          dist[neighbor] = alt\n",
    "          prev[neighbor] = u\n",
    "    \n",
    "    return dist\n",
    "  \n",
    "  def minDistanceTwoPoints(self, cy, cx, dy, dx):\n",
    "    return self._dist[(cy,cx)][(dy,dx)]\n",
    "  \n",
    "  def distanceToNearestGoal(self, new_y, new_x):\n",
    "    nearest_goal = None\n",
    "    min_dist = np.inf\n",
    "    for v in self._dist[(new_y,new_x)]:\n",
    "      if v in self._goal_loc and self._dist[(new_y,new_x)][v] < min_dist:\n",
    "        min_dist = self._dist[(new_y,new_x)][v]\n",
    "        nearest_goal = v\n",
    "    return nearest_goal\n",
    "    \n",
    "  def handleDoor(self):\n",
    "    pass\n",
    "  \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self, title=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if title != None:\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      plt.title(\"The Grid\")\n",
    "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    \n",
    "    for e in self._goals:\n",
    "      if e == self._key:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{K}$\", ha='center', va='center')\n",
    "      elif e > 0:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    y = np.where(self._layout==self._door)[0]\n",
    "    x = np.where(self._layout==self._door)[1]\n",
    "    for i in range(y.shape[0]): \n",
    "      plt.text(x[i], y[i], r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    \n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
    "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
    "      return location\n",
    "\n",
    "  def step(self, action, agent_inventory):\n",
    "    item = None\n",
    "    y, x = self._state\n",
    "        \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == self._key: # a key\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      item = 'KEY'\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "      print('PICKED UP KEY')\n",
    "    elif self._layout[new_y, new_x] == self._door: # a door\n",
    "      reward = 5\n",
    "      if 'KEY' not in agent_inventory:\n",
    "        reward = self._layout[new_y, new_x]\n",
    "        new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] > 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      \n",
    "#       reward = 0.\n",
    "      distToNearestGoal, nearestGoal = self.distanceToNearestGoal(new_y, new_x)\n",
    "      distToNearestGoal = float(distToNearestGoal)\n",
    "      \n",
    "      #print(self._layout[nearestGoal[0],nearestGoal[1]], distToNearestGoal, minDistance, distToNearestGoal/minDistance, -distToNearestGoal/minDistance, np.exp(-distToNearestGoal/minDistance))\n",
    "      reward = self._layout[nearestGoal[0],nearestGoal[1]]*np.exp(-distToNearestGoal)\n",
    "      if self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' not in agent_inventory:\n",
    "        reward = 0.\n",
    "#       elif self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' in agent_inventory:\n",
    "#         print agent_inventory\n",
    "        \n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs(), item\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxFTKIfFj1vP"
   },
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2723
    },
    "colab_type": "code",
    "id": "Hd1tV95Gj1vQ",
    "outputId": "b08857a7-5596-4e30-c70f-5210197ba216"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzvwPUbR+pGmxz5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d63d2710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw1+PATNEC5ghWsAM0QJmiBYw83+g+IO8FyKULwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d646fed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXx4DZogWMEO0gBmiBcwQLWDm/+5ug7xOn6UrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d648e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ocvjwEzRAuYIVrADNECZogWMPN/Cj+DvLGmtDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d6204e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh0+PATNEC5ghWsAM0QJmiBYw838w+oO8mBSfoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d60ac2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5fbf6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5ebddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5d5f210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD98eA2aIFjBDtIAZogXMEC1g5v/kHYO84zJdAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5c5c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVA9fHgNmiBYwQ7SAGaIFzBAtYOb/L6KDvDXP1dkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5b72710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPIDq4ctjwAzRAmaIFjBDtIAZogXM/B+j04O8sM0pnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95d5a0c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tempHallway = Hallway(goal_loc=[(1,1,1)], discount=0.98)\n",
    "\n",
    "# tasks = []\n",
    "\n",
    "# for x in range(0,tempHallway._layout.shape[1]-2,12):\n",
    "#   tasks.append(Hallway(goal_loc=[(r,x+1,1) for r in range(1,tempHallway._layout.shape[0]-1,2)], discount=0.98))\n",
    "# del tasks[0]\n",
    "# del tempHallway\n",
    "\n",
    "# for idt,task in enumerate(tasks):\n",
    "#   task.plot_grid(title=\"grid_{}\".format(idt) )\n",
    "  \n",
    "maze = Maze(15,15,0.,0.,10,101)\n",
    "  \n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = maze._goal_locations, discount=0.98, layout=np.copy(maze._maze_clean)))\n",
    "for goal in maze._goal_locations:\n",
    "    tasks.append(Hallway(goal_loc = [goal], discount=0.98, layout=np.copy(maze._maze_clean)))\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    task.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwHC3_5hj1vb"
   },
   "outputs": [],
   "source": [
    "class NEURAL_TEACHER_STUDENT(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, number_of_features_teacher,\n",
    "                number_of_features_student,\n",
    "                number_of_features_substitute,\n",
    "                number_of_hidden_teacher,\n",
    "                number_of_hidden_student,\n",
    "                number_of_hidden_substitute,\n",
    "                number_of_actions_teacher,\n",
    "                number_of_actions_student,\n",
    "                number_of_actions_substitute,\n",
    "                initial_state_teacher,\n",
    "                initial_state_student, \n",
    "                initial_state_substitute,\n",
    "                rl_alg_teacher='DQN',\n",
    "                rl_alg_student='DQN', \n",
    "                rl_alg_substitute='DQN',\n",
    "                num_offline_updates_teacher=20, \n",
    "                num_offline_updates_student=25,\n",
    "                num_offline_updates_substitute=25,\n",
    "                step_size_teacher=0.01,\n",
    "                step_size_student=0.01,\n",
    "                step_size_substitute=0.01): \n",
    "    # HMMM?\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    self._prev_action_student = 0\n",
    "    self._step_student = step_size_student\n",
    "    self._num_features_student = number_of_features_student\n",
    "    self._num_action_student = number_of_actions_student\n",
    "    self._num_hidden_student = number_of_hidden_student\n",
    "    self._initial_state_student = initial_state_student\n",
    "    self._s_student = initial_state_student\n",
    "    self._s_student = np.reshape(self._s_student, (1,-1))\n",
    "    self._times_trained_student = 0\n",
    "    self._inventory_student = set()\n",
    "    self._replayBuffer_student = []\n",
    "    self._num_offline_updates_student = num_offline_updates_student\n",
    "    self._rl_alg_student = rl_alg_student\n",
    "\n",
    "\n",
    "    self._prev_action_substitute = 0\n",
    "    self._step_substitute = step_size_substitute\n",
    "    self._num_features_substitute = number_of_features_substitute\n",
    "    self._num_action_substitute = number_of_actions_substitute\n",
    "    self._num_hidden_substitute = number_of_hidden_substitute\n",
    "    self._initial_state_substitute = initial_state_substitute\n",
    "    self._s_substitute = initial_state_substitute\n",
    "    self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
    "    self._times_trained_substitute = 0\n",
    "    self._inventory_substitute = set()\n",
    "    self._replayBuffer_substitute = []\n",
    "    self._num_offline_updates_substitute = num_offline_updates_substitute\n",
    "    self._rl_alg_substitute = rl_alg_substitute\n",
    "\n",
    "    \n",
    "    self._prev_action_teacher = 0\n",
    "    self._step_teacher = step_size_teacher\n",
    "    self._num_features_teacher = number_of_features_teacher\n",
    "    self._num_action_teacher = number_of_actions_teacher\n",
    "    self._num_hidden_teacher = number_of_hidden_teacher\n",
    "    self._initial_state_teacher = initial_state_teacher\n",
    "    self._s_teacher = initial_state_teacher\n",
    "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
    "    self._times_trained_teacher = 0\n",
    "    self._replayBuffer_teacher = []\n",
    "    self._num_offline_updates_teacher = num_offline_updates_teacher\n",
    "    self._rl_alg_teacher = rl_alg_teacher\n",
    "    self.name = 'HYPER '+self._rl_alg_teacher\n",
    "  \n",
    "    # ?????????? should it be the number of tasks\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    \n",
    "    \n",
    "    self._times_used = 0.\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_teacher()\n",
    "    self.resetReplayBuffer_teacher()\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    self._times_trained_teacher = 0\n",
    "    self._prev_action_teacher = 0\n",
    "\n",
    "    self.resetReplayBuffer('TRAIN_STUDENT')\n",
    "    self.resetState('TRAIN_STUDENT')\n",
    "    self._times_trained_student = 0\n",
    "    self._prev_action_student = 0\n",
    "    self._inventory_student = set()\n",
    "\n",
    "    self.resetReplayBuffer('TEST_SUBSTITUTE')\n",
    "    self.resetState('TEST_SUBSTITUTE')\n",
    "    self._times_trained_substitute = 0\n",
    "    self._prev_action_substitute = 0\n",
    "    self._inventory_substitute = set()\n",
    "\n",
    "    self._times_used = 0\n",
    "\n",
    "\n",
    "\n",
    "  def resetReplayBuffer_teacher(self):\n",
    "    self._replayBuffer_teacher = []\n",
    "    \n",
    "  def resetState_teacher(self):\n",
    "    self._s_teacher = self._initial_state_teacher \n",
    "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.disTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_teacher = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_teacher = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_teacher = tf.placeholder(tf.float64, shape=(1,self._num_features_teacher))\n",
    "    self._dense_1_teacher = tf.layers.dense(self.stateTensor_teacher,\n",
    "                                    self._num_hidden_teacher, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_teacher = tf.layers.dense(self._dense_1_teacher,\n",
    "                                    self._num_action_teacher, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_teacher = tf.reshape(self._dense_2_teacher, (self._num_action_teacher,))    \n",
    "    self._softmx_teacher = tf.nn.softmax(self._q_teacher)\n",
    "    self._cost_teacher = tf.losses.mean_squared_error(self.rewTensor_teacher + self.disTensor_teacher*self.nqTensor_teacher, self._q_teacher[self.actionTensor_teacher])\n",
    "    self._opt_teacher = tf.train.RMSPropOptimizer(self._step_teacher).minimize(self._cost_teacher) \n",
    "    \n",
    "\n",
    "    self.rewTensor_student = tf.placeholder(tf.float64)\n",
    "    self.disTensor_student = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_student = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_student = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_student = tf.placeholder(tf.float64, shape=(1,self._num_features_student))\n",
    "    self._dense_1_student = tf.layers.dense(self.stateTensor_student,\n",
    "                                    self._num_hidden_student, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_student = tf.layers.dense(self._dense_1_student,\n",
    "                                    self._num_action_student, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_student = tf.reshape(self._dense_2_student, (self._num_action_student,))    \n",
    "    self._cost_student = tf.losses.mean_squared_error(self.rewTensor_student+ self.disTensor_student*self.nqTensor_student, self._q_student[self.actionTensor_student])\n",
    "    self._opt_student = tf.train.RMSPropOptimizer(self._step_student).minimize(self._cost_student)\n",
    "\n",
    "\n",
    "    self.rewTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.disTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_substitute = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_substitute = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_substitute = tf.placeholder(tf.float64, shape=(1,self._num_features_substitute))\n",
    "    self._dense_0_substitute = tf.layers.dense(self.stateTensor_substitute,\n",
    "                                    self._num_hidden_substitute, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_1_substitute = tf.layers.dense(self._dense_0_substitute,\n",
    "                                    self._num_hidden_substitute, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_substitute = tf.layers.dense(self._dense_1_substitute,\n",
    "                                    self._num_action_substitute, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_substitute = tf.reshape(self._dense_2_substitute, (self._num_action_substitute,))    \n",
    "    self._cost_substitute = tf.losses.mean_squared_error(self.rewTensor_substitute+ self.disTensor_substitute*self.nqTensor_substitute, self._q_substitute[self.actionTensor_substitute])\n",
    "    self._opt_substitute = tf.train.RMSPropOptimizer(self._step_substitute).minimize(self._cost_substitute)\n",
    "    \n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy_teacher(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_teacher(self, q):    \n",
    "    return epsilon_greedy(q, 0.1)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "\n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs_teacher\n",
    "\n",
    "  def q_teacher(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t, probs = self._sess.run([self._q_teacher, self._softmx_teacher], {self.stateTensor_teacher: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step_teacher(self, r, g, s, train):\n",
    "    self._times_used += 1\n",
    "    #print self._times_used\n",
    "    qvs, probs = self.q_teacher(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs_teacher = probs\n",
    "    next_action = self._behaviour_policy_teacher(q_nxtState)\n",
    "    \n",
    "    if r != None and train == True:\n",
    "      if self._rl_alg_teacher == 'NEURALSARSA':\n",
    "        target = self._target_policy_teacher(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "        #print vob\n",
    "        self._sess.run(self._opt_teacher,{\n",
    "            self.nqTensor_teacher: vob,\n",
    "            self.rewTensor_teacher: r,\n",
    "            self.disTensor_teacher: g,\n",
    "            self.actionTensor_teacher: self._prev_action_teacher,\n",
    "            self.stateTensor_teacher: self._s_teacher})\n",
    "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_teacher):\n",
    "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
    "          self._sess.run(self._opt_teacher,{\n",
    "              self.nqTensor_teacher: replay[4],\n",
    "              self.rewTensor_teacher: replay[2],\n",
    "              self.disTensor_teacher: replay[3],\n",
    "              self.actionTensor_teacher: replay[1],\n",
    "              self.stateTensor_teacher: replay[0]})\n",
    "      elif self._rl_alg_teacher == 'DQN':\n",
    "        # This function should return an action\n",
    "        # Optimiser\n",
    "        vob = np.max(q_nxtState)\n",
    "        self._sess.run(self._opt_teacher,{\n",
    "            self.nqTensor_teacher: vob,\n",
    "            self.rewTensor_teacher: r,\n",
    "            self.disTensor_teacher: g,\n",
    "            self.actionTensor_teacher: self._prev_action_teacher,\n",
    "            self.stateTensor_teacher: self._s_teacher})\n",
    "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_teacher):\n",
    "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
    "          self._sess.run(self._opt_teacher,{\n",
    "              self.nqTensor_teacher: replay[4],\n",
    "              self.rewTensor_teacher: replay[2],\n",
    "              self.disTensor_teacher: replay[3],\n",
    "              self.actionTensor_teacher: replay[1],\n",
    "              self.stateTensor_teacher: replay[0]})\n",
    "\n",
    "    self._s_teacher = np.reshape(s, (1,-1))\n",
    "    self._prev_action_teacher = next_action\n",
    "    \n",
    "    return next_action\n",
    "\n",
    "  def reset_teacher(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_teacher()\n",
    "    self.resetReplayBuffer_teacher()\n",
    "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
    "    self._times_trained_teacher = 0\n",
    "    self._prev_action_teacher = 0\n",
    "\n",
    "\n",
    "\n",
    "    # resetReplayBuffer_student\n",
    "  def resetReplayBuffer(self, STUDENT_TYPE):\n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      self._replayBuffer_student = []\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      self._replayBuffer_substitute = []\n",
    "\n",
    "    # resetState_student\n",
    "  def resetState(self, STUDENT_TYPE):\n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      self._s_student = self._initial_state_student \n",
    "      self._s_student = np.reshape(self._s_student, (1,-1))\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      self._s_substitute = self._initial_state_substitute\n",
    "      self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
    "\n",
    "\n",
    "  def _target_policy_student(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_student(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.05)\n",
    "\n",
    "  def _target_policy_substitute(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_substitute(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.05)\n",
    "  \n",
    "  def q_student(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t = self._sess.run(self._q_student, {self.stateTensor_student: obs})\n",
    "    return t\n",
    "  \n",
    "  def q_substitute(self, obs):\n",
    "    obs = np.reshape(obs, (1,-1))\n",
    "    t = self._sess.run(self._q_substitute, {self.stateTensor_substitute: obs})\n",
    "    return t\n",
    "\n",
    "  # step_student\n",
    "  def step(self, r, g, s, item, train, STUDENT_TYPE):\n",
    "    cost = None\n",
    "    \n",
    "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
    "      if item != None:\n",
    "        self._inventory_student.add(item)\n",
    "      \n",
    "      # This function should return an action\n",
    "      q_nxtState = np.reshape(self.q_student(s), (-1,))\n",
    "      next_action = self._behaviour_policy_student(q_nxtState, train)\n",
    "      \n",
    "\n",
    "      if self._rl_alg_student == 'NEURALSARSA':\n",
    "        target = self._target_policy_student(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        \n",
    "        # Optimiser\n",
    "        vob = q_nxtState[target]\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_student,{\n",
    "              self.nqTensor_student: vob,\n",
    "              self.rewTensor_student: r,\n",
    "              self.disTensor_student: g,\n",
    "              self.actionTensor_student: self._prev_action_student,\n",
    "              self.stateTensor_student: self._s_student})\n",
    "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_student):\n",
    "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
    "            self._sess.run(self._opt_student,{\n",
    "                self.nqTensor_student: replay[4],\n",
    "                self.rewTensor_student: replay[2],\n",
    "                self.disTensor_student: replay[3],\n",
    "                self.actionTensor_student: replay[1],\n",
    "                self.stateTensor_student: replay[0]})\n",
    "      elif self._rl_alg_student == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_student,{\n",
    "              self.nqTensor_student: vob,\n",
    "              self.rewTensor_student: r,\n",
    "              self.disTensor_student: g,\n",
    "              self.actionTensor_student: self._prev_action_student,\n",
    "              self.stateTensor_student: self._s_student})\n",
    "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_student):\n",
    "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
    "            self._sess.run(self._opt_student,{\n",
    "                self.nqTensor_student: replay[4],\n",
    "                self.rewTensor_student: replay[2],\n",
    "                self.disTensor_student: replay[3],\n",
    "                self.actionTensor_student: replay[1],\n",
    "                self.stateTensor_student: replay[0]})\n",
    "\n",
    "      \n",
    "          \n",
    "      self._s_student = np.reshape(s, (1,-1))\n",
    "      self._prev_action_student = next_action\n",
    "      return next_action, self._inventory_student, cost\n",
    "\n",
    "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
    "      if item != None:\n",
    "        self._inventory_substitute.add(item)\n",
    "      \n",
    "      # This function should return an action\n",
    "      q_nxtState = np.reshape(self.q_substitute(s), (-1,))\n",
    "      next_action = self._behaviour_policy_substitute(q_nxtState, train)\n",
    "      \n",
    "\n",
    "      if self._rl_alg_substitute == 'NEURALSARSA':\n",
    "        target = self._target_policy_substitute(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        \n",
    "        # Optimiser\n",
    "        vob = q_nxtState[target]\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_substitute,{\n",
    "              self.nqTensor_substitute: vob,\n",
    "              self.rewTensor_substitute: r,\n",
    "              self.disTensor_substitute: g,\n",
    "              self.actionTensor_substitute: self._prev_action_substitute,\n",
    "              self.stateTensor_substitute: self._s_substitute})\n",
    "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_substitute):\n",
    "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
    "            self._sess.run(self._opt_substitute,{\n",
    "                self.nqTensor_substitute: replay[4],\n",
    "                self.rewTensor_substitute: replay[2],\n",
    "                self.disTensor_substitute: replay[3],\n",
    "                self.actionTensor_substitute: replay[1],\n",
    "                self.stateTensor_substitute: replay[0]})\n",
    "      elif self._rl_alg_substitute == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt_substitute,{\n",
    "              self.nqTensor_substitute: vob,\n",
    "              self.rewTensor_substitute: r,\n",
    "              self.disTensor_substitute: g,\n",
    "              self.actionTensor_substitute: self._prev_action_substitute,\n",
    "              self.stateTensor_substitute: self._s_substitute})\n",
    "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates_substitute):\n",
    "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
    "            self._sess.run(self._opt_substitute,{\n",
    "                self.nqTensor_substitute: replay[4],\n",
    "                self.rewTensor_substitute: replay[2],\n",
    "                self.disTensor_substitute: replay[3],\n",
    "                self.actionTensor_substitute: replay[1],\n",
    "                self.stateTensor_substitute: replay[0]})\n",
    "          \n",
    "      self._s_substitute = np.reshape(s, (1,-1))\n",
    "      self._prev_action_substitute = next_action\n",
    "      return next_action, self._inventory_substitute, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixZUk41Zj1v6"
   },
   "source": [
    "## Task Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS1RGyMGj1v7"
   },
   "outputs": [],
   "source": [
    "class TaskSelector(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, reward_signal, number_of_tasks_selection_steps):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "    self._tasks_buffer = np.zeros((5,len(tasks)))\n",
    "    self._tasks_buffer_scaled = np.zeros((5,len(tasks)))\n",
    "    self._tasks_episodes_completed_train = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    self._tasks_episodes_completed_test = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    self._time = 0\n",
    "    self._number_of_tasks_selection_steps = number_of_tasks_selection_steps\n",
    "    \n",
    "    self._train_tasks_times_selected = np.zeros((len(tasks)))\n",
    "    self._test_tasks_times_selected = np.zeros((len(tasks)))\n",
    "    self._train_task_accuracy = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    self._test_task_accuracy = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    \n",
    "    self._tasks_slopes = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
    "    \n",
    "  def resetReplayBuffer(self):\n",
    "    self._rl_agent.resetReplayBuffer()    \n",
    "  \n",
    "  def step(self, action_task_id):\n",
    "    if self._reward_signal == 'GPG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'SPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[action_task_id] += 1.\n",
    "      \n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "       \n",
    "    elif self._reward_signal == 'TPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[-1] += 1.\n",
    "      \n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[-1], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[-1, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[-1, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[-1, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "      \n",
    "        \n",
    "    \n",
    "    elif self._reward_signal == 'MPG':\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      \n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[uniform_sampled_task_id] += 1.\n",
    "      \n",
    "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[uniform_sampled_task_id], self._rl_agent, False)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected[:]\n",
    "        \n",
    "    elif self._reward_signal == 'VCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'GVCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'L2G':\n",
    "      pass      \n",
    "    \n",
    "    self._tasks_buffer[:,action_task_id] = np.roll(self._tasks_buffer[:,action_task_id], 1)\n",
    "    self._tasks_buffer[0,action_task_id] = reward_after\n",
    "    X = np.arange(self._tasks_buffer.shape[0])\n",
    "    slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,action_task_id])\n",
    "    rhat = np.abs(slope)\n",
    "    \n",
    "    for i in range(self._tasks_buffer.shape[1]):\n",
    "      slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,i])\n",
    "      self._tasks_slopes[i, self._time] = np.abs(slope)\n",
    "#       print(\"Task: \", i, slope)\n",
    "    \n",
    "    \n",
    "    self._time += 1\n",
    "#     self._unscaled_reward_history.append(rhat)\n",
    "#     temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "#     p_20 = np.percentile(temp_history, 20)\n",
    "#     p_80 = np.percentile(temp_history, 80)        \n",
    "\n",
    "#     if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
    "#       raise ValueError('Action {} is out of bounds for a '\n",
    "#                        '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
    "    \n",
    "#     r = None\n",
    "#     if rhat <= p_20:\n",
    "#       r = -1.\n",
    "#     elif rhat > p_80:\n",
    "#       r = 1.\n",
    "#     else:\n",
    "#       r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "      \n",
    "#     self._tasks_buffer_scaled[:,action_task_id] = np.roll(self._tasks_buffer_scaled[:,action_task_id], 1)\n",
    "#     self._tasks_buffer_scaled[0,action_task_id] = r\n",
    "    \n",
    "    #print reward_steps_after\n",
    "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
    "    return rhat, reward_steps_after, np.reshape(self._tasks_buffer.T,(-1,)), ep_comp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrXBQ9NZj1v-"
   },
   "outputs": [],
   "source": [
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(24, 24))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  #print vmin, vmax\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
    "#   plt.savefig('./action_values_{}'.format(title))\n",
    "#   plt.close()\n",
    "\n",
    "def plot_greedy_policy(grid, title, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid(title)\n",
    "  plt.hold('on')\n",
    "  for i in range(grid._layout.shape[0]):\n",
    "    for j in range(grid._layout.shape[1]):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_slopes(slopes, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Slopes of Tasks; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(slopes.shape[0]):    \n",
    "    plot = plt.plot(slopes[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48UmChiQ_zZa"
   },
   "outputs": [],
   "source": [
    "def plot_task_accuracy(accuracy_train, accuracy_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Train Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_train.shape[0]):    \n",
    "    plot = plt.plot(accuracy_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Test Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_test.shape[0]):    \n",
    "    plot = plt.plot(accuracy_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6Y1XQzF_zZd"
   },
   "outputs": [],
   "source": [
    "def plot_completed_episodes(completed_train, completed_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Train Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_train.shape[0]):    \n",
    "    plot = plt.plot(completed_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Test Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_test.shape[0]):    \n",
    "    plot = plt.plot(completed_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGZIM_Gcj1wA"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  reward_delta_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  \n",
    "  student_type = ['TRAIN_STUDENT', 'TEST_SUBSTITUTE']\n",
    "  alg = algs[0]\n",
    "  \n",
    "  for typ in student_type:\n",
    "    print('Running:', alg.name + ' ' + typ)\n",
    "    reward_dict[alg.name + ' ' + typ] = []\n",
    "    reward_delta_dict[alg.name + ' ' + typ] = []\n",
    "    action_dict[alg.name + ' ' + typ] = []\n",
    "    prob_dict[alg.name + ' ' + typ] = []\n",
    "    entropy_dict[alg.name + ' ' + typ] = []\n",
    "    \n",
    "    qs = None\n",
    "    completed_train_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "    completed_test_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "    train_task_accuracy = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "    test_task_accuracy = np.zeros((len(tasks),number_of_steps_of_selecting_tasks)) \n",
    "    task_slopes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
    "\n",
    "#     alg.reset()\n",
    "    bandit = TaskSelector(alg, tasks, reward_signal, number_of_steps_of_selecting_tasks, typ)\n",
    "\n",
    "    reward_dict[alg.name + ' ' + typ].append([0.])\n",
    "    reward_delta_dict[alg.name + ' ' + typ].append([])\n",
    "    action_dict[alg.name + ' ' + typ].append([])\n",
    "    prob_dict[alg.name + ' ' + typ].append([])\n",
    "    entropy_dict[alg.name + ' ' + typ].append([])\n",
    "    action = None\n",
    "    reward = None\n",
    "    prob = None\n",
    "    entropy = None\n",
    "    reward_delta = None\n",
    "    success_student_episode = False\n",
    "    capability = alg._initial_state_teacher\n",
    "\n",
    "    for i in range(number_of_steps_of_selecting_tasks):\n",
    "      print('Step: ', i)\n",
    "      if typ == 'TRAIN_STUDENT':\n",
    "        action = alg.step_teacher(reward, 0., capability, True) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, True)\n",
    "      elif typ == 'TEST_SUBSTITUTE':\n",
    "        action = alg.step_teacher(reward, 0., capability, False) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, False)\n",
    "      prob = alg.getProbs()\n",
    "      entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "      reward, reward_from_environment, capability, success_student_episode = bandit.step(action)\n",
    "      bandit.resetReplayBuffer()\n",
    "      \n",
    "      reward_dict[alg.name + ' ' + typ][-1].append(reward_from_environment+reward_dict[alg.name + ' ' + typ][-1][-1])\n",
    "      reward_delta_dict[alg.name + ' ' + typ][-1].append(reward)\n",
    "      action_dict[alg.name + ' ' + typ][-1].append(action)\n",
    "      prob_dict[alg.name + ' ' + typ][-1].append(prob.copy())\n",
    "      entropy_dict[alg.name + ' ' + typ][-1].append(entropy)\n",
    "      \n",
    "    if typ == 'TRAIN_STUDENT':\n",
    "      h, w = tasks[-1]._layout.shape\n",
    "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "      qs = np.array([[[alg.q_student(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "    elif typ == 'TEST_SUBSTITUTE':\n",
    "      h, w = tasks[-1]._layout.shape\n",
    "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "      qs = np.array([[[alg.q_substitute(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "\n",
    "    completed_train_episodes = bandit._tasks_episodes_completed_train\n",
    "    completed_test_episodes = bandit._tasks_episodes_completed_test\n",
    "    train_task_accuracy = bandit._train_task_accuracy\n",
    "    test_task_accuracy = bandit._test_task_accuracy\n",
    "    task_slopes += bandit._tasks_slopes\n",
    "    \n",
    "    if typ == 'TRAIN_STUDENT':\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_slopes(task_slopes, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
    "    else:\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "      plot_task_slopes(task_slopes, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
    "\n",
    "    plot_action_values(qs, alg.name + ' ' + typ)\n",
    "        \n",
    "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
    "  smoothed_rewards = {}\n",
    "  smoothed_rewards_stds = {}\n",
    "  smoothed_reward_deltas = {}\n",
    "  smoothed_reward_deltas_stds = {}\n",
    "  smoothed_actions = {}\n",
    "  smoothed_probs = {}\n",
    "  smoothed_entropies = {}\n",
    "  smoothed_entropies_stds = {}\n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, rewards in reward_delta_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    \n",
    "    plt.figure(figsize=(44,40))\n",
    "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
    "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Reward')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Delta')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_reward_deltas[agent] /= repetitions    \n",
    "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTRJuSa_j1wC"
   },
   "outputs": [],
   "source": [
    "def run_step(env, agent, train, student_type):     \n",
    "    env.resetState()\n",
    "    agent.resetState(student_type)\n",
    "    number_of_steps = env.distanceToGoal()\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
    "    steps_completed = 0\n",
    "    total_reward = 0.\n",
    "    while steps_completed != number_of_steps:\n",
    "      reward, discount, next_state, item = env.step(action, agent_inventory)\n",
    "      \n",
    "      if item != None:\n",
    "        if student_type == 'TRAIN_STUDENT':\n",
    "          agent._inventory_student.add(item)\n",
    "        else:\n",
    "          agent._inventory_substitute.add(item)\n",
    "        \n",
    "      total_reward += reward\n",
    "      \n",
    "      # Dont want to remove the key on train==True, cuz then cant get reward on train==false, where we record the reward\n",
    "      if reward == 100 and train == False:\n",
    "        if student_type == 'TRAIN_STUDENT':\n",
    "          agent._inventory_student.remove('KEY')\n",
    "        else:\n",
    "          agent._inventory_substitute.remove('KEY')\n",
    "      \n",
    "      \n",
    "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train, student_type)\n",
    "      \n",
    "      if discount == 0:\n",
    "        #print(total_reward)\n",
    "        print('EPISODE COMPLETED')\n",
    "        return total_reward, total_reward/steps_completed, True\n",
    "      \n",
    "      steps_completed += 1\n",
    "    \n",
    "    mean_reward = total_reward/number_of_steps\n",
    "\n",
    "    return total_reward, mean_reward, False\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1672I5Z7j1wE"
   },
   "source": [
    "# Reward as Reward Signal for Bandit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK8BVplBj1wE"
   },
   "source": [
    "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
    "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
    "#### NeuralRL Controller does not have inventory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SsYpc2SRPe4"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 100\n",
    "reps = 1\n",
    "\n",
    "# reward_signals=['SPG','TPG','MPG']\n",
    "# rl_algs = ['DQN','NEURALSARSA']\n",
    "\n",
    "reward_signals=['SPG']\n",
    "rl_algs = ['DQN']\n",
    "\n",
    "hidden_units_teacher_net = 100\n",
    "hidden_units_student_net = 100\n",
    "hidden_units_substitute_net = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "colab_type": "code",
    "id": "dDzz94z_iRXz",
    "outputId": "bce2d9b1-e97f-44ed-95e4-9c8281c3a93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Running:', 'HYPER DQN TRAIN_STUDENT')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 5 arguments (6 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c43a4fb4ea01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0mreps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                             \u001b[0mvision_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                             tabular_grid)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-867f801b7cf2>\u001b[0m in \u001b[0;36mtrain_task_agents\u001b[0;34m(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_task_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_arms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_of_selecting_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvision_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_type_driver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   \u001b[0mreward_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_delta_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_of_selecting_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvision_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_type_driver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m   \u001b[0msmoothed_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0msmoothed_rewards_stds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-867f801b7cf2>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     alg.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaskSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_of_selecting_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mreward_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 5 arguments (6 given)"
     ]
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size_teacher = 0.01\n",
    "step_size_student = 0.01\n",
    "step_size_substitute = 0.01\n",
    "\n",
    "\n",
    "# tempHallway = Hallway(goal_loc=[(1,1,1)], discount=0.98)\n",
    "\n",
    "# tasks = []\n",
    "\n",
    "# for x in range(0,tempHallway._layout.shape[1]-2,12):\n",
    "#   tasks.append(Hallway(goal_loc=[(r,x+1,5) for r in range(1,tempHallway._layout.shape[0]-1,2)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "# del tasks[0]\n",
    "# del tempHallway\n",
    "\n",
    "# tasks = tasks[::2]\n",
    "\n",
    "# for task in tasks:\n",
    "#   task.plot_grid()\n",
    "  \n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "    for teacher_agent in rl_algs:\n",
    "        for student_agent in ['DQN']:\n",
    "            tasks = []\n",
    "            for goal in maze._goal_locations:\n",
    "                tasks.append(Hallway(goal_loc = [goal], tabular=tabular_grid, vision_size=vision_size, discount=0.98, layout=np.copy(maze._maze_clean)))\n",
    "                \n",
    "            # Intrinsically Motivated Curriculum Learning\n",
    "            number_of_arms_tasks = len(tasks)\n",
    "\n",
    "            agents = [\n",
    "            NEURAL_TEACHER_STUDENT(number_of_arms_tasks*5,\n",
    "                                        (2*vision_size + 1)**2,\n",
    "                                        (2*vision_size + 1)**2,\n",
    "                                        hidden_units_teacher_net,\n",
    "                                        hidden_units_student_net,\n",
    "                                        hidden_units_substitute_net,\n",
    "                                        number_of_arms_tasks,\n",
    "                                        4,\n",
    "                                        4,\n",
    "                                        np.zeros((1,number_of_arms_tasks*5)),\n",
    "                                        tasks[0].get_obs(),\n",
    "                                        tasks[0].get_obs(),\n",
    "                                        teacher_agent,\n",
    "                                        student_agent,\n",
    "                                        student_agent,\n",
    "                                        num_offline_updates_teacher=30, \n",
    "                                        num_offline_updates_student=30,\n",
    "                                        num_offline_updates_substitute=30,\n",
    "                                        step_size_teacher=step_size_teacher,\n",
    "                                        step_size_student=step_size_student,\n",
    "                                        step_size_substitute=step_size_substitute),\n",
    "            ]\n",
    "\n",
    "            agents[0].reset()\n",
    "\n",
    "            train_task_agents(agents,\n",
    "                            number_of_arms_tasks,\n",
    "                            number_of_steps_of_selecting_tasks, \n",
    "                            tasks,\n",
    "                            reward_signal,\n",
    "                            reps,\n",
    "                            vision_size,\n",
    "                            tabular_grid)\n",
    "\n",
    "    \n",
    "for task in tasks:\n",
    "    task.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUmqra3NK3a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWmqCB_D_zZ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "New_Student_Experiments_SMART_REPLAY_SMALL_MAZE_LEAN_REWARD_SHAPED.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
