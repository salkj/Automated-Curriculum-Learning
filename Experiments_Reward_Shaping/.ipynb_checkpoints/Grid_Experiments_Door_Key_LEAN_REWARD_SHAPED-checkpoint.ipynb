{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# Door and Key Experiments in Curriculum Learning\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "26/07/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeGNMcHDj1vL"
   },
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mT38a_chiRWz"
   },
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False):\n",
    "    # 10: Key\n",
    "    # -2: Door\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    \n",
    "    self._wall = -1\n",
    "    self._door = -2\n",
    "    self._key = 10\n",
    "    \n",
    "#     self._layout = np.array([\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -2,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1],\n",
    "#         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       ])\n",
    "    \n",
    "    \n",
    "    \n",
    "#      ROOMS\n",
    "\n",
    "    self._layout = np.array([\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0, -1,  0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0, -1, -1],\n",
    "        [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2, -2, -2, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1,  0,  0,  0, -1, -1,  0,  0,  0,  0, -1, -1,  0,  0,  0, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    self._goals = set()\n",
    "    self._goal_loc = []\n",
    "    \n",
    "    for e in goal_loc:\n",
    "      self._layout[e[0],e[1]] = e[2]\n",
    "      self._goal_loc.append((e[0],e[1]))\n",
    "      self._goals.add(e[2])\n",
    "    \n",
    "    #self._goal = value\n",
    "    \n",
    "    # row, col format\n",
    "#     self._start_state = (12, 8)\n",
    "    self._start_state = (11, 8)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "    self._distanceToGoal = None\n",
    "    \n",
    "    #self.distanceToGoal()\n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "  \n",
    "  def distanceToGoal(self):\n",
    "    return np.prod(self._layout.shape)\n",
    "    #return np.count_nonzero(self._layout != -1)*10\n",
    "  \n",
    "  def maxDistanceTwoPoints(self, cy, cx, dy, dx):\n",
    "    distances = []\n",
    "    stack = []\n",
    "    visited = set()\n",
    "    stack.append((cy, cx, 0))\n",
    "    while len(stack) != 0:\n",
    "      #print len(stack)\n",
    "      cur_row, cur_col, dist = stack.pop()\n",
    "      if (cur_row, cur_col) in visited:\n",
    "        continue\n",
    "      visited.add((cur_row, cur_col))\n",
    "      \n",
    "      if (cur_row, cur_col) == (dy, dx):\n",
    "        distances.append(dist)\n",
    "\n",
    "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
    "        stack.append((cur_row+1, cur_col, dist+1))\n",
    "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
    "        stack.append((cur_row-1, cur_col, dist+1))\n",
    "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
    "        stack.append((cur_row, cur_col+1, dist+1))\n",
    "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
    "        stack.append((cur_row, cur_col-1, dist+1))\n",
    "    \n",
    "    return np.max(np.array(distances))\n",
    "  \n",
    "  def distanceToNearestGoal(self, new_y, new_x):\n",
    "    distances = []\n",
    "    locations = []\n",
    "    stack = []\n",
    "    visited = set()\n",
    "    stack.append((new_y, new_x, 0))\n",
    "    while len(stack) != 0:\n",
    "      #print len(stack)\n",
    "      cur_row, cur_col, dist = stack.pop()\n",
    "      if (cur_row, cur_col) in visited:\n",
    "        continue\n",
    "      visited.add((cur_row, cur_col))\n",
    "      \n",
    "      for e in self._goal_loc:\n",
    "        if (cur_row, cur_col) == e:\n",
    "          distances.append(dist)\n",
    "          locations.append(e)\n",
    "          \n",
    "\n",
    "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
    "        stack.append((cur_row+1, cur_col, dist+1))\n",
    "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
    "        stack.append((cur_row-1, cur_col, dist+1))\n",
    "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
    "        stack.append((cur_row, cur_col+1, dist+1))\n",
    "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
    "        stack.append((cur_row, cur_col-1, dist+1))\n",
    "    \n",
    "    # Has to be the absolute closest person\n",
    "    argmin_dist = np.argmin(np.array(distances))\n",
    "    return distances[argmin_dist], locations[argmin_dist] \n",
    "    \n",
    "  def handleDoor(self):\n",
    "    pass\n",
    "  \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self, title=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if title != None:\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      plt.title(\"The Grid\")\n",
    "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    \n",
    "    for e in self._goals:\n",
    "      if e == self._key:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{K}$\", ha='center', va='center')\n",
    "      elif e > 0:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    y = np.where(self._layout==self._door)[0]\n",
    "    x = np.where(self._layout==self._door)[1]\n",
    "    for i in range(y.shape[0]): \n",
    "      plt.text(x[i], y[i], r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    \n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
    "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
    "      return location\n",
    "\n",
    "  def step(self, action, agent_inventory):\n",
    "    item = None\n",
    "    y, x = self._state\n",
    "        \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == self._key: # a key\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      item = 'KEY'\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    elif self._layout[new_y, new_x] == self._door: # a door\n",
    "      reward = 5\n",
    "      if 'KEY' not in agent_inventory:\n",
    "        reward = self._layout[new_y, new_x]\n",
    "        new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] > 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      distToNearestGoal, nearestGoal = self.distanceToNearestGoal(new_y, new_x)\n",
    "      maxDistance = self.maxDistanceTwoPoints(self._start_state[0], self._start_state[1], nearestGoal[0], nearestGoal[1])    \n",
    "      reward = self._layout[nearestGoal[0],nearestGoal[1]]*np.exp(-distToNearestGoal/maxDistance)\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs(), item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxFTKIfFj1vP"
   },
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2627
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2799,
     "status": "ok",
     "timestamp": 1533076732154,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "Hd1tV95Gj1vQ",
    "outputId": "719ffc2e-7d10-464b-f7be-59e59d693783"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69a1907910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCFawBmiBZwhWsAZogWc+R0QZezFH8HRCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69a17f2e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACD1JREFUeJzt3V9olNkZx/HfU6M0MIS2EU26ZMOiUIpYtUKNpXYT8cIL\na+kmIhRhWVP2RjYiilKktN0LRbwI2AVdxCx7s11XbYjaElv0xtKWliglSK8U1iBLrEETgzRp1tOL\nTmQIk//zvu953nw/EEhm8sw5meTHOcx7nomFEATAj69kPQEA80NoAWcILeAMoQWcIbSAM4QWcIbQ\n5oCZ/cDM/jXD/R+Z2ftpzgnJIbQ5EEL4cwjh24t5DDPbYmZ/NLMhMxs0s4tmVlepOaJyCK1zZras\nQg/1dUkfSmosfoxK+qhCj40KIrSRMrPvmtkdMxs2s8/M7FMze9/M3jSzATM7amZfSOqavK2kdpOZ\n9RVrP5X01dnGCyH0hhCuhBBGQwj/kfSBpO8n9xNioQhthMxsuaTfSeqS9A1Jv5X0k5JvqZP0NUmv\nS3q3eFsoqe2W9HGx9pKk1gVM401J9xZQh4RVZT0BlNUkaVkI4YPi191m9veS+7+U9MsQwn8lycxK\na7dKqgohnCl+fcXM/jGfwc3sO5J+IelHC5k8ksVKG6dvSno05baBks//PRnYMurL1H4+14HNbK2k\nP0h6L4Twl7nWIT2ENk5fSHptym0NJZ/P1JpVrvb1uQxqZo2S/iTp1yGET+ZSg/QR2jj9VdKXZnbA\nzJaZ2Y8lfa/kfpumbrJ2wszeM7MqM3trSm1ZZvaapJuSfhNCOL+YySNZhDZCxa3vW5J+JumppJ9K\nuiZpbPJb5lD7jqQhSXskXZnDsO2S3pD0KzMbMbPnZjay4B8CiTGa4H0ws79JOhtC+DjruSBbrLSR\nMrMfmtnq4vb4bUnrJfVmPS9kj9DG61uS/qn/b48PSWoNIQwu5gHN7OeT294pH7+vxISRDrbHgDOs\ntIAzM56IMjOWYSAjIYSyl/ZmPcY4p6vyRQ9TqklzLGr4HaVdU1pXDttjwBlCCzhDaAFnCC3gDKEF\nnCG0gDOEFnCG0ALOEFrAGUILODNjlw9nj4HsTHf2mJUWcCaRhoH59OhOvmfvfPt6F1I3WRPjIfG8\n1Sx2rKT/hmL++ymtK4eVFnCG0ALOEFrAGUILOENoAWcILeAMoQWcIbSAM4QWcIbQAs7QMABEioYB\nICdoGJhFzIfyY65Z7Fg0DEyPlRZwhtACzhBawBlCCzhDaAFnCC3gDKEFnCG0gDOEFnCG0ALOEFrA\nGbp8gEjR5QPkBF0+s4i5kybmmsWORZfP9FhpAWcILeAMoQWcIbSAM4QWcIbQAs4QWsAZQgs4Q2gB\nZwgt4AwNA0CkaBgAciKRhoGkD4hL6R8Sp4bfEQ0DABaE0ALOEFrAGUILOENoAWcILeAMoQWcIbSA\nM4QWcIbQAs7QMABEioYBICfcNwzE/O71S3lupWPFfPifhgEAiSO0gDOEFnCG0ALOEFrAGUILOENo\nAWcILeAMoQWcIbSAMzQMAJGiYQDICfcNAzEfLF/KNWmORcMAgKgRWkStq6tLDQ0N2rRpkw4ePKht\n27apo6NDL1++zLwmK7O+EMX2mJqst8ctLS1av369zpw5o4mJCTU1NWnPnj06duxYJjVpbY95IQq5\nUFVVpba2Np09eza6mrQQWrizatUqDQwM6MWLF9HVpIHQwp3JberExER0NWkgtHDnyZMnqqurU01N\nTXQ1aSC0cGV8fFyXL1/WgQMHoqtJy6yHK4AsdXV16f79+xoeHlZHR4fu3r2rLVu2vHpFN8uarHDJ\nh5pEatIcK68noqa75EPDABAprtMCOZFIw0DsW6+0tvwxPndZHHqP+XmI/bkrh5UWcIbQAs4QWsAZ\nQgs4Q2gBZwgt4AyhBZwhtIAzhBZwhtACztAwAESKhgEgJ2gYmAUNAzQMTK2hYQDAvCT+djPPnz/X\nkSNH9PjxY61du1aFQkHPnj1TZ2dn0kMDuZT4Srtr1y6Nj4+ru7tbp0+f1urVqzU8PJz0sEBuJRra\nmzdv6vbt22pvb3912759+7RixYokhwVyLdHQ3rlzR2am+vr6V7cVCgWdO3cuyWGBXEvlhajly5en\nMQywJCQa2s2bN0uShoaGNDIyokOHDqm5uVm7d+/WvXv3khwayK1EQ7t9+3Y1NzfrwoULqqmpUWdn\npx48eKCVK1dq3bp1SQ4N5Fbi2+Oenh6NjY2pra1Nhw8f1t69e1VbW5v0sEBuJX6dtlAo6Pz580kP\nAywZNAwAkaJhAMgJGgZmkddD70k/b1J+nzsaBgDMC6EFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcI\nLeAMoQWcoWEAiBQNA0BOJNIwEPu719MwEH/DwFKuKa0rh5UWcIbQAs4QWsAZQgs4Q2gBZwgt4Ayh\nBZwhtIAzhBZwhtACzhBawBm6fIBI0eUD5ARdPgnWxNyxQ5dPvDWldeUk/q8uka6xsTHt379fjY2N\nGh0dVXV1tU6dOpX1tFBBbI9zpqenR/X19Tpx4oTa29v19OnTrKeECiO0OfPw4UPdunVLjx490oYN\nG7Rz586sp4QKI7Q5s2PHDvX396uhoUFbt25VU1NT1lNChRHanNm4caNu3Lih1tZW9ff36/jx41lP\nCRVGaHPm+vXramlp0aVLl3Tx4kX19fVlPSVUGKHNmb6+PvX29kqSamtrtWbNmoxnhErjkk/OVFdX\n6+rVq7p27ZoGBwd18uTJrKeECiO0OXP06NGsp4CEsT0GnKFhAIgUDQNATtAwQE0iNWmOlbea0rpy\nWGkBZwgt4AyhBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDKEFnKHLB4jUdF0+M4YWQHzYHgPO\nEFrAGUILOENoAWcILeDM/wDEvQMADJmMdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f699913f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6999057710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkoMDzPPVGW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6998f6d990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDzLuEoQPNRIDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6998e00c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6998d13e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69dc56b450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCC3gzP8B8dcUtyUf/KkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69a205a790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f699ff33350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACHRJREFUeJzt3FFoVPkVx/HfqYnbwBBKI2os0QeFUsRVV6gR6jYRH3yw\nlm4iQivt1pTugzTiKkrxobAPWpElEApaxCzbl66N2xBjIT7oi6UtLVG2QZZ90FKDBF0lRq2QbDb/\nPjiRa3aSMWZm7v9cvx8QnDs5c08m+fG/zP2fWAhBAPz4WtoNAJgbQgs4Q2gBZwgt4AyhBZwhtIAz\nhNYhM/uemX06y/MfmNl7lewJlUNoHQoh/DWE8J35vIaZVZtZt5n9x8wmzezNAl9z3MzumdnnZvbb\n+ZwPpUNonTGzBSV8uSuSfiJpuMB53pG0Q9IaSa9L+oGZ/bKE58ZLIrSRMLM3zOyqmY2a2Z/M7CMz\ne8/Mvm9mQ2Z2yMyGJXVNHUvUrjezgXztR5K+Xux8IYQvQgidIYS/SZos8CU/lfR+CGE4hDAs6X1J\nb5fmu8V8ENoImFm1pD9L6pL0TUl/lPSjxJcslfQNScslTa12IVHbI+nDfG23pJYStLVa0ieJx5/k\njyFlVWk3AElSo6QFIYTf5R/3mNk/E89/Kek3IYQvJMnMkrWbJFWFEDrzjz82s3+VoKecpNHE49H8\nMaSMlTYOyyTdnnZsKPH/z6cCW0B9gdr/lqCnx5JqE49r88eQMkIbh2FJ35p2rCHx/9lGsQrVLi9B\nT9clrU08Xpc/hpQR2jj8XdKXZrbXzBaY2Q8lfTfxvM1QN1U7YWa/MrMqM3trWu2MzGyhmU19aPWa\nmb2WePoPkt41s2VmtkzSu5I+eOHvCGVDaCOQv/R9S9IvJI1I+rGkPkljU1/yArU/l3Rf0k5JH7/g\nqT+T9D89vTzvl/TEzJbnX/f3+R4GJf1bUl8I4fScvjGUhTEEHycz+4ekkyGED9PuBXFhpY2Emb1p\nZkvyl8c/09NNDf1p94X4ENp4fFtP74WOSNovqSWEcGc+L2hmvzazR2b2cNq/v5SiYaSDy2PAGVZa\nwJlZd0SZGcswkJIQQsFbfUW3Mc7lLv2tCtVU8lzU8DOqdE2yrhAujwFnCC3gDKEFnCG0gDOEFnCG\n0ALOEFrAGUILOENoAWcILeDMrFM+7D0G0jPT3mNWWsCZsgwMzGVGd+pv+M51rvdl6qZqYtwknrWa\n+Z6r3L9DMf/+JOsKYaUFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcILeAMoQWcIbSAMwwMAJFiYADI\nCAYGioh5U37MNfM9FwMDM2OlBZwputICr4pHjx7p4MGDunv3rlatWqVcLqcHDx6oo6Mj7daew0oL\n5G3fvl3j4+Pq6enRiRMntGTJEo2Ojqbd1lcQWkDSpUuXdOXKFbW1tT07tnv3bi1cuDDFrgojtICk\nq1evysxUX1//7Fgul9OpU6dS7KowQgskVFdXp91CUYQWkLRhwwZJ0v379/Xw4UPt379fTU1N2rFj\nh65fv55yd88jtICkLVu2qKmpSWfOnFFtba06Ojp08+ZNLVq0SKtXr067vecQWiCvt7dXY2Njam1t\n1YEDB7Rr1y7V1dWl3dZXcJ8WyMvlcjp9+nTabRTFSgs4w5QPECmmfICMYMqniJgnaWKume+5mPKZ\nGSst4AyhBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDKEFnGFgAIgUAwNARpRlYKDcG8Slym8S\np4afEQMDAF4KoQWcIbSAM4QWcIbQAs4QWsAZQgs4Q2gBZwgt4AyhBZxhYACIFAMDQEa4HxiI+a/X\nv8q9Jc8V8+Z/BgYAlB2hBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDKEFnGFgAIgUAwNARrgf\nGIh5Y/mrXFPJczEwACBqhBZR6+rqUkNDg9avX699+/Zp8+bNam9v1+TkZOo1aSn6QRSXx9SkfXnc\n3NysNWvWqLOzUxMTE2psbNTOnTt1+PDhVGoqdXnMB1HIhKqqKrW2turkyZPR1VQKoYU7ixcv1tDQ\nkJ48eRJdTSUQWrgzdZk6MTERXU0lEFq4c+/ePS1dulS1tbXR1VQCoYUr4+PjOnfunPbu3RtdTaUU\n3VwBpKmrq0s3btzQ6Oio2tvbde3aNW3cuPHZJ7pp1qSFWz7UlKWmkufK6o6omW75MDAARIr7tEBG\nlGVgIPZLr0pd8sf43qWx6T3m9yH2964QVlrAGUILOENoAWcILeAMoQWcIbSAM4QWcIbQAs4QWsAZ\nQgs4w8AAECkGBoCMYGCgCAYGGBiYXsPAAIA5IbSAM4QWcIbQAs4QWsAZQgs4Q2gBZwgt4AyhBZwh\ntIAzDAwAkWJgAMgIBgaKyOqm93K/b1J23zsGBgDMCaEFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcI\nLeAMoQWcYWAAiBQDA0BGlGVgIPa/Xs/AQPwDA69yTbKuEFZawBlCCzhDaAFnCC3gDKEFnCG0gDOE\nFnCG0ALOEFrAGUILOENoAWeY8gEixZQPkBFM+ZSxJuaJHaZ84q1J1hVSNLTwZWxsTHv27NGKFSv0\n+PFj1dTU6Pjx42m3hRLi8jhjent7VV9fr6NHj6qtrU0jIyNpt4QSI7QZc+vWLV2+fFm3b9/W2rVr\ntW3btrRbQokR2ozZunWrBgcH1dDQoE2bNqmxsTHtllBihDZj1q1bp4sXL6qlpUWDg4M6cuRI2i2h\nxAhtxly4cEHNzc3q7u7W2bNnNTAwkHZLKDFCmzEDAwPq7++XJNXV1WnlypUpd4RS45ZPxtTU1Oj8\n+fPq6+vTnTt3dOzYsbRbQokR2ow5dOhQ2i2gzLg8BpxhYACIFAMDQEYwMEBNWWoqea6s1STrCmGl\nBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDKEFnCG0gDOEFnCGKR8gUjNN+cwaWgDx4fIYcIbQ\nAs4QWsAZQgs4Q2gBZ/4PX1APyMJWYscAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69992cdcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFnCG0ALOEFrAmf8Bmdr9yXdIaMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69a240e290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(8,1,5), (8,2,5), (8,3,5), (2,1,5), (2,2,5), (2,3,5), (2,12,5), (2,13,5), (2,14,5), (5,4,5), (8,13,5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(5, 4, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 12, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 13, 5)], discount=0.98))\n",
    "for idt,task in enumerate(tasks):\n",
    "  task.plot_grid(title=\"grid_{}\".format(idt) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ERMJb-tFj1vW"
   },
   "outputs": [],
   "source": [
    "class GeneralQ(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, num_offline_updates=30, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._replayBuffer_A = []\n",
    "    if double:\n",
    "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
    "      self._replayBuffer_B = []\n",
    "    self._s = initial_state\n",
    "    self._initial_state = initial_state\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._target_policy = target_policy\n",
    "    self._double = double\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._last_action = 0\n",
    "    self._inventory = set()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_A = []\n",
    "    if self._double:\n",
    "      self._replayBuffer_B = []\n",
    "      \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    if self._double:\n",
    "      return (self._q + self._q2)/2\n",
    "    else:\n",
    "      return self._q\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "\n",
    "  def step(self, r, g, s, item, train):\n",
    "    td = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "      \n",
    "    if self._double:\n",
    "      next_action = self._behaviour_policy(self.q_values[s,:], train)\n",
    "      if np.random.random() <= 0.5:\n",
    "        expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q2[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "          #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q2[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "          self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      else:\n",
    "        expectation = np.sum(self._target_policy(self._q2[s,:], next_action) * self._q[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q2[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])   \n",
    "          #self._q2[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q2[s,:], next_action))] - self._q2[self._s,self._last_action])    \n",
    "          self._replayBuffer_B.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_B[np.random.randint(len(self._replayBuffer_B))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      return self._last_action, self._inventory, td\n",
    "    else:\n",
    "      next_action = self._behaviour_policy(self._q[s,:], train)\n",
    "      # This is expected sarsa, but still functions as expected.\n",
    "      expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q[s,:])\n",
    "      td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "      if train == True:\n",
    "        self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "        self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "\n",
    "        for _ in range(self._num_offline_updates):\n",
    "          replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "          expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "          self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      #print(self._inventory)\n",
    "      return self._last_action, self._inventory, td\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8RECPLMKj1vY"
   },
   "outputs": [],
   "source": [
    "def Q_target_policy(q, a):\n",
    "  return np.eye(len(q))[np.argmax(q)]\n",
    "\n",
    "def SARSA_target_policy(q, a):\n",
    "  return np.eye(len(q))[a]\n",
    "\n",
    "def gen_behaviour_policy(q, train):\n",
    "  #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "  return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMr_z0RZsHNj"
   },
   "source": [
    "An agent that uses **Neural-Sarsa/DQN** to learn action values.  The agent should expect a nxn input which it should flatten into a vector, and then pass through a multi-layer perceptron with a single hidden layer with 100 hidden nodes and ReLU activations.  Each weight layer should also have a bias.  Initialize all weights uniformly randomly in $[-0.05, 0.05]$.\n",
    "\n",
    "```\n",
    "NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "            \n",
    "DQN(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "```\n",
    "\n",
    "The number `vision_size` will be either 1 or 2 below.  The input vector will be of size $(2v + 1)^2$, which will correspond to a square local view of the grid, centered on the agent, and of size $(2v + 1) \\times (2v + 1)$ (so either 3x3 or 5x5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qwHC3_5hj1vb"
   },
   "outputs": [],
   "source": [
    "class NEURAL_CONTROLLER_DRIVER(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, number_of_features_controller,\n",
    "                number_of_features_driver,\n",
    "                number_of_hidden_controller,\n",
    "                number_of_hidden_driver,\n",
    "                number_of_actions_controller,\n",
    "                number_of_actions_driver,\n",
    "                initial_state_controller,\n",
    "                initial_state_driver, \n",
    "                rl_alg_controller='DQN',\n",
    "                rl_alg_driver='DQN', \n",
    "                num_offline_updates_controller=20, \n",
    "                num_offline_updates_driver=25,\n",
    "                step_size_controller=0.01,\n",
    "                step_size_driver=0.01): \n",
    "    # HMMM?\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    self._prev_action_driver = 0\n",
    "    self._step_driver = step_size_driver\n",
    "    self._num_features_driver = number_of_features_driver\n",
    "    self._num_action_driver = number_of_actions_driver\n",
    "    self._num_hidden_driver = number_of_hidden_driver\n",
    "    self._initial_state_driver = initial_state_driver\n",
    "    self._s_driver = initial_state_driver\n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "    self._times_trained_driver = 0\n",
    "    self._inventory = set()\n",
    "    self._replayBuffer_driver = []\n",
    "    self._num_offline_updates_driver = num_offline_updates_driver\n",
    "    self._rl_alg_driver = rl_alg_driver\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    self._prev_action_controller = 0\n",
    "    self._step_controller = step_size_controller\n",
    "    self._num_features_controller = number_of_features_controller\n",
    "    self._num_action_controller = number_of_actions_controller\n",
    "    self._num_hidden_controller = number_of_hidden_controller\n",
    "    self._initial_state_controller = initial_state_controller\n",
    "    self._s_controller = initial_state_controller\n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    self._times_trained_controller = 0\n",
    "    self._replayBuffer_controller = []\n",
    "    self._num_offline_updates_controller = num_offline_updates_controller\n",
    "    self._rl_alg_controller = rl_alg_controller\n",
    "    self.name = 'HYPER '+self._rl_alg_controller\n",
    "  \n",
    "    # ?????????? should it be the number of tasks\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    \n",
    "    \n",
    "    self._times_used = 0.\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "    self.resetReplayBuffer()\n",
    "    self.resetState()\n",
    "    self._times_trained_driver = 0\n",
    "    self._prev_action_driver = 0\n",
    "    self._inventory = set()\n",
    "    self._times_used = 0\n",
    "\n",
    "  def resetReplayBuffer_controller(self):\n",
    "    self._replayBuffer_controller = []\n",
    "    \n",
    "  def resetState_controller(self):\n",
    "    self._s_controller = self._initial_state_controller \n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.disTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_controller = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_controller = tf.placeholder(tf.float64, shape=(1,self._num_features_controller))\n",
    "    self._dense_1_controller = tf.layers.dense(self.stateTensor_controller,\n",
    "                                    self._num_hidden_controller, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_controller = tf.layers.dense(self._dense_1_controller,\n",
    "                                    self._num_action_controller, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_controller = tf.reshape(self._dense_2_controller, (self._num_action_controller,))    \n",
    "    self._softmx_controller = tf.nn.softmax(self._q_controller)\n",
    "    self._cost_controller = tf.losses.mean_squared_error(self.rewTensor_controller + self.disTensor_controller*self.nqTensor_controller, self._q_controller[self.actionTensor_controller])\n",
    "    self._opt_controller = tf.train.GradientDescentOptimizer(self._step_controller).minimize(self._cost_controller) \n",
    "    \n",
    "\n",
    "    self.rewTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.disTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_driver = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_driver = tf.placeholder(tf.float64, shape=(1,self._num_features_driver))\n",
    "    self._dense_1_driver = tf.layers.dense(self.stateTensor_driver,\n",
    "                                    self._num_hidden_driver, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_driver = tf.layers.dense(self._dense_1_driver,\n",
    "                                    self._num_action_driver, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_driver = tf.reshape(self._dense_2_driver, (self._num_action_driver,))    \n",
    "    self._cost_driver = tf.losses.mean_squared_error(self.rewTensor_driver+ self.disTensor_driver*self.nqTensor_driver, self._q_driver[self.actionTensor_driver])\n",
    "    self._opt_driver = tf.train.GradientDescentOptimizer(self._step_driver).minimize(self._cost_driver)\n",
    "\n",
    "\n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy_controller(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_controller(self, q):    \n",
    "    return epsilon_greedy(q, 0.2)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "\n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs_controller\n",
    "\n",
    "  def q_controller(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t, probs = self._sess.run([self._q_controller, self._softmx_controller], {self.stateTensor_controller: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step_controller(self, r, g, s):\n",
    "    self._times_used += 1\n",
    "    #print self._times_used\n",
    "    qvs, probs = self.q_controller(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs_controller = probs\n",
    "    next_action = self._behaviour_policy_controller(q_nxtState)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg_controller == 'NEURALSARSA':\n",
    "        target = self._target_policy_controller(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "        #print vob\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "      elif self._rl_alg_controller == 'DQN':\n",
    "        # This function should return an action\n",
    "        # Optimiser\n",
    "        vob = np.max(q_nxtState)\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "\n",
    "    self._s_controller = np.reshape(s, (1,-1))\n",
    "    self._prev_action_controller = next_action\n",
    "    \n",
    "    return next_action\n",
    "\n",
    "  def reset_controller(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "\n",
    "\n",
    "\n",
    "    # resetReplayBuffer_driver\n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_driver = []\n",
    "    \n",
    "    # resetState_driver\n",
    "  def resetState(self):\n",
    "    self._s_driver = self._initial_state_driver \n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "\n",
    "\n",
    "  def _target_policy_driver(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_driver(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)\n",
    "\n",
    "  def q_driver(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t = self._sess.run(self._q_driver, {self.stateTensor_driver: obs})\n",
    "    return t\n",
    "  \n",
    "  # step_driver\n",
    "  def step(self, r, g, s, item, train):\n",
    "    cost = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "    \n",
    "    # This function should return an action\n",
    "    q_nxtState = np.reshape(self.q_driver(s), (-1,))\n",
    "    next_action = self._behaviour_policy_driver(q_nxtState, train)\n",
    "    \n",
    "\n",
    "    if self._rl_alg_driver == 'NEURALSARSA':\n",
    "      target = self._target_policy_driver(q_nxtState, next_action)\n",
    "      target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "      \n",
    "      # Optimiser\n",
    "      vob = q_nxtState[target]\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#           self.nqTensor_driver: vob,\n",
    "#           self.rewTensor_driver: r,\n",
    "#           self.disTensor_driver: g,\n",
    "#           self.actionTensor_driver: self._prev_action_driver,\n",
    "#           self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "    elif self._rl_alg_driver == 'DQN':\n",
    "      vob = np.max(q_nxtState)\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#               self.nqTensor_driver: vob,\n",
    "#               self.rewTensor_driver: r,\n",
    "#               self.disTensor_driver: g,\n",
    "#               self.actionTensor_driver: self._prev_action_driver,\n",
    "#               self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "\n",
    "    \n",
    "        \n",
    "    self._s_driver = np.reshape(s, (1,-1))\n",
    "    self._prev_action_driver = next_action\n",
    "    return next_action, self._inventory, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCpz-tYIj1vf"
   },
   "source": [
    "## Agent 0: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_ezyxkw3j1vg"
   },
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "  \n",
    "  This agent returns an action between 0 and 'number_of_arms', \n",
    "  uniformly at random. The 'previous_action' argument of 'step'\n",
    "  is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, number_of_arms):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'random'\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return np.ones((self._number_of_arms))/self._number_of_arms\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 1: REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    " \n",
    "  def __init__(self, number_of_arms, step_size=0.1, baseline=False):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._lr = step_size\n",
    "    self.name = 'reinforce, baseline: {}'.format(baseline)\n",
    "    self._baseline = baseline\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.total_reward = 0;\n",
    "    self.number_rewards = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      self.number_rewards += 1.\n",
    "      self.total_reward += reward\n",
    "      self.action_values[0,previous_action] += reward\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "      self.updatePreferences(previous_action, reward)\n",
    "#    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "#     if unvisited[0].size > 0:\n",
    "#       return unvisited[0][0]\n",
    "#     else:\n",
    "#       return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.number_rewards = 0.\n",
    "    self.total_reward = 0.\n",
    "  \n",
    "  def updatePreferences(self, previous_action, reward):\n",
    "    if not self._baseline: \n",
    "      self.action_preferences[0,previous_action]+=self._lr*reward*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*reward*self.softmax()[i]\n",
    "    else:\n",
    "      self.action_preferences[0,previous_action]+=self._lr*(reward - self.total_reward/self.number_rewards)*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*(reward - self.total_reward/self.number_rewards)*self.softmax()[i]\n",
    "    \n",
    "  def softmax(self):\n",
    "    q = np.sum(np.exp(self.action_preferences),axis=1)\n",
    "    t = np.exp(self.action_preferences)/q\n",
    "    return t.flatten()\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP3(object):\n",
    "\n",
    "  def __init__(self, number_of_arms, gamma):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'exp3 Gamma: ' + str(gamma)\n",
    "    \n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.weights = np.ones((1,self._number_of_arms))\n",
    "    \n",
    "    self.time = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      xhat = np.zeros((1, self._number_of_arms))\n",
    "      xhat[0,previous_action] = reward/self.action_values[0,previous_action]\n",
    "      self.weights = self.weights*np.exp(self.gamma*xhat/self._number_of_arms)\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "    self.action_values[0,:] = (1-self.gamma)*(self.weights)/(np.sum(self.weights)) + self.gamma/self._number_of_arms\n",
    "    action = np.random.choice(self._number_of_arms, p=self.action_values[0,:])\n",
    "    self.time += 1.\n",
    "    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "    return unvisited[0][0] if unvisited[0].size > 0 else action\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.action_values[0,:]\n",
    "  \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2, self._number_of_arms))\n",
    "    self.weights = np.ones((1, self._number_of_arms))\n",
    "    self.time = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA9ryUOej1v3"
   },
   "source": [
    "## Agent 7: Task Selection via RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EVhim-3Rj1v4"
   },
   "outputs": [],
   "source": [
    "class NS_DQN(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, teacher_student, number_of_features, number_of_hidden, number_of_actions, initial_state, rl_alg='DQN', num_offline_updates=30, step_size=0.01):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    self._prev_action = 0\n",
    "    self._step = step_size\n",
    "    self._num_features = number_of_features\n",
    "    self._num_action = number_of_actions\n",
    "    self._num_hidden = number_of_hidden\n",
    "    self._initial_state = initial_state\n",
    "    self._s = initial_state\n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    self._times_trained = 0\n",
    "    self._replayBuffer = []\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._rl_alg = rl_alg\n",
    "    self._teacher_student = teacher_student\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._inventory = set()\n",
    "    \n",
    "    if self._teacher_student == True:\n",
    "      self.name = 'HYPER '+self._rl_alg\n",
    "    else:\n",
    "      self.name = self._rl_alg\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer = []\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor = tf.placeholder(tf.float64)\n",
    "    self.disTensor = tf.placeholder(tf.float64)\n",
    "    self.nqTensor = tf.placeholder(tf.float64)\n",
    "    self.actionTensor = tf.placeholder(tf.int32)\n",
    "    self.stateTensor = tf.placeholder(tf.float64, shape=(1,self._num_features))\n",
    "    self._dense_1 = tf.layers.dense(self.stateTensor,\n",
    "                                    self._num_hidden, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2 = tf.layers.dense(self._dense_1,\n",
    "                                    self._num_action, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q = tf.reshape(self._dense_2, (self._num_action,))    \n",
    "    self._softmx = tf.nn.softmax(self._q)\n",
    "    self._cost = tf.losses.mean_squared_error(self.rewTensor + self.disTensor*self.nqTensor, self._q[self.actionTensor])\n",
    "    self._opt = tf.train.GradientDescentOptimizer(self._step).minimize(self._cost) \n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy(self, q, train):\n",
    "    if self._teacher_student == True:   \n",
    "      return epsilon_greedy(q, 0.2)#if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    else:\n",
    "      return epsilon_greedy(q, 0.2) if train == True else epsilon_greedy(q, 0.05)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs\n",
    "  \n",
    "  def q_noProbs(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t = self._sess.run(self._q, {self.stateTensor: obs})\n",
    "    return t\n",
    "  \n",
    "  def q(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t, probs = self._sess.run([self._q, self._softmx], {self.stateTensor: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step(self, r, g, s, item=None, train=True):\n",
    "    cost = None\n",
    "\n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "\n",
    "    qvs, probs = self.q(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs = probs\n",
    "    next_action = self._behaviour_policy(q_nxtState, train)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg == 'NEURALSARSA':\n",
    "        target = self._target_policy(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "      elif self._rl_alg == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "\n",
    "    self._s = np.reshape(s, (1,-1))\n",
    "    self._prev_action = next_action\n",
    "    return next_action, self._inventory, cost\n",
    "\n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState()\n",
    "    self.resetReplayBuffer()\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._times_trained = 0\n",
    "    self._prev_action = 0\n",
    "    self._inventory = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixZUk41Zj1v6"
   },
   "source": [
    "## Task Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nS1RGyMGj1v7"
   },
   "outputs": [],
   "source": [
    "class TaskSelector(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, reward_signal):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "    self._tasks_buffer = np.zeros((5,len(tasks)))\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._rl_agent.resetReplayBuffer()    \n",
    "  \n",
    "  def step(self, action_task_id):\n",
    "    if self._reward_signal == 'PG':\n",
    "      run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "    elif self._reward_signal == 'GPG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'SPG':\n",
    "      run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after = run_step(self._tasks[action_task_id], self._rl_agent, False)\n",
    "    elif self._reward_signal == 'TPG':\n",
    "      run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after = run_step(self._tasks[-1], self._rl_agent, False)\n",
    "    elif self._reward_signal == 'MPG':\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      run_step(self._tasks[action_task_id], self._rl_agent, True)\n",
    "      reward_after, reward_steps_after = run_step(self._tasks[uniform_sampled_task_id], self._rl_agent, False)\n",
    "    elif self._reward_signal == 'VCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'GVCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'L2G':\n",
    "      pass      \n",
    "    \n",
    "    self._tasks_buffer[:,action_task_id] = np.roll(self._tasks_buffer[:,action_task_id], 1)\n",
    "    self._tasks_buffer[0,action_task_id] = reward_after\n",
    "    X = np.arange(self._tasks_buffer.shape[0])\n",
    "    slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,action_task_id])\n",
    "    rhat = slope \n",
    "    \n",
    "    self._unscaled_reward_history.append(rhat)\n",
    "    temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "    p_20 = np.percentile(temp_history, 20)\n",
    "    p_80 = np.percentile(temp_history, 80)        \n",
    "\n",
    "    if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
    "      raise ValueError('Action {} is out of bounds for a '\n",
    "                       '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
    "    \n",
    "    r = None\n",
    "    if rhat <= p_20:\n",
    "      r = -1.\n",
    "    elif rhat > p_80:\n",
    "      r = 1.\n",
    "    else:\n",
    "      r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "    \n",
    "    #print reward_steps_after\n",
    "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
    "    return r, reward_after, np.reshape(self._tasks_buffer.T,(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DrXBQ9NZj1v-"
   },
   "outputs": [],
   "source": [
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  #print vmin, vmax\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
    "#   plt.savefig('./action_values_{}'.format(title))\n",
    "#   plt.close()\n",
    "\n",
    "def plot_greedy_policy(grid, title, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid(title)\n",
    "  plt.hold('on')\n",
    "  for i in range(grid._layout.shape[0]):\n",
    "    for j in range(grid._layout.shape[1]):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QGZIM_Gcj1wA"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  reward_delta_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  \n",
    "  for alg in algs:\n",
    "    print('Running:', alg.name)\n",
    "    reward_dict[alg.name] = []\n",
    "    reward_delta_dict[alg.name] = []\n",
    "    action_dict[alg.name] = []\n",
    "    prob_dict[alg.name] = []\n",
    "    entropy_dict[alg.name] = []\n",
    "    \n",
    "    rl_agent = None\n",
    "    qs = None\n",
    "    \n",
    "    for qq in range(repetitions):\n",
    "      print('Rep:', qq)\n",
    "      \n",
    "      if isinstance(alg, NEURAL_CONTROLLER_DRIVER):\n",
    "        alg.reset()\n",
    "        bandit = TaskSelector(alg, tasks, reward_signal)\n",
    "\n",
    "        reward_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        capability = alg._initial_state_controller\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          try:\n",
    "            action = alg.step_controller(reward, 0.98, capability)\n",
    "            prob = alg.getProbs()\n",
    "            entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "          except:\n",
    "            raise ValueError(\n",
    "                \"The step function of algorithm `{}` failed.\\\n",
    "                Perhaps you have a bug, such as a typo.\\\n",
    "                Or, perhaps your value estimates or policy has diverged.\\\n",
    "                (E.g., internal quantities may have become NaNs.)\\\n",
    "                Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "          reward, reward_from_environment, capability = bandit.step(action)\n",
    "          bandit.resetReplayBuffer()\n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "        \n",
    "        h, w = tasks[-1]._layout.shape\n",
    "        obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "        if qs is not None:\n",
    "          qs += np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        else:\n",
    "          qs = np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "\n",
    "      else:\n",
    "        alg.reset()\n",
    "        rl_agent = None\n",
    "        \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          rl_agent = NS_DQN(teacher_student=False,\n",
    "                      number_of_features=(2*vision_size + 1)**2,\n",
    "                      number_of_hidden=hidden_units,\n",
    "                      number_of_actions=4,\n",
    "                      initial_state=tasks[0].get_obs(),\n",
    "                      rl_alg=agent_type_driver,\n",
    "                      step_size=step_size)\n",
    "        elif agent_type_driver == 'Q':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=Q_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        elif agent_type_driver == 'SARSA':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=SARSA_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        \n",
    "        bandit = TaskSelector(rl_agent, tasks, reward_signal)\n",
    "        \n",
    "        reward_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        capability = None\n",
    "        if 'HYPER' in alg.name: \n",
    "          capability = alg._initial_state\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          try:\n",
    "            # This is for when the teacher is neural and student not\n",
    "            if 'HYPER' in alg.name:\n",
    "              action, _, _ = alg.step(reward, 0.98, capability)\n",
    "            else:\n",
    "              action = alg.step(action, reward)\n",
    "            prob = alg.getProbs()\n",
    "            entropy = -1.*np.sum(prob*np.log(prob))\n",
    "          except:\n",
    "            raise ValueError(\n",
    "                \"The step function of algorithm `{}` failed.\\\n",
    "                Perhaps you have a bug, such as a typo.\\\n",
    "                Or, perhaps your value estimates or policy has diverged.\\\n",
    "                (E.g., internal quantities may have become NaNs.)\\\n",
    "                Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "          reward, reward_from_environment, capability = bandit.step(action)\n",
    "          #print reward_from_environment\n",
    "          bandit.resetReplayBuffer()\n",
    "          \n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          #print reward_dict[alg.name][-1]\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "          \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          h, w = tasks[-1]._layout.shape\n",
    "          obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "          if qs is not None:\n",
    "            qs += np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "          else:\n",
    "            qs = np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        elif agent_type_driver == 'Q' or agent_type_driver == 'SARSA':\n",
    "          if qs is not None:\n",
    "            qs += rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "          else:\n",
    "            qs = rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "      #print('')      \n",
    "    \n",
    "    qs /= repetitions\n",
    "    plot_action_values(qs, alg.name)\n",
    "        \n",
    "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
    "  \n",
    "  smoothed_rewards = {}\n",
    "  smoothed_rewards_stds = {}\n",
    "  \n",
    "  smoothed_reward_deltas = {}\n",
    "  smoothed_reward_deltas_stds = {}\n",
    "  \n",
    "  smoothed_actions = {}\n",
    "  \n",
    "  smoothed_probs = {}\n",
    "  \n",
    "  smoothed_entropies = {}\n",
    "  smoothed_entropies_stds = {}\n",
    "  \n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, rewards in reward_delta_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    \n",
    "    plt.figure(figsize=(44,40))\n",
    "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
    "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "#     plt.savefig('./'+agent + ', Reward Signal: {}; {}'.format(reward_signal, agent_type_driver_driver))\n",
    "#     plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Reward')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Average Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Delta')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_reward_deltas[agent] /= repetitions    \n",
    "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Delta Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Maximum Likelihood, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wTRJuSa_j1wC"
   },
   "outputs": [],
   "source": [
    "def run_step(env, agent, train):     \n",
    "    env.resetState()\n",
    "    agent.resetState()\n",
    "    number_of_steps = env.distanceToGoal()\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "      agent_inventory = agent._inventory\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      agent_inventory = agent._inventory\n",
    "    steps_completed = 0\n",
    "    total_reward = 0.\n",
    "    while steps_completed != number_of_steps:\n",
    "      reward, discount, next_state, item = env.step(action, agent_inventory)\n",
    "      \n",
    "      if item != None:\n",
    "        agent._inventory.add(item)\n",
    "        \n",
    "      total_reward += reward\n",
    "      \n",
    "      # Dont want to remove the key on train==True, cuz then cant get reward on train==false, where we record the reward\n",
    "      if reward == 100 and train == False:\n",
    "        agent._inventory.remove('KEY')\n",
    "      \n",
    "      if discount == 0:\n",
    "        return total_reward, total_reward/steps_completed\n",
    "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train)\n",
    "      steps_completed += 1\n",
    "    \n",
    "    mean_reward = total_reward/number_of_steps\n",
    "\n",
    "    return total_reward, mean_reward\n",
    "  \n",
    "def run_episode(env, agent, number_of_episodes, train):\n",
    "    # Mean Reward across all the episodes( aka all steps )\n",
    "    mean_reward = 0.\n",
    "    \n",
    "    # Mean Duration per episode\n",
    "    mean_duration = 0.\n",
    "    \n",
    "    # List of (Total Reward Per Epsiode)/(Duration)\n",
    "    signal_per_episode = np.zeros((1, number_of_episodes))\n",
    "    reward_per_episode = np.zeros((1, number_of_episodes))\n",
    "    duration_per_episode = np.zeros((1, number_of_episodes))\n",
    "    \n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    \n",
    "    episodes_completed = 0\n",
    "    total_reward_per_episode = 0.\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    i = 0.\n",
    "    while episodes_completed != number_of_episodes:\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      total_reward_per_episode += reward\n",
    "      \n",
    "      if discount == 0:\n",
    "        duration = time.time() - start_time\n",
    "        signal_per_episode[0,episodes_completed] = (total_reward_per_episode/duration)\n",
    "        reward_per_episode[0,episodes_completed] = (total_reward_per_episode)\n",
    "        duration_per_episode[0,episodes_completed] = (duration)\n",
    "        \n",
    "        episodes_completed += 1\n",
    "        mean_duration += (duration - mean_duration)/(episodes_completed)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_reward_per_episode = 0.\n",
    "        \n",
    "      action = agent.step(reward, discount, next_state, train)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "      i += 1.\n",
    "    \n",
    "    mean_signal = np.mean(signal_per_episode)\n",
    "    total_reward = np.sum(reward_per_episode)\n",
    "    total_duration = np.sum(duration_per_episode)\n",
    "\n",
    "    return total_reward, total_duration\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1672I5Z7j1wE"
   },
   "source": [
    "# Reward as Reward Signal for Bandit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK8BVplBj1wE"
   },
   "source": [
    "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
    "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
    "#### NeuralRL Controller does not have inventory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iakrzzD7j1wF"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 800\n",
    "reps = 5\n",
    "reward_signals=['MPG','SPG']\n",
    "drivers = ['SARSA','Q']\n",
    "hidden_units_controller_net = 100\n",
    "hidden_units_driver_net = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "DeQu7zIcj1wH",
    "outputId": "1ab5c018-2824-4637-db25-e678f5c60027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Running:', 'random')\n",
      "('Rep:', 0)\n",
      "('Rep:', 1)\n"
     ]
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "#tasks.append(Hallway(goal_loc = [(8,2,-3), (2,2,5), (2,13,5), (8,13,100)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "\n",
    "tasks.append(Hallway(goal_loc = [(8,1,5), (8,2,10), (8,3,5), (2,1,5), (2,2,5), (2,3,5), (2,12,5), (2,13,5), (2,14,5), (5,4,5), (8,13,100)], discount=0.98))\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "      Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "for driver in drivers:\n",
    "  train_task_agents(agents,\n",
    "                    number_of_arms_tasks,\n",
    "                    number_of_steps_of_selecting_tasks, \n",
    "                    tasks,\n",
    "                    'MPG',\n",
    "                    reps,\n",
    "                    vision_size,\n",
    "                    tabular_grid,\n",
    "                    driver,\n",
    "                    hidden_units_driver_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 18332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4653005,
     "status": "ok",
     "timestamp": 1533071261000,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "CYwswTJ2j1wJ",
    "outputId": "906c4def-0340-4196-e301-9f103849351d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(5, 4, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 10)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 12, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 13, 100)], discount=0.98))\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "    EXP3(number_of_arms_tasks, 0.2),\n",
    "    REINFORCE(number_of_arms_tasks, baseline=False),\n",
    "    REINFORCE(number_of_arms_tasks, baseline=True),\n",
    "    NS_DQN(teacher_student=True,\n",
    "           number_of_features=number_of_arms_tasks*5,\n",
    "                      number_of_hidden=hidden_units_controller_net,\n",
    "                      number_of_actions=number_of_arms_tasks,\n",
    "                      initial_state=np.zeros((1,number_of_arms_tasks*5)),\n",
    "                      rl_alg='DQN',\n",
    "                      step_size=step_size),\n",
    "    NS_DQN(teacher_student=True,\n",
    "           number_of_features=number_of_arms_tasks*5,\n",
    "                      number_of_hidden=hidden_units_controller_net,\n",
    "                      number_of_actions=number_of_arms_tasks,\n",
    "                      initial_state=np.zeros((1,number_of_arms_tasks*5)),\n",
    "                      rl_alg='NEURALSARSA',\n",
    "                      step_size=step_size),\n",
    "    Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  for driver in drivers:\n",
    "    train_task_agents(agents,\n",
    "                      number_of_arms_tasks,\n",
    "                      number_of_steps_of_selecting_tasks, \n",
    "                      tasks,\n",
    "                      reward_signal,\n",
    "                      reps,\n",
    "                      vision_size,\n",
    "                      tabular_grid,\n",
    "                      driver,\n",
    "                      hidden_units_driver_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2SsYpc2SRPe4"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 500\n",
    "reps = 5\n",
    "\n",
    "# reward_signals=['MPG','SPG']\n",
    "# drivers = ['NEURALSARSA','DQN']\n",
    "\n",
    "reward_signals=['MPG','SPG']\n",
    "\n",
    "\n",
    "hidden_units_controller_net = 100\n",
    "hidden_units_driver_net = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8426
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4692933,
     "status": "error",
     "timestamp": 1533075954695,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "dDzz94z_iRXz",
    "outputId": "318059f6-c057-4706-c3a6-ddad069e27a4"
   },
   "outputs": [],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "step_size_controller = 0.01\n",
    "step_size_driver = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = [(5, 4, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 1, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 2, 10)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 3, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 1, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 2, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 3, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 12, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 13, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(2, 14, 5)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "tasks.append(Hallway(goal_loc = [(8, 13, 100)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
    "\n",
    "for task in tasks:\n",
    "  task.plot_grid()\n",
    "  \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks)\n",
    "\n",
    "agents = [\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'NEURALSARSA',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'DQN',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'DQN',\n",
    "                              'NEURALSARSA',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "    NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*5,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*5)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'NEURALSARSA',\n",
    "                              'NEURALSARSA',\n",
    "                              num_offline_updates_controller=25, \n",
    "                              num_offline_updates_driver=25,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "#     Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  train_task_agents(agents,\n",
    "                    number_of_arms_tasks,\n",
    "                    number_of_steps_of_selecting_tasks, \n",
    "                    tasks,\n",
    "                    reward_signal,\n",
    "                    reps,\n",
    "                    vision_size,\n",
    "                    tabular_grid)\n",
    "        \n",
    "# agents = [\n",
    "#     Random(number_of_arms_tasks),\n",
    "# ]\n",
    "\n",
    "\n",
    "# for reward_signal in reward_signals:\n",
    "#   for driver in drivers:\n",
    "#     train_task_agents(agents,\n",
    "#                       number_of_arms_tasks,\n",
    "#                       number_of_steps_of_selecting_tasks, \n",
    "#                       tasks,\n",
    "#                       reward_signal,\n",
    "#                       reps,\n",
    "#                       vision_size,\n",
    "#                       tabular_grid,\n",
    "#                       driver,\n",
    "#                       hidden_units_driver_net)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XBBSjyIFRlD7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Grid_Experiments_Door_Key_LEAN.ipynb",
   "provenance": [
    {
     "file_id": "1a1ONHRz5bcd2rJyLUD53OUMR8npSp0QZ",
     "timestamp": 1522325021849
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1521476023411
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1517660129183
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1517174839485
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1515086437469
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1511098107887
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
