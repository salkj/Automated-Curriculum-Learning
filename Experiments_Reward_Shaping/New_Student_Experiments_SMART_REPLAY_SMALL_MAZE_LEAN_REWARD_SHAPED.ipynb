{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_Student_Experiments_SMART_REPLAY_SMALL_MAZE_LEAN_REWARD_SHAPED.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "pYs6LMEbNqoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Student Smart Replay Small Maze Experiments in Curriculum Learning\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "Salkey, Jayson\n",
        "\n",
        "26/07/2018\n",
        "\n",
        "-----------------------------------\n"
      ]
    },
    {
      "metadata": {
        "id": "ztQEQvnKh2t6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0tQ4aiAaIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "metadata": {
        "id": "YzYtxi8Wh5SJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from collections import namedtuple\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NDhSYfSDcCC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set options"
      ]
    },
    {
      "metadata": {
        "id": "Ps5OnkPmDbMX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "np.set_printoptions(precision=3, suppress=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RhnQ3GUk6HWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Maze(object):\n",
        "    def __init__(self, width, height, complexity, density, num_goals, goal):\n",
        "        self._maze, self._maze_clean, self._goal_locations = self.maze(width, height, complexity, density, num_goals, goal)\n",
        "        \n",
        "    def maze(self, width=81, height=51, complexity=.75, density=.75, num_goals=1, goal=1):\n",
        "        goal_locations = []\n",
        "        \n",
        "        # Only odd shapes\n",
        "        shape = ((height // 2) * 2 + 1, (width // 2) * 2 + 1)\n",
        "        # Adjust complexity and density relative to maze size\n",
        "        complexity = int(complexity * (5 * (shape[0] + shape[1]))) # number of components\n",
        "        density    = int(density * ((shape[0] // 2) * (shape[1] // 2))) # size of components\n",
        "        # Build actual maze\n",
        "        Z = np.zeros(shape)\n",
        "        # Fill borders\n",
        "        Z[0, :] = Z[-1, :] = 1\n",
        "        Z[:, 0] = Z[:, -1] = 1\n",
        "        # Make aisles\n",
        "        for i in range(density):\n",
        "            x, y = np.random.randint(0, (shape[1] // 2)+1) * 2, np.random.randint(0, (shape[0] // 2)+1) * 2 # pick a random position\n",
        "            Z[y, x] = 1\n",
        "            for j in range(complexity):\n",
        "                neighbours = []\n",
        "                if x > 1:             neighbours.append((y, x - 2))\n",
        "                if x < shape[1] - 2:  neighbours.append((y, x + 2))\n",
        "                if y > 1:             neighbours.append((y - 2, x))\n",
        "                if y < shape[0] - 2:  neighbours.append((y + 2, x))\n",
        "                if len(neighbours):\n",
        "                    y_,x_ = neighbours[np.random.randint(0, len(neighbours))]\n",
        "                    if Z[y_, x_] == 0:\n",
        "                        Z[y_, x_] = 1\n",
        "                        Z[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
        "                        x, y = x_, y_\n",
        "        Z[Z == 1] = -1\n",
        "        Y = np.copy(Z)\n",
        "        for x in range(0, num_goals):\n",
        "            idx = np.random.randint(len(np.where(Z == 0)[0]))\n",
        "            Z[np.where(Z == 0)[0][idx],np.where(Z == 0)[1][idx]] = goal\n",
        "        for e in zip(np.where(Z == goal)[0],np.where(Z == goal)[1]):\n",
        "            goal_locations.append((e[0],e[1],goal))\n",
        "        return Z,Y,goal_locations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WeGNMcHDj1vL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A hallway world"
      ]
    },
    {
      "metadata": {
        "id": "mT38a_chiRWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Hallway(object):\n",
        "\n",
        "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False, layout=None):\n",
        "    # 10: Key\n",
        "    # -2: Door\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    \n",
        "    self._wall = -1\n",
        "    self._door = -2\n",
        "    self._key = 10\n",
        "    \n",
        "    \n",
        "#     self._layout = np.array([\n",
        "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "#         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "#       ])\n",
        "    if layout.all() != None:\n",
        "        self._layout = layout\n",
        "    else:\n",
        "        self._layout = np.array([\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0, -1, -1, -1, -1,  0,  0, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1],\n",
        "              [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
        "              [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "            ])\n",
        "    \n",
        "    \n",
        "    self._goals = set()\n",
        "    self._goal_loc = []\n",
        "    \n",
        "    for e in goal_loc:\n",
        "      #print(self._layout.shape)\n",
        "      #print(e)\n",
        "      self._layout[e[0],e[1]] = e[2]\n",
        "      self._goal_loc.append((e[0],e[1]))\n",
        "      self._goals.add(e[2])\n",
        "    \n",
        "    #self._goal = value\n",
        "    \n",
        "    # row, col format\n",
        "#     self._start_state = (12, 8)\n",
        "    self._start_state = (1, 1)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "    self._tabular = tabular\n",
        "    self._vision_size = vision_size\n",
        "    self._discount = discount\n",
        "    self._distanceToGoal = None\n",
        "    \n",
        "    self._minDistanceStartGoal = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1],\n",
        "                                                          self._goal_loc[-1][0], self._goal_loc[-1][1])\n",
        "    \n",
        "    #self.distanceToGoal()\n",
        "  def resetState(self):\n",
        "    self._state = self._start_state\n",
        "  \n",
        "  def distanceToGoal(self):\n",
        "    #return np.prod(self._layout.shape) if len(self._goal_loc) > 1 else self._minDistanceStartGoal\n",
        "    #return np.count_nonzero(self._layout != self._wall)\n",
        "    #return np.sum(self._layout.shape)\n",
        "    return np.prod(self._layout.shape)\n",
        "    \n",
        "  def minDistanceTwoPoints(self, cy, cx, dy, dx):\n",
        "    distances = []\n",
        "    stack = []\n",
        "    visited = set()\n",
        "    stack.append((cy, cx, 0))\n",
        "    while len(stack) != 0:\n",
        "      #print len(stack)\n",
        "      cur_row, cur_col, dist = stack.pop()\n",
        "      if (cur_row, cur_col) in visited:\n",
        "        continue\n",
        "      visited.add((cur_row, cur_col))\n",
        "      \n",
        "      if (cur_row, cur_col) == (dy, dx):\n",
        "        distances.append(dist)\n",
        "\n",
        "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
        "        stack.append((cur_row+1, cur_col, dist+1))\n",
        "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
        "        stack.append((cur_row-1, cur_col, dist+1))\n",
        "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
        "        stack.append((cur_row, cur_col+1, dist+1))\n",
        "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
        "        stack.append((cur_row, cur_col-1, dist+1))\n",
        "    \n",
        "    return np.min(np.array(distances))\n",
        "  \n",
        "  def distanceToNearestGoal(self, new_y, new_x):\n",
        "    distances = []\n",
        "    locations = []\n",
        "    stack = []\n",
        "    visited = set()\n",
        "    stack.append((new_y, new_x, 0))\n",
        "    while len(stack) != 0:\n",
        "      #print len(stack)\n",
        "      cur_row, cur_col, dist = stack.pop()\n",
        "      if (cur_row, cur_col) in visited:\n",
        "        continue\n",
        "      visited.add((cur_row, cur_col))\n",
        "      \n",
        "      for e in self._goal_loc:\n",
        "        if (cur_row, cur_col) == e:\n",
        "          distances.append(dist)\n",
        "          locations.append(e)\n",
        "          \n",
        "\n",
        "      if cur_row+1 < self._layout.shape[0] and self._layout[cur_row+1, cur_col] != self._wall:\n",
        "        stack.append((cur_row+1, cur_col, dist+1))\n",
        "      if cur_row-1 > -1 and self._layout[cur_row-1, cur_col] != self._wall:\n",
        "        stack.append((cur_row-1, cur_col, dist+1))\n",
        "      if cur_col+1 < self._layout.shape[1] and self._layout[cur_row, cur_col+1] != self._wall:\n",
        "        stack.append((cur_row, cur_col+1, dist+1))\n",
        "      if cur_col-1 > -1 and self._layout[cur_row, cur_col-1] != self._wall:\n",
        "        stack.append((cur_row, cur_col-1, dist+1))\n",
        "    \n",
        "    # Has to be the absolute closest person\n",
        "    argmin_dist = np.argmin(np.array(distances))\n",
        "    #print(distances[argmin_dist], locations[argmin_dist])\n",
        "    return distances[argmin_dist], locations[argmin_dist] \n",
        "    \n",
        "  def handleDoor(self):\n",
        "    pass\n",
        "  \n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "    return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self, title=None):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    if title != None:\n",
        "      plt.title(title)\n",
        "    else:\n",
        "      plt.title(\"The Grid\")\n",
        "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    \n",
        "    for e in self._goals:\n",
        "      if e == self._key:\n",
        "        y = np.where(self._layout==e)[0]\n",
        "        x = np.where(self._layout==e)[1]\n",
        "        for i in range(y.shape[0]): \n",
        "          plt.text(x[i], y[i], r\"$\\mathbf{K}$\", ha='center', va='center')\n",
        "      elif e > 0:\n",
        "        y = np.where(self._layout==e)[0]\n",
        "        x = np.where(self._layout==e)[1]\n",
        "        for i in range(y.shape[0]): \n",
        "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    y = np.where(self._layout==self._door)[0]\n",
        "    x = np.where(self._layout==self._door)[1]\n",
        "    for i in range(y.shape[0]): \n",
        "      plt.text(x[i], y[i], r\"$\\mathbf{D}$\", ha='center', va='center')\n",
        "    \n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return self.get_obs_at(x, y)\n",
        "\n",
        "  def get_obs_at(self, x, y):\n",
        "    if self._tabular:\n",
        "      return y*self._layout.shape[1] + x\n",
        "    else:\n",
        "      v = self._vision_size\n",
        "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
        "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
        "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
        "      return location\n",
        "\n",
        "  def step(self, action, agent_inventory):\n",
        "    item = None\n",
        "    y, x = self._state\n",
        "        \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    discount = self._discount\n",
        "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
        "      reward = -1\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == self._key: # a key\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      item = 'KEY'\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "      print('PICKED UP KEY')\n",
        "    elif self._layout[new_y, new_x] == self._door: # a door\n",
        "      reward = 5\n",
        "      if 'KEY' not in agent_inventory:\n",
        "        reward = self._layout[new_y, new_x]\n",
        "        new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] > 0: # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    else:\n",
        "      \n",
        "#       reward = 0.\n",
        "      distToNearestGoal, nearestGoal = self.distanceToNearestGoal(new_y, new_x)\n",
        "      distToNearestGoal = float(distToNearestGoal)\n",
        "      \n",
        "      #print(self._layout[nearestGoal[0],nearestGoal[1]], distToNearestGoal, minDistance, distToNearestGoal/minDistance, -distToNearestGoal/minDistance, np.exp(-distToNearestGoal/minDistance))\n",
        "      reward = self._layout[nearestGoal[0],nearestGoal[1]]*np.exp(-distToNearestGoal)\n",
        "      if self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' not in agent_inventory:\n",
        "        reward = 0.\n",
        "#       elif self._layout[nearestGoal[0],nearestGoal[1]] == 100 and 'KEY' in agent_inventory:\n",
        "#         print agent_inventory\n",
        "        \n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "    \n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs(), item\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxFTKIfFj1vP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Hallway(s)"
      ]
    },
    {
      "metadata": {
        "id": "Hd1tV95Gj1vQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2723
        },
        "outputId": "b08857a7-5596-4e30-c70f-5210197ba216"
      },
      "cell_type": "code",
      "source": [
        "# tempHallway = Hallway(goal_loc=[(1,1,1)], discount=0.98)\n",
        "\n",
        "# tasks = []\n",
        "\n",
        "# for x in range(0,tempHallway._layout.shape[1]-2,12):\n",
        "#   tasks.append(Hallway(goal_loc=[(r,x+1,1) for r in range(1,tempHallway._layout.shape[0]-1,2)], discount=0.98))\n",
        "# del tasks[0]\n",
        "# del tempHallway\n",
        "\n",
        "# for idt,task in enumerate(tasks):\n",
        "#   task.plot_grid(title=\"grid_{}\".format(idt) )\n",
        "  \n",
        "maze = Maze(25,25,0.,0.,10,101)\n",
        "  \n",
        "tasks = []\n",
        "tasks.append(Hallway(goal_loc = maze._goal_locations, discount=0.98, layout=np.copy(maze._maze_clean)))\n",
        "for goal in maze._goal_locations:\n",
        "    tasks.append(Hallway(goal_loc = [goal], discount=0.98, layout=np.copy(maze._maze_clean)))\n",
        "\n",
        "\n",
        "for task in tasks:\n",
        "    task.plot_grid()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYNT/AY3ZQHOts/0KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6e75bbe0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIpwAkERTiCo/wOd36XQThm1+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6ef27438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOEEgiKcQFD/A7kNpdBSWj3oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6eecbd68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6ed84358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6ecb09b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiKcQFCEEwjqf4lQpfyxcODsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6e821048>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMpxSUIZTCspwSkH9D1GVpdD5Z82EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c6420b5f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c627f6438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaAIJxAU4QSC+h9YuaXQSkRgTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c626e44a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ninACQRFOIKj/Alompfz9UMH3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c62515ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD2CAYAAAAtfpAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBMIinACQRFOICjCCQT1X61OrNiKugwsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0c62484588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MKfA7ifHvO-M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Implement agents\n"
      ]
    },
    {
      "metadata": {
        "id": "qwHC3_5hj1vb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NEURAL_TEACHER_STUDENT(object):\n",
        "  \n",
        "  # Target Network is the same, as C-step is just C=1\n",
        "  \n",
        "  def __init__(self, number_of_features_teacher,\n",
        "                number_of_features_student,\n",
        "                number_of_features_substitute,\n",
        "                number_of_hidden_teacher,\n",
        "                number_of_hidden_student,\n",
        "                number_of_hidden_substitute,\n",
        "                number_of_actions_teacher,\n",
        "                number_of_actions_student,\n",
        "                number_of_actions_substitute,\n",
        "                initial_state_teacher,\n",
        "                initial_state_student, \n",
        "                initial_state_substitute,\n",
        "                rl_alg_teacher='DQN',\n",
        "                rl_alg_student='DQN', \n",
        "                rl_alg_substitute='DQN',\n",
        "                num_offline_updates_teacher=20, \n",
        "                num_offline_updates_student=25,\n",
        "                num_offline_updates_substitute=25,\n",
        "                step_size_teacher=0.01,\n",
        "                step_size_student=0.01,\n",
        "                step_size_substitute=0.01): \n",
        "    # HMMM?\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    self._prev_action_student = 0\n",
        "    self._step_student = step_size_student\n",
        "    self._num_features_student = number_of_features_student\n",
        "    self._num_action_student = number_of_actions_student\n",
        "    self._num_hidden_student = number_of_hidden_student\n",
        "    self._initial_state_student = initial_state_student\n",
        "    self._s_student = initial_state_student\n",
        "    self._s_student = np.reshape(self._s_student, (1,-1))\n",
        "    self._times_trained_student = 0\n",
        "    self._inventory_student = set()\n",
        "    self._replayBuffer_student = []\n",
        "    self._num_offline_updates_student = num_offline_updates_student\n",
        "    self._rl_alg_student = rl_alg_student\n",
        "\n",
        "\n",
        "    self._prev_action_substitute = 0\n",
        "    self._step_substitute = step_size_substitute\n",
        "    self._num_features_substitute = number_of_features_substitute\n",
        "    self._num_action_substitute = number_of_actions_substitute\n",
        "    self._num_hidden_substitute = number_of_hidden_substitute\n",
        "    self._initial_state_substitute = initial_state_substitute\n",
        "    self._s_substitute = initial_state_substitute\n",
        "    self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
        "    self._times_trained_substitute = 0\n",
        "    self._inventory_substitute = set()\n",
        "    self._replayBuffer_substitute = []\n",
        "    self._num_offline_updates_substitute = num_offline_updates_substitute\n",
        "    self._rl_alg_substitute = rl_alg_substitute\n",
        "\n",
        "    \n",
        "    self._prev_action_teacher = 0\n",
        "    self._step_teacher = step_size_teacher\n",
        "    self._num_features_teacher = number_of_features_teacher\n",
        "    self._num_action_teacher = number_of_actions_teacher\n",
        "    self._num_hidden_teacher = number_of_hidden_teacher\n",
        "    self._initial_state_teacher = initial_state_teacher\n",
        "    self._s_teacher = initial_state_teacher\n",
        "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
        "    self._times_trained_teacher = 0\n",
        "    self._replayBuffer_teacher = []\n",
        "    self._num_offline_updates_teacher = num_offline_updates_teacher\n",
        "    self._rl_alg_teacher = rl_alg_teacher\n",
        "    self.name = 'HYPER '+self._rl_alg_teacher\n",
        "  \n",
        "    # ?????????? should it be the number of tasks\n",
        "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
        "    \n",
        "    \n",
        "    self._times_used = 0.\n",
        "    \n",
        "    self.handleTF()\n",
        "  \n",
        "  def reset(self):\n",
        "    tf.reset_default_graph()\n",
        "    self.handleTF()\n",
        "    self.resetState_teacher()\n",
        "    self.resetReplayBuffer_teacher()\n",
        "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
        "    self._times_trained_teacher = 0\n",
        "    self._prev_action_teacher = 0\n",
        "\n",
        "    self.resetReplayBuffer('TRAIN_STUDENT')\n",
        "    self.resetState('TRAIN_STUDENT')\n",
        "    self._times_trained_student = 0\n",
        "    self._prev_action_student = 0\n",
        "    self._inventory_student = set()\n",
        "\n",
        "    self.resetReplayBuffer('TEST_SUBSTITUTE')\n",
        "    self.resetState('TEST_SUBSTITUTE')\n",
        "    self._times_trained_substitute = 0\n",
        "    self._prev_action_substitute = 0\n",
        "    self._inventory_substitute = set()\n",
        "\n",
        "    self._times_used = 0\n",
        "\n",
        "\n",
        "\n",
        "  def resetReplayBuffer_teacher(self):\n",
        "    self._replayBuffer_teacher = []\n",
        "    \n",
        "  def resetState_teacher(self):\n",
        "    self._s_teacher = self._initial_state_teacher \n",
        "    self._s_teacher = np.reshape(self._s_teacher, (1,-1))\n",
        "    \n",
        "  def handleTF(self):\n",
        "    self._sess = tf.Session()\n",
        "    #tf.reset_default_graph()\n",
        "    self.rewTensor_teacher = tf.placeholder(tf.float64)\n",
        "    self.disTensor_teacher = tf.placeholder(tf.float64)\n",
        "    self.nqTensor_teacher = tf.placeholder(tf.float64)\n",
        "    self.actionTensor_teacher = tf.placeholder(tf.int32)\n",
        "    self.stateTensor_teacher = tf.placeholder(tf.float64, shape=(1,self._num_features_teacher))\n",
        "    self._dense_1_teacher = tf.layers.dense(self.stateTensor_teacher,\n",
        "                                    self._num_hidden_teacher, activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._dense_2_teacher = tf.layers.dense(self._dense_1_teacher,\n",
        "                                    self._num_action_teacher, activation=None,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._q_teacher = tf.reshape(self._dense_2_teacher, (self._num_action_teacher,))    \n",
        "    self._softmx_teacher = tf.nn.softmax(self._q_teacher)\n",
        "    self._cost_teacher = tf.losses.mean_squared_error(self.rewTensor_teacher + self.disTensor_teacher*self.nqTensor_teacher, self._q_teacher[self.actionTensor_teacher])\n",
        "    self._opt_teacher = tf.train.RMSPropOptimizer(self._step_teacher).minimize(self._cost_teacher) \n",
        "    \n",
        "\n",
        "    self.rewTensor_student = tf.placeholder(tf.float64)\n",
        "    self.disTensor_student = tf.placeholder(tf.float64)\n",
        "    self.nqTensor_student = tf.placeholder(tf.float64)\n",
        "    self.actionTensor_student = tf.placeholder(tf.int32)\n",
        "    self.stateTensor_student = tf.placeholder(tf.float64, shape=(1,self._num_features_student))\n",
        "    self._dense_1_student = tf.layers.dense(self.stateTensor_student,\n",
        "                                    self._num_hidden_student, activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._dense_2_student = tf.layers.dense(self._dense_1_student,\n",
        "                                    self._num_action_student, activation=None,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._q_student = tf.reshape(self._dense_2_student, (self._num_action_student,))    \n",
        "    self._cost_student = tf.losses.mean_squared_error(self.rewTensor_student+ self.disTensor_student*self.nqTensor_student, self._q_student[self.actionTensor_student])\n",
        "    self._opt_student = tf.train.RMSPropOptimizer(self._step_student).minimize(self._cost_student)\n",
        "\n",
        "\n",
        "    self.rewTensor_substitute = tf.placeholder(tf.float64)\n",
        "    self.disTensor_substitute = tf.placeholder(tf.float64)\n",
        "    self.nqTensor_substitute = tf.placeholder(tf.float64)\n",
        "    self.actionTensor_substitute = tf.placeholder(tf.int32)\n",
        "    self.stateTensor_substitute = tf.placeholder(tf.float64, shape=(1,self._num_features_substitute))\n",
        "    self._dense_0_substitute = tf.layers.dense(self.stateTensor_substitute,\n",
        "                                    self._num_hidden_substitute, activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._dense_1_substitute = tf.layers.dense(self._dense_0_substitute,\n",
        "                                    self._num_hidden_substitute, activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._dense_2_substitute = tf.layers.dense(self._dense_1_substitute,\n",
        "                                    self._num_action_substitute, activation=None,\n",
        "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
        "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
        "    self._q_substitute = tf.reshape(self._dense_2_substitute, (self._num_action_substitute,))    \n",
        "    self._cost_substitute = tf.losses.mean_squared_error(self.rewTensor_substitute+ self.disTensor_substitute*self.nqTensor_substitute, self._q_substitute[self.actionTensor_substitute])\n",
        "    self._opt_substitute = tf.train.RMSPropOptimizer(self._step_substitute).minimize(self._cost_substitute)\n",
        "    \n",
        "    # HMMM?\n",
        "    self._sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  def _target_policy_teacher(self, q, a):\n",
        "    return np.eye(len(q))[a]\n",
        " \n",
        "  def _behaviour_policy_teacher(self, q):    \n",
        "    return epsilon_greedy(q, 0.1)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
        "\n",
        "  def getProbs(self):\n",
        "    # softmax\n",
        "    return self._probs_teacher\n",
        "\n",
        "  def q_teacher(self, obs):\n",
        "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
        "    obs = np.reshape(obs,(1,-1))\n",
        "    #print obs\n",
        "    t, probs = self._sess.run([self._q_teacher, self._softmx_teacher], {self.stateTensor_teacher: obs})\n",
        "    return t, probs\n",
        "  \n",
        "  def step_teacher(self, r, g, s, train):\n",
        "    self._times_used += 1\n",
        "    #print self._times_used\n",
        "    qvs, probs = self.q_teacher(s)\n",
        "    q_nxtState = np.reshape(qvs, (-1,))\n",
        "    self._probs_teacher = probs\n",
        "    next_action = self._behaviour_policy_teacher(q_nxtState)\n",
        "    \n",
        "    if r != None and train == True:\n",
        "      if self._rl_alg_teacher == 'NEURALSARSA':\n",
        "        target = self._target_policy_teacher(q_nxtState, next_action)\n",
        "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
        "        vob = q_nxtState[target]\n",
        "        #print vob\n",
        "        self._sess.run(self._opt_teacher,{\n",
        "            self.nqTensor_teacher: vob,\n",
        "            self.rewTensor_teacher: r,\n",
        "            self.disTensor_teacher: g,\n",
        "            self.actionTensor_teacher: self._prev_action_teacher,\n",
        "            self.stateTensor_teacher: self._s_teacher})\n",
        "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
        "        for _ in range(self._num_offline_updates_teacher):\n",
        "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
        "          self._sess.run(self._opt_teacher,{\n",
        "              self.nqTensor_teacher: replay[4],\n",
        "              self.rewTensor_teacher: replay[2],\n",
        "              self.disTensor_teacher: replay[3],\n",
        "              self.actionTensor_teacher: replay[1],\n",
        "              self.stateTensor_teacher: replay[0]})\n",
        "      elif self._rl_alg_teacher == 'DQN':\n",
        "        # This function should return an action\n",
        "        # Optimiser\n",
        "        vob = np.max(q_nxtState)\n",
        "        self._sess.run(self._opt_teacher,{\n",
        "            self.nqTensor_teacher: vob,\n",
        "            self.rewTensor_teacher: r,\n",
        "            self.disTensor_teacher: g,\n",
        "            self.actionTensor_teacher: self._prev_action_teacher,\n",
        "            self.stateTensor_teacher: self._s_teacher})\n",
        "        self._replayBuffer_teacher.append([self._s_teacher, self._prev_action_teacher, r, g, vob])\n",
        "        for _ in range(self._num_offline_updates_teacher):\n",
        "          replay = self._replayBuffer_teacher[np.random.randint(len(self._replayBuffer_teacher))]\n",
        "          self._sess.run(self._opt_teacher,{\n",
        "              self.nqTensor_teacher: replay[4],\n",
        "              self.rewTensor_teacher: replay[2],\n",
        "              self.disTensor_teacher: replay[3],\n",
        "              self.actionTensor_teacher: replay[1],\n",
        "              self.stateTensor_teacher: replay[0]})\n",
        "\n",
        "    self._s_teacher = np.reshape(s, (1,-1))\n",
        "    self._prev_action_teacher = next_action\n",
        "    \n",
        "    return next_action\n",
        "\n",
        "  def reset_teacher(self):\n",
        "    tf.reset_default_graph()\n",
        "    self.handleTF()\n",
        "    self.resetState_teacher()\n",
        "    self.resetReplayBuffer_teacher()\n",
        "    self._probs_teacher = np.ones((1, self._num_features_teacher))/(self._num_features_teacher*1.)\n",
        "    self._times_trained_teacher = 0\n",
        "    self._prev_action_teacher = 0\n",
        "\n",
        "\n",
        "\n",
        "    # resetReplayBuffer_student\n",
        "  def resetReplayBuffer(self, STUDENT_TYPE):\n",
        "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
        "      self._replayBuffer_student = []\n",
        "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
        "      self._replayBuffer_substitute = []\n",
        "\n",
        "    # resetState_student\n",
        "  def resetState(self, STUDENT_TYPE):\n",
        "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
        "      self._s_student = self._initial_state_student \n",
        "      self._s_student = np.reshape(self._s_student, (1,-1))\n",
        "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
        "      self._s_substitute = self._initial_state_substitute\n",
        "      self._s_substitute = np.reshape(self._s_substitute, (1,-1))\n",
        "\n",
        "\n",
        "  def _target_policy_student(self, q, a):\n",
        "    return np.eye(len(q))[a]\n",
        " \n",
        "  def _behaviour_policy_student(self, q, train):\n",
        "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
        "    return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.05)\n",
        "\n",
        "  def _target_policy_substitute(self, q, a):\n",
        "    return np.eye(len(q))[a]\n",
        " \n",
        "  def _behaviour_policy_substitute(self, q, train):\n",
        "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
        "    return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.05)\n",
        "  \n",
        "  def q_student(self, obs):\n",
        "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
        "    obs = np.reshape(obs,(1,-1))\n",
        "    #print obs\n",
        "    t = self._sess.run(self._q_student, {self.stateTensor_student: obs})\n",
        "    return t\n",
        "  \n",
        "  def q_substitute(self, obs):\n",
        "    obs = np.reshape(obs, (1,-1))\n",
        "    t = self._sess.run(self._q_substitute, {self.stateTensor_substitute: obs})\n",
        "    return t\n",
        "\n",
        "  # step_student\n",
        "  def step(self, r, g, s, item, train, STUDENT_TYPE):\n",
        "    cost = None\n",
        "    \n",
        "    if STUDENT_TYPE == 'TRAIN_STUDENT':\n",
        "      if item != None:\n",
        "        self._inventory_student.add(item)\n",
        "      \n",
        "      # This function should return an action\n",
        "      q_nxtState = np.reshape(self.q_student(s), (-1,))\n",
        "      next_action = self._behaviour_policy_student(q_nxtState, train)\n",
        "      \n",
        "\n",
        "      if self._rl_alg_student == 'NEURALSARSA':\n",
        "        target = self._target_policy_student(q_nxtState, next_action)\n",
        "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
        "        \n",
        "        # Optimiser\n",
        "        vob = q_nxtState[target]\n",
        "        if train == True:\n",
        "          self._sess.run(self._opt_student,{\n",
        "              self.nqTensor_student: vob,\n",
        "              self.rewTensor_student: r,\n",
        "              self.disTensor_student: g,\n",
        "              self.actionTensor_student: self._prev_action_student,\n",
        "              self.stateTensor_student: self._s_student})\n",
        "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
        "          for _ in range(self._num_offline_updates_student):\n",
        "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
        "            self._sess.run(self._opt_student,{\n",
        "                self.nqTensor_student: replay[4],\n",
        "                self.rewTensor_student: replay[2],\n",
        "                self.disTensor_student: replay[3],\n",
        "                self.actionTensor_student: replay[1],\n",
        "                self.stateTensor_student: replay[0]})\n",
        "      elif self._rl_alg_student == 'DQN':\n",
        "        vob = np.max(q_nxtState)\n",
        "        if train == True:\n",
        "          self._sess.run(self._opt_student,{\n",
        "              self.nqTensor_student: vob,\n",
        "              self.rewTensor_student: r,\n",
        "              self.disTensor_student: g,\n",
        "              self.actionTensor_student: self._prev_action_student,\n",
        "              self.stateTensor_student: self._s_student})\n",
        "          self._replayBuffer_student.append([self._s_student, self._prev_action_student, r, g, vob])\n",
        "          for _ in range(self._num_offline_updates_student):\n",
        "            replay = self._replayBuffer_student[np.random.randint(len(self._replayBuffer_student))]\n",
        "            self._sess.run(self._opt_student,{\n",
        "                self.nqTensor_student: replay[4],\n",
        "                self.rewTensor_student: replay[2],\n",
        "                self.disTensor_student: replay[3],\n",
        "                self.actionTensor_student: replay[1],\n",
        "                self.stateTensor_student: replay[0]})\n",
        "\n",
        "      \n",
        "          \n",
        "      self._s_student = np.reshape(s, (1,-1))\n",
        "      self._prev_action_student = next_action\n",
        "      return next_action, self._inventory_student, cost\n",
        "\n",
        "    elif STUDENT_TYPE == 'TEST_SUBSTITUTE':\n",
        "      if item != None:\n",
        "        self._inventory_substitute.add(item)\n",
        "      \n",
        "      # This function should return an action\n",
        "      q_nxtState = np.reshape(self.q_substitute(s), (-1,))\n",
        "      next_action = self._behaviour_policy_substitute(q_nxtState, train)\n",
        "      \n",
        "\n",
        "      if self._rl_alg_substitute == 'NEURALSARSA':\n",
        "        target = self._target_policy_substitute(q_nxtState, next_action)\n",
        "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
        "        \n",
        "        # Optimiser\n",
        "        vob = q_nxtState[target]\n",
        "\n",
        "        if train == True:\n",
        "          self._sess.run(self._opt_substitute,{\n",
        "              self.nqTensor_substitute: vob,\n",
        "              self.rewTensor_substitute: r,\n",
        "              self.disTensor_substitute: g,\n",
        "              self.actionTensor_substitute: self._prev_action_substitute,\n",
        "              self.stateTensor_substitute: self._s_substitute})\n",
        "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
        "          for _ in range(self._num_offline_updates_substitute):\n",
        "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
        "            self._sess.run(self._opt_substitute,{\n",
        "                self.nqTensor_substitute: replay[4],\n",
        "                self.rewTensor_substitute: replay[2],\n",
        "                self.disTensor_substitute: replay[3],\n",
        "                self.actionTensor_substitute: replay[1],\n",
        "                self.stateTensor_substitute: replay[0]})\n",
        "      elif self._rl_alg_substitute == 'DQN':\n",
        "        vob = np.max(q_nxtState)\n",
        "        if train == True:\n",
        "          self._sess.run(self._opt_substitute,{\n",
        "              self.nqTensor_substitute: vob,\n",
        "              self.rewTensor_substitute: r,\n",
        "              self.disTensor_substitute: g,\n",
        "              self.actionTensor_substitute: self._prev_action_substitute,\n",
        "              self.stateTensor_substitute: self._s_substitute})\n",
        "          self._replayBuffer_substitute.append([self._s_substitute, self._prev_action_substitute, r, g, vob])\n",
        "          for _ in range(self._num_offline_updates_substitute):\n",
        "            replay = self._replayBuffer_substitute[np.random.randint(len(self._replayBuffer_substitute))]\n",
        "            self._sess.run(self._opt_substitute,{\n",
        "                self.nqTensor_substitute: replay[4],\n",
        "                self.rewTensor_substitute: replay[2],\n",
        "                self.disTensor_substitute: replay[3],\n",
        "                self.actionTensor_substitute: replay[1],\n",
        "                self.stateTensor_substitute: replay[0]})\n",
        "          \n",
        "      self._s_substitute = np.reshape(s, (1,-1))\n",
        "      self._prev_action_substitute = next_action\n",
        "      return next_action, self._inventory_substitute, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixZUk41Zj1v6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task Selector"
      ]
    },
    {
      "metadata": {
        "id": "nS1RGyMGj1v7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TaskSelector(object):\n",
        "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
        "  \n",
        "  def __init__(self, rl_agent, tasks, reward_signal, number_of_tasks_selection_steps, student_type):\n",
        "    self._unscaled_reward_history = []\n",
        "    self._student_type = student_type\n",
        "    self._rl_agent = rl_agent\n",
        "    self._tasks = tasks\n",
        "    self._reward_signal = reward_signal\n",
        "    self._tasks_buffer = np.zeros((5,len(tasks)))\n",
        "    self._tasks_buffer_scaled = np.zeros((5,len(tasks)))\n",
        "    self._tasks_episodes_completed_train = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
        "    self._tasks_episodes_completed_test = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
        "    self._time = 0\n",
        "    self._number_of_tasks_selection_steps = number_of_tasks_selection_steps\n",
        "    \n",
        "    self._train_tasks_times_selected = np.zeros((len(tasks)))\n",
        "    self._test_tasks_times_selected = np.zeros((len(tasks)))\n",
        "    self._train_task_accuracy = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
        "    self._test_task_accuracy = np.zeros((len(tasks), number_of_tasks_selection_steps))\n",
        "    \n",
        "  def resetReplayBuffer(self):\n",
        "    self._rl_agent.resetReplayBuffer(self._student_type)    \n",
        "  \n",
        "  def step(self, action_task_id):\n",
        "    \n",
        "    print('Selected Task: ', action_task_id)\n",
        "    \n",
        "    if self._reward_signal == 'SPG':\n",
        "      self._train_tasks_times_selected[action_task_id] += 1.\n",
        "      self._test_tasks_times_selected[action_task_id] += 1.\n",
        "      \n",
        "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True, self._student_type)\n",
        "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[action_task_id], self._rl_agent, False, self._student_type)\n",
        "      \n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
        "      \n",
        "      self._train_task_accuracy[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] / self._train_tasks_times_selected[action_task_id]\n",
        "      \n",
        "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
        "        if x != action_task_id:\n",
        "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
        "          self._train_task_accuracy[x, self._time] = \\\n",
        "          self._tasks_episodes_completed_train[x, self._time] / self._train_tasks_times_selected[x]\n",
        "      \n",
        "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
        "      \n",
        "      self._test_task_accuracy[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[action_task_id, self._time] / self._test_tasks_times_selected[action_task_id]\n",
        "   \n",
        "      for x in range(self._tasks_episodes_completed_test.shape[0]):\n",
        "        if x != action_task_id:\n",
        "          self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
        "          self._test_task_accuracy[x, self._time] = \\\n",
        "          self._tasks_episodes_completed_test[x, self._time] / self._test_tasks_times_selected[x]\n",
        "       \n",
        "    elif self._reward_signal == 'TPG':\n",
        "      self._train_tasks_times_selected[action_task_id] += 1.\n",
        "      self._test_tasks_times_selected[-1] += 1.\n",
        "      \n",
        "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True, self._student_type)\n",
        "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[-1], self._rl_agent, False, self._student_type)\n",
        "      \n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
        "      \n",
        "      self._train_task_accuracy[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] / self._train_tasks_times_selected[action_task_id]\n",
        "      \n",
        "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
        "        if x != action_task_id:\n",
        "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
        "          self._train_task_accuracy[x, self._time] = \\\n",
        "          self._tasks_episodes_completed_train[x, self._time] / self._train_tasks_times_selected[x]\n",
        "      \n",
        "      self._tasks_episodes_completed_test[-1, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[-1, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[-1, self._time-1] \n",
        "      \n",
        "      self._test_task_accuracy[-1, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[-1, self._time] / self._test_tasks_times_selected[-1]\n",
        "      \n",
        "      for x in range(self._tasks_episodes_completed_test.shape[0]-1):\n",
        "        self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
        "        self._test_task_accuracy[x, self._time] = \\\n",
        "        self._tasks_episodes_completed_test[x, self._time] / self._test_tasks_times_selected[x]  \n",
        "        \n",
        "    \n",
        "    elif self._reward_signal == 'MPG':\n",
        "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
        "      \n",
        "      self._train_tasks_times_selected[action_task_id] += 1.\n",
        "      self._test_tasks_times_selected[uniform_sampled_task_id] += 1.\n",
        "      \n",
        "      _, _, ep_comp_train = run_step(self._tasks[action_task_id], self._rl_agent, True, self._student_type)\n",
        "      reward_after, reward_steps_after, ep_comp_test = run_step(self._tasks[uniform_sampled_task_id], self._rl_agent, False, self._student_type)\n",
        "      \n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
        "      \n",
        "      self._train_task_accuracy[action_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_train[action_task_id, self._time] / self._train_tasks_times_selected[action_task_id]\n",
        "      \n",
        "      for x in range(self._tasks_episodes_completed_train.shape[0]):\n",
        "        if x != action_task_id:\n",
        "          self._tasks_episodes_completed_train[x, self._time] = self._tasks_episodes_completed_train[x, self._time-1]\n",
        "          self._train_task_accuracy[x, self._time] = \\\n",
        "          self._tasks_episodes_completed_train[x, self._time] / self._train_tasks_times_selected[x]\n",
        "      \n",
        "      \n",
        "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1] \n",
        "      \n",
        "      self._test_task_accuracy[uniform_sampled_task_id, self._time] = \\\n",
        "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] / self._test_tasks_times_selected[uniform_sampled_task_id]\n",
        "      \n",
        "      for x in range(self._tasks_episodes_completed_test.shape[0]):\n",
        "        if x != uniform_sampled_task_id:\n",
        "          self._tasks_episodes_completed_test[x, self._time] = self._tasks_episodes_completed_test[x, self._time-1]\n",
        "          self._test_task_accuracy[x, self._time] = \\\n",
        "          self._tasks_episodes_completed_test[x, self._time] / self._test_tasks_times_selected[x]     \n",
        "    \n",
        "    self._time += 1\n",
        "    self._tasks_buffer[:,action_task_id] = np.roll(self._tasks_buffer[:,action_task_id], 1)\n",
        "    self._tasks_buffer[0,action_task_id] = reward_after\n",
        "    X = np.arange(self._tasks_buffer.shape[0])\n",
        "    slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,action_task_id])\n",
        "    rhat = slope \n",
        "    \n",
        "#     for i in range(self._tasks_buffer.shape[1]):\n",
        "#       slope, _, _, _, _ = stats.linregress(X, self._tasks_buffer[:,i])\n",
        "#       print(\"Task: \", i, slope)\n",
        "    \n",
        "#     self._unscaled_reward_history.append(rhat)\n",
        "#     temp_history = np.array(sorted(self._unscaled_reward_history))\n",
        "#     p_20 = np.percentile(temp_history, 20)\n",
        "#     p_80 = np.percentile(temp_history, 80)        \n",
        "\n",
        "#     if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
        "#       raise ValueError('Action {} is out of bounds for a '\n",
        "#                        '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
        "    \n",
        "#     r = None\n",
        "#     if rhat <= p_20:\n",
        "#       r = -1.\n",
        "#     elif rhat > p_80:\n",
        "#       r = 1.\n",
        "#     else:\n",
        "#       r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
        "      \n",
        "#     self._tasks_buffer_scaled[:,action_task_id] = np.roll(self._tasks_buffer_scaled[:,action_task_id], 1)\n",
        "#     self._tasks_buffer_scaled[0,action_task_id] = r\n",
        "    \n",
        "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
        "    return rhat, reward_after, np.reshape(self._tasks_buffer.T,(-1,)), ep_comp_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DrXBQ9NZj1v-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(24, 24))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  #print vmin, vmax\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
        "#   plt.savefig('./action_values_{}'.format(title))\n",
        "#   plt.close()\n",
        "\n",
        "def plot_greedy_policy(grid, title, q):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(q, axis=2)\n",
        "  grid.plot_grid(title)\n",
        "  plt.hold('on')\n",
        "  for i in range(grid._layout.shape[0]):\n",
        "    for j in range(grid._layout.shape[1]):\n",
        "      action_name = action_names[greedy_actions[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48UmChiQ_zZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_task_accuracy(accuracy_train, accuracy_test, student, teacher, reward_signal):\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Average Accuracy of Train Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Time')\n",
        "  for x in range(accuracy_train.shape[0]):    \n",
        "    plot = plt.plot(accuracy_train[x,:], label='Task '+str(x))\n",
        "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Average Accuracy of Test Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Time')\n",
        "  for x in range(accuracy_test.shape[0]):    \n",
        "    plot = plt.plot(accuracy_test[x,:], label='Task '+str(x))\n",
        "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6Y1XQzF_zZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_completed_episodes(completed_train, completed_test, student, teacher, reward_signal):\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Cumulative Average Number of Train Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
        "  plt.ylabel('Episodes Completed')\n",
        "  plt.xlabel('Time')\n",
        "  for x in range(completed_train.shape[0]):    \n",
        "    plot = plt.plot(completed_train[x,:], label='Task '+str(x))\n",
        "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Cumulative Average Number of Test Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
        "  plt.ylabel('Episodes Completed')\n",
        "  plt.xlabel('Time')\n",
        "  for x in range(completed_test.shape[0]):    \n",
        "    plot = plt.plot(completed_test[x,:], label='Task '+str(x))\n",
        "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGZIM_Gcj1wA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  reward_delta_dict = {}\n",
        "  action_dict = {}\n",
        "  prob_dict = {}\n",
        "  entropy_dict = {}\n",
        "  \n",
        "  student_type = ['TRAIN_STUDENT', 'TEST_SUBSTITUTE']\n",
        "  alg = algs[0]\n",
        "  \n",
        "  for typ in student_type:\n",
        "    print('Running:', alg.name + ' ' + typ)\n",
        "    reward_dict[alg.name + ' ' + typ] = []\n",
        "    reward_delta_dict[alg.name + ' ' + typ] = []\n",
        "    action_dict[alg.name + ' ' + typ] = []\n",
        "    prob_dict[alg.name + ' ' + typ] = []\n",
        "    entropy_dict[alg.name + ' ' + typ] = []\n",
        "    \n",
        "    qs = None\n",
        "    completed_train_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
        "    completed_test_episodes = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
        "    train_task_accuracy = np.zeros((len(tasks),number_of_steps_of_selecting_tasks))\n",
        "    test_task_accuracy = np.zeros((len(tasks),number_of_steps_of_selecting_tasks)) \n",
        "      \n",
        "#     alg.reset()\n",
        "    bandit = TaskSelector(alg, tasks, reward_signal, number_of_steps_of_selecting_tasks, typ)\n",
        "\n",
        "    reward_dict[alg.name + ' ' + typ].append([0.])\n",
        "    reward_delta_dict[alg.name + ' ' + typ].append([])\n",
        "    action_dict[alg.name + ' ' + typ].append([])\n",
        "    prob_dict[alg.name + ' ' + typ].append([])\n",
        "    entropy_dict[alg.name + ' ' + typ].append([])\n",
        "    action = None\n",
        "    reward = None\n",
        "    prob = None\n",
        "    entropy = None\n",
        "    reward_delta = None\n",
        "    success_student_episode = False\n",
        "    capability = alg._initial_state_teacher\n",
        "\n",
        "    for i in range(number_of_steps_of_selecting_tasks):\n",
        "      print('Step: ', i)\n",
        "      if typ == 'TRAIN_STUDENT':\n",
        "        action = alg.step_teacher(reward, 0., capability, True) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, True)\n",
        "      elif typ == 'TEST_SUBSTITUTE':\n",
        "        action = alg.step_teacher(reward, 0., capability, False) if success_student_episode == True else alg.step_teacher(reward, 0.98, capability, False)\n",
        "      prob = alg.getProbs()\n",
        "      entropy = -1.0 * np.sum(prob * np.log(prob))\n",
        "      reward, reward_from_environment, capability, success_student_episode = bandit.step(action)\n",
        "      bandit.resetReplayBuffer()\n",
        "      \n",
        "      reward_dict[alg.name + ' ' + typ][-1].append(reward_from_environment+reward_dict[alg.name + ' ' + typ][-1][-1])\n",
        "      reward_delta_dict[alg.name + ' ' + typ][-1].append(reward)\n",
        "      action_dict[alg.name + ' ' + typ][-1].append(action)\n",
        "      prob_dict[alg.name + ' ' + typ][-1].append(prob.copy())\n",
        "      entropy_dict[alg.name + ' ' + typ][-1].append(entropy)\n",
        "      \n",
        "    if typ == 'TRAIN_STUDENT':\n",
        "      h, w = tasks[-1]._layout.shape\n",
        "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
        "      qs = np.array([[[alg.q_student(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
        "    elif typ == 'TEST_SUBSTITUTE':\n",
        "      h, w = tasks[-1]._layout.shape\n",
        "      obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
        "      qs = np.array([[[alg.q_substitute(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
        "\n",
        "    completed_train_episodes = bandit._tasks_episodes_completed_train\n",
        "    completed_test_episodes = bandit._tasks_episodes_completed_test\n",
        "    train_task_accuracy = bandit._train_task_accuracy\n",
        "    test_task_accuracy = bandit._test_task_accuracy\n",
        "    \n",
        "    if typ == 'TRAIN_STUDENT':\n",
        "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
        "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_student, alg.name + ', ' + typ, reward_signal)\n",
        "    else:\n",
        "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
        "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_substitute, alg.name + ', ' + typ, reward_signal)\n",
        "      \n",
        "    plot_action_values(qs, alg.name + ' ' + typ)\n",
        "        \n",
        "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict\n",
        "\n",
        "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
        "  bandit = None\n",
        "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
        "  smoothed_rewards = {}\n",
        "  smoothed_rewards_stds = {}\n",
        "  smoothed_reward_deltas = {}\n",
        "  smoothed_reward_deltas_stds = {}\n",
        "  smoothed_actions = {}\n",
        "  smoothed_probs = {}\n",
        "  smoothed_entropies = {}\n",
        "  smoothed_entropies_stds = {}\n",
        "  agent_set = set()\n",
        "  \n",
        "  for agent, rewards in reward_dict.items():\n",
        "    agent_set.add(agent)\n",
        "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
        "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
        "  \n",
        "  for agent, rewards in reward_delta_dict.items():\n",
        "    agent_set.add(agent)\n",
        "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
        "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
        "  \n",
        "  for agent, probs in prob_dict.items():\n",
        "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
        "\n",
        "  for agent, entropies in entropy_dict.items():\n",
        "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
        "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
        "    \n",
        "  for agent in agent_set:\n",
        "    smoothed_probs[agent] /= repetitions\n",
        "    \n",
        "    plt.figure(figsize=(44,40))\n",
        "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
        "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Task')\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
        "  plt.ylabel('Reward')\n",
        "  plt.xlabel('Time')\n",
        "  for agent in agent_set:\n",
        "    smoothed_rewards[agent] /= repetitions    \n",
        "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
        "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
        "  plt.ylabel('Delta')\n",
        "  plt.xlabel('Time')\n",
        "  for agent in agent_set:\n",
        "    smoothed_reward_deltas[agent] /= repetitions    \n",
        "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
        "  plt.legend(loc='upper right')\n",
        "  \n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
        "  plt.ylabel('Policy Entropy')\n",
        "  plt.xlabel('Time')\n",
        "  for agent in agent_set:\n",
        "    smoothed_entropies[agent] /= repetitions  \n",
        "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
        "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
        "  plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTRJuSa_j1wC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_step(env, agent, train, student_type):     \n",
        "    env.resetState()\n",
        "    agent.resetState(student_type)\n",
        "    number_of_steps = env.distanceToGoal()\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "      agent_inventory = agent._inventory_student if student_type == 'TRAIN_STUDENT' else agent._inventory_substitute\n",
        "    steps_completed = 0\n",
        "    total_reward = 0.\n",
        "    while steps_completed != number_of_steps:\n",
        "      reward, discount, next_state, item = env.step(action, agent_inventory)\n",
        "      \n",
        "      if item != None:\n",
        "        if student_type == 'TRAIN_STUDENT':\n",
        "          agent._inventory_student.add(item)\n",
        "        else:\n",
        "          agent._inventory_substitute.add(item)\n",
        "        \n",
        "      total_reward += reward\n",
        "      \n",
        "      # Dont want to remove the key on train==True, cuz then cant get reward on train==false, where we record the reward\n",
        "      if reward == 100 and train == False:\n",
        "        if student_type == 'TRAIN_STUDENT':\n",
        "          agent._inventory_student.remove('KEY')\n",
        "        else:\n",
        "          agent._inventory_substitute.remove('KEY')\n",
        "      \n",
        "      \n",
        "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train, student_type)\n",
        "      \n",
        "      if discount == 0:\n",
        "        #print(total_reward)\n",
        "        print('EPISODE COMPLETED')\n",
        "        return total_reward, total_reward/steps_completed, True\n",
        "      \n",
        "      steps_completed += 1\n",
        "    \n",
        "    mean_reward = total_reward/number_of_steps\n",
        "\n",
        "    return total_reward, mean_reward, False\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "  \n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1672I5Z7j1wE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reward as Reward Signal for Bandit(s)"
      ]
    },
    {
      "metadata": {
        "id": "XK8BVplBj1wE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
        "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
        "#### NeuralRL Controller does not have inventory as input"
      ]
    },
    {
      "metadata": {
        "id": "2SsYpc2SRPe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number_of_steps_of_selecting_tasks = 100\n",
        "reps = 1\n",
        "\n",
        "# reward_signals=['SPG','TPG','MPG']\n",
        "# rl_algs = ['DQN','NEURALSARSA']\n",
        "\n",
        "reward_signals=['SPG']\n",
        "rl_algs = ['DQN']\n",
        "\n",
        "hidden_units_teacher_net = 100\n",
        "hidden_units_student_net = 100\n",
        "hidden_units_substitute_net = 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDzz94z_iRXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "outputId": "bce2d9b1-e97f-44ed-95e4-9c8281c3a93a"
      },
      "cell_type": "code",
      "source": [
        "vision_size = 1\n",
        "tabular_grid = False\n",
        "step_size_teacher = 0.01\n",
        "step_size_student = 0.01\n",
        "step_size_substitute = 0.01\n",
        "\n",
        "\n",
        "# tempHallway = Hallway(goal_loc=[(1,1,1)], discount=0.98)\n",
        "\n",
        "# tasks = []\n",
        "\n",
        "# for x in range(0,tempHallway._layout.shape[1]-2,12):\n",
        "#   tasks.append(Hallway(goal_loc=[(r,x+1,5) for r in range(1,tempHallway._layout.shape[0]-1,2)], tabular=tabular_grid, vision_size=vision_size, discount=0.98))\n",
        "# del tasks[0]\n",
        "# del tempHallway\n",
        "\n",
        "# tasks = tasks[::2]\n",
        "\n",
        "# for task in tasks:\n",
        "#   task.plot_grid()\n",
        "  \n",
        "\n",
        "for reward_signal in reward_signals:\n",
        "    for teacher_agent in rl_algs:\n",
        "        for student_agent in ['DQN']:\n",
        "            tasks = []\n",
        "            for goal in maze._goal_locations:\n",
        "                tasks.append(Hallway(goal_loc = [goal], tabular=tabular_grid, vision_size=vision_size, discount=0.98, layout=np.copy(maze._maze_clean)))\n",
        "                \n",
        "            # Intrinsically Motivated Curriculum Learning\n",
        "            number_of_arms_tasks = len(tasks)\n",
        "\n",
        "            agents = [\n",
        "            NEURAL_TEACHER_STUDENT(number_of_arms_tasks*5,\n",
        "                                        (2*vision_size + 1)**2,\n",
        "                                        (2*vision_size + 1)**2,\n",
        "                                        hidden_units_teacher_net,\n",
        "                                        hidden_units_student_net,\n",
        "                                        hidden_units_substitute_net,\n",
        "                                        number_of_arms_tasks,\n",
        "                                        4,\n",
        "                                        4,\n",
        "                                        np.zeros((1,number_of_arms_tasks*5)),\n",
        "                                        tasks[0].get_obs(),\n",
        "                                        tasks[0].get_obs(),\n",
        "                                        teacher_agent,\n",
        "                                        student_agent,\n",
        "                                        student_agent,\n",
        "                                        num_offline_updates_teacher=30, \n",
        "                                        num_offline_updates_student=30,\n",
        "                                        num_offline_updates_substitute=30,\n",
        "                                        step_size_teacher=step_size_teacher,\n",
        "                                        step_size_student=step_size_student,\n",
        "                                        step_size_substitute=step_size_substitute),\n",
        "            ]\n",
        "\n",
        "            agents[0].reset()\n",
        "\n",
        "            train_task_agents(agents,\n",
        "                            number_of_arms_tasks,\n",
        "                            number_of_steps_of_selecting_tasks, \n",
        "                            tasks,\n",
        "                            reward_signal,\n",
        "                            reps,\n",
        "                            vision_size,\n",
        "                            tabular_grid)\n",
        "\n",
        "    \n",
        "for task in tasks:\n",
        "    task.plot_grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running: HYPER DQN TRAIN_STUDENT\n",
            "Step:  0\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  1\n",
            "Selected Task:  3\n",
            "Step:  2\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "Step:  3\n",
            "Selected Task:  9\n",
            "Step:  4\n",
            "Selected Task:  9\n",
            "Step:  5\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "EPISODE COMPLETED\n",
            "Step:  6\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "EPISODE COMPLETED\n",
            "Step:  7\n",
            "Selected Task:  3\n",
            "Step:  8\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "Step:  9\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "Step:  10\n",
            "Selected Task:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE COMPLETED\n",
            "Step:  11\n",
            "Selected Task:  9\n",
            "EPISODE COMPLETED\n",
            "Step:  12\n",
            "Selected Task:  9\n",
            "Step:  13\n",
            "Selected Task:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUmqra3NK3a0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWmqCB_D_zZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}