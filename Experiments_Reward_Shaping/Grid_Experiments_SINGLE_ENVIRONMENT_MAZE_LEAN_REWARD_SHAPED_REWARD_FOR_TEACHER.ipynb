{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# Maze Experiments in Curriculum Learning in a Single Environment\n",
    "## Still the same principle, just saving memory\n",
    "## Give the Reward To Teacher\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "26/07/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, width, height, complexity, density, num_goals, goal):\n",
    "        self._maze, self._maze_clean, self._goal_locations = self.maze(width, height, complexity, density, num_goals, goal)\n",
    "        \n",
    "    def maze(self, width=81, height=51, complexity=.75, density=.75, num_goals=1, goal=1):\n",
    "        goal_locations = []\n",
    "        \n",
    "        # Only odd shapes\n",
    "        shape = ((height // 2) * 2 + 1, (width // 2) * 2 + 1)\n",
    "        # Adjust complexity and density relative to maze size\n",
    "        complexity = int(complexity * (5 * (shape[0] + shape[1]))) # number of components\n",
    "        density    = int(density * ((shape[0] // 2) * (shape[1] // 2))) # size of components\n",
    "        # Build actual maze\n",
    "        Z = np.zeros(shape)\n",
    "        # Fill borders\n",
    "        Z[0, :] = Z[-1, :] = 1\n",
    "        Z[:, 0] = Z[:, -1] = 1\n",
    "        # Make aisles\n",
    "        for i in range(density):\n",
    "            x, y = np.random.randint(0, (shape[1] // 2)+1) * 2, np.random.randint(0, (shape[0] // 2)+1) * 2 # pick a random position\n",
    "            Z[y, x] = 1\n",
    "            for j in range(complexity):\n",
    "                neighbours = []\n",
    "                if x > 1:             neighbours.append((y, x - 2))\n",
    "                if x < shape[1] - 2:  neighbours.append((y, x + 2))\n",
    "                if y > 1:             neighbours.append((y - 2, x))\n",
    "                if y < shape[0] - 2:  neighbours.append((y + 2, x))\n",
    "                if len(neighbours):\n",
    "                    y_,x_ = neighbours[np.random.randint(0, len(neighbours))]\n",
    "                    if Z[y_, x_] == 0:\n",
    "                        Z[y_, x_] = 1\n",
    "                        Z[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
    "                        x, y = x_, y_\n",
    "        Z[Z == 1] = -1\n",
    "        Y = np.copy(Z)\n",
    "        for x in range(0, num_goals):\n",
    "            idx = np.random.randint(len(np.where(Z == 0)[0]))\n",
    "            Z[np.where(Z == 0)[0][idx],np.where(Z == 0)[1][idx]] = goal\n",
    "        for e in zip(np.where(Z == goal)[0],np.where(Z == goal)[1]):\n",
    "            goal_locations.append((e[0],e[1],goal))\n",
    "        return Z,Y,goal_locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeGNMcHDj1vL"
   },
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mT38a_chiRWz"
   },
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, tabular=True, vision_size=1, discount=0.98, noisy=False, layout=None):\n",
    "    \n",
    "    self._wall = -1\n",
    "    self._layout = layout\n",
    "    \n",
    "    # row, col format\n",
    "    self._start_state = (1, 1)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "    \n",
    "    self._goals = set()\n",
    "    self._goal_loc = []\n",
    "    self._distances_to_goal = []\n",
    "    \n",
    "    self._dist = {}\n",
    "    for r,c in zip(np.where(self._layout != self._wall)[0],np.where(self._layout != self._wall)[1]):\n",
    "      self._dist[(r, c)] = self.dijkstra(r, c)\n",
    "      \n",
    "    for e in goal_loc:\n",
    "      self._layout[e[0],e[1]] = e[2]\n",
    "      self._goal_loc.append((e[0],e[1]))\n",
    "      self._goals.add(e[2])\n",
    "      self._distances_to_goal.append(self.minDistanceTwoPoints(self._start_state[0], self._start_state[1], e[0],e[1]))\n",
    "    \n",
    "    \n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "  \n",
    "  def distanceLeft(self, teacher_action):\n",
    "    goal_y = self._goal_loc[teacher_action][0]\n",
    "    goal_x = self._goal_loc[teacher_action][1]\n",
    "    return self.minDistanceTwoPoints(self._state[0], self._state[1], goal_y, goal_x)\n",
    "  \n",
    "  def distanceToGoal(self, teacher_action):\n",
    "    return np.prod(self._layout.shape)\n",
    "  \n",
    "  def dijkstra(self, cy, cx):\n",
    "    dist = {}\n",
    "    prev = {}\n",
    "    Q = set()\n",
    "    \n",
    "    for r,c in zip(np.where(self._layout != self._wall)[0],np.where(self._layout != self._wall)[1]):\n",
    "      dist[(r,c)] = np.inf\n",
    "      prev[(r,c)] = None\n",
    "      Q.add((r,c))\n",
    "    \n",
    "    dist[(cy,cx)] = 0.\n",
    "    \n",
    "    while len(Q) != 0:\n",
    "      ud = np.inf\n",
    "      u = None\n",
    "      for e in Q:\n",
    "        if dist[e] < ud:\n",
    "          ud = dist[e]\n",
    "          u = e\n",
    "      Q.remove(u)\n",
    "      \n",
    "      neighbors_u = []\n",
    "      if u[0]+1 < self._layout.shape[0] and self._layout[u[0]+1, u[1]] != self._wall and (u[0]+1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]+1, u[1]))\n",
    "      \n",
    "      if u[0]-1 > -1 and self._layout[u[0]-1, u[1]] != self._wall and (u[0]-1, u[1]) in Q:\n",
    "        neighbors_u.append((u[0]-1, u[1]))\n",
    "      \n",
    "      if u[1]+1 < self._layout.shape[1] and self._layout[u[0], u[1]+1] != self._wall and (u[0], u[1]+1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]+1))\n",
    "      \n",
    "      if u[1]-1 > -1 and self._layout[u[0], u[1]-1] != self._wall and (u[0], u[1]-1) in Q:\n",
    "        neighbors_u.append((u[0], u[1]-1))\n",
    "        \n",
    "      for neighbor in neighbors_u:\n",
    "        alt = dist[u] + 1.\n",
    "        if alt < dist[neighbor]:\n",
    "          dist[neighbor] = alt\n",
    "          prev[neighbor] = u\n",
    "    \n",
    "    return dist\n",
    "  \n",
    "  def minDistanceTwoPoints(self, cy, cx, dy, dx):\n",
    "    return self._dist[(cy,cx)][(dy,dx)]\n",
    "    \n",
    "  def handleDoor(self):\n",
    "    pass\n",
    "  \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self, title=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != self._wall, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title != None:\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      plt.title(\"The Grid\")\n",
    "    plt.text(self._start_state[1], self._start_state[0], r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    for e in self._goals:\n",
    "      if e > 0:\n",
    "        y = np.where(self._layout==e)[0]\n",
    "        x = np.where(self._layout==e)[1]\n",
    "        for i in range(y.shape[0]): \n",
    "          plt.text(x[i], y[i], r\"$\\mathbf{G}$\", ha='center', va='center')  \n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      #location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], -1, 2)\n",
    "      location = self._layout[y-v:y+v+1,x-v:x+v+1]\n",
    "      return location\n",
    "\n",
    "  def step(self, action, teacher_action, agent_inventory):\n",
    "    item = None\n",
    "    goal_y = self._goal_loc[teacher_action][0]\n",
    "    goal_x = self._goal_loc[teacher_action][1]\n",
    "    y, x = self._state\n",
    "        \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == self._wall:  # a wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] > 0 and (new_y, new_x) == (goal_y, goal_x) : # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      #minDistanceStart = self.minDistanceTwoPoints(self._start_state[0], self._start_state[1], goal_y, goal_x)\n",
    "      distToGoal = self.minDistanceTwoPoints(new_y, new_x, goal_y, goal_x)\n",
    "      distToGoal = float(distToGoal)\n",
    "      #minDistanceStart = float(minDistanceStart)\n",
    "      reward = np.exp(-distToGoal) - 1.\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs(), item\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxFTKIfFj1vP"
   },
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2841,
     "status": "ok",
     "timestamp": 1533290522088,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "Hd1tV95Gj1vQ",
    "outputId": "cea9e3b9-2da0-499c-811e-efe547ab3fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task: 0', (7, 15, 1))\n",
      "('Task: 1', (10, 15, 1))\n",
      "('Task: 2', (16, 1, 1))\n",
      "('Task: 3', (19, 11, 1))\n",
      "('Task: 4', (19, 18, 1))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAD8CAYAAACbxyOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACjdJREFUeJzt3W9oXXcdx/HPt1vqIjFjJhqitkEaGDVgixna6gPTNIOK\nsYitVCUgNI9lhIaJ64MhspYa8AqrEtAUZDApVUPaKu2Dbmi2BwqND7IggovaWbRxWf5SSHD9+SB3\n3W3+3ibn5Py+575fkAc7OTnfc07z2bnc+8k5FkIQAD92ZL0DAB4OoQWcIbSAM4QWcIbQAs4QWsAZ\nQhshM3vezF7KeB++ZWbX1vn+q2Z2cjv3CUsIbQbMbM7MZotf75rZ3ZJl3yyultgH6Gb2lJldMbN3\nil9vmNkPzOzxtX4mhPByCOFIUvuA5BDaDIQQPhRCqA0h1Er6p6Qvlyz7ZZKzzOzzkl6VNCzpyRDC\nhyUdkfQ/SfvW+JlHktwHJIvQZs+KX8t9wMx+Ubz6jprZZ+7/gFmjmf3KzCbM7E0z+8462z8naSCE\n8MMQwn8lKYTwrxDC90MIfyhu79tm9pqZ/cjM3pb0fHHZcMnMp83sL2Y2ZWYvrrHP2AaENl5fkfSy\npMclXZH0E0kyMyv+958lNUo6LOkZM3t6+QbM7IOSDkr6TRnzPifpb5I+KumF4rJQ3E69pF9Lek5S\nvaQ3JX1hk8eFLSK08XothHA9LJXDX5L06eLyz0qqDyG8EEJ4N4TwD0k/l/SNVbbxhJb+jf/z3gIz\nO1e8Ws6b2XMl694OIfw0hHAvhLCwbDtfkvRGCGGwOPPHpdvE9iK08SoNxV1Jj5nZDkm7JX285E2l\nKUnf09IVcrkpSfe0dEWWJIUQvhtCeELSoKRHS9Z9a519+dgq319vfaTo0Y1XQWTekjQeQnhyoxVD\nCHfN7I+Svibp9xutvs73/q2l/1mU2rXRfKSDK60f773x8ydJc2b2rJk9ZmaPmFmLmT21xs89K+lk\ncf2PSJKZfULSJx9i9m8lfcrMvlqc94ykhs0eCLaG0Gav3M9jgySFEO5J6pS0X9LfJU1I+pmk2lV/\nKITXJbVL+qKkv5rZO5J+p6WPgV4sa3AIk5K+rqV3ot+WtEfS62XuNxJm/BE84AtXWsAZQgs4Q2gB\nZwgt4My6n9OaGe9SARkJIaza796wXLH8E/XV3Cpz3XLXS2vdPM7P4zEx//11V8PLY8AZQgs4Q2gB\nZwgt4AyhBZwhtIAzhBZwZt2/8qFcAWRnrXIFV1rAmUQbURv9be7SjQQ3Xi+tdWOZzzn1M7/cRlSS\n2yzd7mpSuUfUwsKCTp48qaamJs3Pz6u6ulrnzp1LYxRQcVJ5eTw0NKTGxkadOXNG3d3dmpqaSmMM\nUJFSCe2tW7f0yiuv6Pbt29q3b5+OHOGRMEBSUgltR0eHRkdHtWvXLh08eFAHDhxIYwxQkVIJ7f79\n+3X9+nUdO3ZMo6OjOn36dBpjgIqUSmivXr2qQ4cO6dKlS7p48aJu3ryZxhigIqUS2ps3b+rataXn\nEdfV1WnPnj1pjAEqUiof+VRXV+vy5cu6cuWK7ty5o7Nnz6YxBqhI1BiBSFFjBHKCGmMG8z3dWCzr\n+Vmc04dZN4saI1dawBlCCzhDaAFnCC3gDKEFnCG0gDOEFnCGRhQQKRpRQE7QiHI+3+MxZd2I8nT8\nq+FKCzhDaAFnCC3gDKEFnCG0gDOEFnCG0ALOEFrAGWqMQKSoMQI5QY3R+XyPx5R1jZEbuwHYVoQW\ncIbQAs4QWsAZQgs4Q2gBZwgt4AyhBZyhxghEihojkBPUGJ3P93hMnn6nqDEC2DJCCzhDaAFnCC3g\nDKEFnCG0gDOEFnCGRhQQKRpRQE7QiHI+P5ZGUFo3VuPGbitxpQWcIbSAM4QWcIbQAs4QWsAZQgs4\nQ2gBZwgt4Aw1RiBS1BiBnKDG6Hx+Ho9pM/OT/D3dzPztrDFuGFoga3Nzc+rt7dXExISam5tVU1Oj\n6elpFQqFrHctE7w8RvQ6Ozu1uLiowcFB9fX1qaGhQTMzM1nvVmYILaJ248YNDQ8Pq7u7+/6yrq4u\n7dy5M8O9yhahRdRGRkZkZmpsbLy/rKamRv39/RnuVbYILVyoqqrKeheiQWgRtdbWVknS5OSkZmdn\n1dPTo7a2Nh09elRjY2MZ7102CC2i1t7erra2Ng0MDKi2tlaFQkHj4+Oqr69XS0tL1ruXCUKL6A0N\nDWlhYUHHjx/XqVOndOLECdXV1WW9W5mhxghEihojkBPUGJ3P93hM3I2x/Pmr4UoLOENoAWcILeAM\noQWcIbSAM4QWcIbQAs7QiAIiRSMKyAkaUc7nezwmT79TNKIAbBmhBZwhtIAzhBZwhtACzhBawBlC\nCzhDaAFnqDECkaLGCOQENUbn82Op8aV1YzVu7LYSz6dF9Hg+7YN4eYzo8XzaBxFaRI3n065EaBE1\nnk+7EqGFCzyf9n2EFlHj+bQrEVpEjefTrkRoET2eT/sgaoxApKgxAjlBjdH5/FhqjFmf0yR/Tzcz\nn7sxAlgToQWcIbSAM4QWcIbQAs4QWsAZQgs4QyMKiBSNKCAnaEQ5n++xEZXGjdXSakRlffyr4UoL\nOENoAWcILeAMoQWcIbSAM4QWcIbQAs4QWsAZaoxApKgxAjlBjdH5fI81xko/fmqMQIUhtIAzhBZw\nhtACzhBawBlCCzhDaAFnaEQBkaIRBeQEjSjn8z02grJ+1GUaN5ajEQVgTYQWcIbQAs4QWsAZQgs4\nQ2gBZwgt4AyhBZyhxghEihojkBPUGJ3Pp8bo79+UGiNQYQgt4AyhBZwhtIAzhBZwhtACzhBawBlC\nCzhDjRGIFDVGICeoMTqfH0uNMeu7IWY9fztrjBuG9mHNzc2pt7dXExMTam5uVk1Njaanp1UoFJIe\nBVSkxF8ed3Z2anFxUYODg+rr61NDQ4NmZmaSHgNUrERDe+PGDQ0PD6u7u/v+sq6uLu3cuTPJMUBF\nSzS0IyMjMjM1NjbeX1ZTU6P+/v4kxwAVLZV3j6uqqtLYLAAlHNrW1lZJ0uTkpGZnZ9XT06O2tjYd\nPXpUY2NjSY4CKlaioW1vb1dbW5sGBgZUW1urQqGg8fFx1dfXq6WlJclRQMWiEQVEikYUkBM0opzP\nz+MxxTQ/xkYUV1rAGUILOENoAWcILeAMoQWcIbSAM4QWcIbQAs5QYwQiRY0RyAlqjM7nezwmbuxW\n/vzVcKUFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcILeAMNUYgUtQYgZygxuh8/la2ud5jSSv5nC5f\nN4nzVLpNaozYNB5LWp7YzhOhrVA8lrQ8MZ4nQluheCxpeWI8T4S2wvFY0vLEdJ4IbYXisaTlifE8\nbfg5Le8exz1/K9s8fPiw9u7dq/Pnz0uSdu/erY6ODl24cKGiz+nydZM4T6XbLDdTa31OS2idz9/K\nNufn59XT06OpqSk1NTVpx46lF159fX0VfU6Xr5vEeSrdZuqhLWP7AFJAIwrIiUQbUV5uwpWn+Xk8\nJubTiAJyhdACzhBawBlCCzhDaAFnCC3gDKEFnCG0gDOEFnCG7jEQqU39wQCA+PDyGHCG0ALOEFrA\nGUILOENoAWf+Dy0cZE1mw6iZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3043e80790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = Maze(20,20,0.1,0.05,5,1)  \n",
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "\n",
    "tasks = []\n",
    "tasks.append(Hallway(goal_loc = maze._goal_locations, tabular=tabular_grid, vision_size=vision_size, discount=0.98, layout=np.copy(maze._maze_clean)))\n",
    "\n",
    "for idx, goal_loc in enumerate(maze._goal_locations):\n",
    "  print('Task: '+str(idx), goal_loc)\n",
    "\n",
    "for task in tasks:\n",
    "    task.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ERMJb-tFj1vW"
   },
   "outputs": [],
   "source": [
    "class GeneralQ(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, num_offline_updates=30, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._replayBuffer_A = []\n",
    "    if double:\n",
    "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
    "      self._replayBuffer_B = []\n",
    "    self._s = initial_state\n",
    "    self._initial_state = initial_state\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._target_policy = target_policy\n",
    "    self._double = double\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._last_action = 0\n",
    "    self._inventory = set()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_A = []\n",
    "    if self._double:\n",
    "      self._replayBuffer_B = []\n",
    "      \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    if self._double:\n",
    "      return (self._q + self._q2)/2\n",
    "    else:\n",
    "      return self._q\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "\n",
    "  def step(self, r, g, s, item, train):\n",
    "    td = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "      \n",
    "    if self._double:\n",
    "      next_action = self._behaviour_policy(self.q_values[s,:], train)\n",
    "      if np.random.random() <= 0.5:\n",
    "        expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q2[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "          #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q2[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "          self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      else:\n",
    "        expectation = np.sum(self._target_policy(self._q2[s,:], next_action) * self._q[s,:])\n",
    "        td = self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])\n",
    "        if train == True:\n",
    "          self._q2[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])   \n",
    "          #self._q2[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q2[s,:], next_action))] - self._q2[self._s,self._last_action])    \n",
    "          self._replayBuffer_B.append([self._s,self._last_action,r,g,s,next_action])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer_B[np.random.randint(len(self._replayBuffer_B))]\n",
    "            expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "            self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      return self._last_action, self._inventory, td\n",
    "    else:\n",
    "      next_action = self._behaviour_policy(self._q[s,:], train)\n",
    "      # This is expected sarsa, but still functions as expected.\n",
    "      expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q[s,:])\n",
    "      td = self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "      if train == True:\n",
    "        self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "        #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "        self._replayBuffer_A.append([self._s,self._last_action,r,g,s,next_action])\n",
    "        for _ in range(self._num_offline_updates):\n",
    "          replay = self._replayBuffer_A[np.random.randint(len(self._replayBuffer_A))]\n",
    "          expectation = np.sum(self._target_policy(self._q[replay[4],:], replay[5]) * self._q2[replay[4],:])\n",
    "          self._q[replay[0],replay[1]] += self._step_size*(replay[2] + replay[3] * expectation - self._q[replay[0],replay[1]])\n",
    "\n",
    "      self._s = s\n",
    "      self._last_action = next_action\n",
    "      #print(self._inventory)\n",
    "      return self._last_action, self._inventory, td\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8RECPLMKj1vY"
   },
   "outputs": [],
   "source": [
    "def Q_target_policy(q, a):\n",
    "  return np.eye(len(q))[np.argmax(q)]\n",
    "\n",
    "def SARSA_target_policy(q, a):\n",
    "  return np.eye(len(q))[a]\n",
    "\n",
    "def gen_behaviour_policy(q, train):\n",
    "  #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "  return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMr_z0RZsHNj"
   },
   "source": [
    "An agent that uses **Neural-Sarsa/DQN** to learn action values.  The agent should expect a nxn input which it should flatten into a vector, and then pass through a multi-layer perceptron with a single hidden layer with 100 hidden nodes and ReLU activations.  Each weight layer should also have a bias.  Initialize all weights uniformly randomly in $[-0.05, 0.05]$.\n",
    "\n",
    "```\n",
    "NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "            \n",
    "DQN(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "```\n",
    "\n",
    "The number `vision_size` will be either 1 or 2 below.  The input vector will be of size $(2v + 1)^2$, which will correspond to a square local view of the grid, centered on the agent, and of size $(2v + 1) \\times (2v + 1)$ (so either 3x3 or 5x5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qwHC3_5hj1vb"
   },
   "outputs": [],
   "source": [
    "class NEURAL_CONTROLLER_DRIVER(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, number_of_features_controller,\n",
    "                number_of_features_driver,\n",
    "                number_of_hidden_controller,\n",
    "                number_of_hidden_driver,\n",
    "                number_of_actions_controller,\n",
    "                number_of_actions_driver,\n",
    "                initial_state_controller,\n",
    "                initial_state_driver, \n",
    "                rl_alg_controller='DQN',\n",
    "                rl_alg_driver='DQN', \n",
    "                num_offline_updates_controller=25, \n",
    "                num_offline_updates_driver=25,\n",
    "                step_size_controller=0.01,\n",
    "                step_size_driver=0.01): \n",
    "    # HMMM?\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    self._prev_action_driver = 0\n",
    "    self._step_driver = step_size_driver\n",
    "    self._num_features_driver = number_of_features_driver\n",
    "    self._num_action_driver = number_of_actions_driver\n",
    "    self._num_hidden_driver = number_of_hidden_driver\n",
    "    self._initial_state_driver = initial_state_driver\n",
    "    self._s_driver = initial_state_driver\n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "    self._times_trained_driver = 0\n",
    "    self._inventory = set()\n",
    "    self._replayBuffer_driver = []\n",
    "    self._num_offline_updates_driver = num_offline_updates_driver\n",
    "    self._rl_alg_driver = rl_alg_driver\n",
    "    self._eps_driver = 1.0\n",
    "\n",
    "\n",
    "    \n",
    "    self._prev_action_controller = 0\n",
    "    self._step_controller = step_size_controller\n",
    "    self._num_features_controller = number_of_features_controller\n",
    "    self._num_action_controller = number_of_actions_controller\n",
    "    self._num_hidden_controller = number_of_hidden_controller\n",
    "    self._initial_state_controller = initial_state_controller\n",
    "    self._s_controller = initial_state_controller\n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    self._times_trained_controller = 0\n",
    "    self._replayBuffer_controller = []\n",
    "    self._num_offline_updates_controller = num_offline_updates_controller\n",
    "    self._rl_alg_controller = rl_alg_controller\n",
    "    self.name = 'HYPER '+self._rl_alg_controller\n",
    "    self._eps_controller = 1.0\n",
    "    \n",
    "    # ?????????? should it be the number of tasks\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    \n",
    "    \n",
    "    self._times_used = 0.\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "    self._eps_controller = 1.0\n",
    "    self.resetReplayBuffer()\n",
    "    self.resetState()\n",
    "    self._times_trained_driver = 0\n",
    "    self._prev_action_driver = 0\n",
    "    self._eps_driver = 1.0\n",
    "    self._inventory = set()\n",
    "    self._times_used = 0\n",
    "\n",
    "  def resetReplayBuffer_controller(self):\n",
    "    self._replayBuffer_controller = []\n",
    "    \n",
    "  def resetState_controller(self):\n",
    "    self._s_controller = self._initial_state_controller \n",
    "    self._s_controller = np.reshape(self._s_controller, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.disTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_controller = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_controller = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_controller = tf.placeholder(tf.float64, shape=(1,self._num_features_controller))\n",
    "    self._dense_1_controller = tf.layers.dense(self.stateTensor_controller,\n",
    "                                    self._num_hidden_controller, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_controller = tf.layers.dense(self._dense_1_controller,\n",
    "                                    self._num_action_controller, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_controller = tf.reshape(self._dense_2_controller, (self._num_action_controller,))    \n",
    "    self._softmx_controller = tf.nn.softmax(self._q_controller)\n",
    "    self._cost_controller = tf.losses.mean_squared_error(self.rewTensor_controller + self.disTensor_controller*self.nqTensor_controller, self._q_controller[self.actionTensor_controller])\n",
    "    self._opt_controller = tf.train.RMSPropOptimizer(self._step_controller).minimize(self._cost_controller) \n",
    "    \n",
    "\n",
    "    self.rewTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.disTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.nqTensor_driver = tf.placeholder(tf.float64)\n",
    "    self.actionTensor_driver = tf.placeholder(tf.int32)\n",
    "    self.stateTensor_driver = tf.placeholder(tf.float64, shape=(1,self._num_features_driver))\n",
    "    self._dense_1_driver = tf.layers.dense(self.stateTensor_driver,\n",
    "                                    self._num_hidden_driver, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2_driver = tf.layers.dense(self._dense_1_driver,\n",
    "                                    self._num_action_driver, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q_driver = tf.reshape(self._dense_2_driver, (self._num_action_driver,))    \n",
    "    self._cost_driver = tf.losses.mean_squared_error(self.rewTensor_driver+ self.disTensor_driver*self.nqTensor_driver, self._q_driver[self.actionTensor_driver])\n",
    "    self._opt_driver = tf.train.RMSPropOptimizer(self._step_driver).minimize(self._cost_driver)\n",
    "\n",
    "\n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy_controller(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_controller(self, q):    \n",
    "    return epsilon_greedy(q, self._eps_controller)# if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "\n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs_controller\n",
    "\n",
    "  def q_controller(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t, probs = self._sess.run([self._q_controller, self._softmx_controller], {self.stateTensor_controller: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step_controller(self, r, g, s):\n",
    "    self._times_used += 1\n",
    "    #print self._times_used\n",
    "    qvs, probs = self.q_controller(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs_controller = probs\n",
    "    next_action = self._behaviour_policy_controller(q_nxtState)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg_controller == 'NEURALSARSA':\n",
    "        target = self._target_policy_controller(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "        #print vob\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "      elif self._rl_alg_controller == 'DQN':\n",
    "        # This function should return an action\n",
    "        # Optimiser\n",
    "        vob = np.max(q_nxtState)\n",
    "        self._sess.run(self._opt_controller,{\n",
    "            self.nqTensor_controller: vob,\n",
    "            self.rewTensor_controller: r,\n",
    "            self.disTensor_controller: g,\n",
    "            self.actionTensor_controller: self._prev_action_controller,\n",
    "            self.stateTensor_controller: self._s_controller})\n",
    "        self._replayBuffer_controller.append([self._s_controller, self._prev_action_controller, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_controller):\n",
    "          replay = self._replayBuffer_controller[np.random.randint(len(self._replayBuffer_controller))]\n",
    "          self._sess.run(self._opt_controller,{\n",
    "              self.nqTensor_controller: replay[4],\n",
    "              self.rewTensor_controller: replay[2],\n",
    "              self.disTensor_controller: replay[3],\n",
    "              self.actionTensor_controller: replay[1],\n",
    "              self.stateTensor_controller: replay[0]})\n",
    "\n",
    "    self._s_controller = np.reshape(s, (1,-1))\n",
    "    self._prev_action_controller = next_action\n",
    "    \n",
    "    return next_action\n",
    "\n",
    "  def reset_controller(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState_controller()\n",
    "    self.resetReplayBuffer_controller()\n",
    "    self._probs_controller = np.ones((1, self._num_features_controller))/(self._num_features_controller*1.)\n",
    "    self._times_trained_controller = 0\n",
    "    self._prev_action_controller = 0\n",
    "\n",
    "\n",
    "\n",
    "    # resetReplayBuffer_driver\n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer_driver = []\n",
    "    \n",
    "    # resetState_driver\n",
    "  def resetState(self):\n",
    "    self._s_driver = self._initial_state_driver \n",
    "    self._s_driver = np.reshape(self._s_driver, (1,-1))\n",
    "\n",
    "\n",
    "  def _target_policy_driver(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy_driver(self, q, train):\n",
    "    #return epsilon_greedy(q, 0.1) if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    return epsilon_greedy(q, self._eps_driver) if train == True else epsilon_greedy(q, 0.01)\n",
    "\n",
    "  def q_driver(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    #print obs\n",
    "    t = self._sess.run(self._q_driver, {self.stateTensor_driver: obs})\n",
    "    return t\n",
    "  \n",
    "  # step_driver\n",
    "  def step(self, r, g, s, item, train, steps_taken):\n",
    "    cost = None\n",
    "    \n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "    \n",
    "    # This function should return an action\n",
    "    q_nxtState = np.reshape(self.q_driver(s), (-1,))\n",
    "    next_action = self._behaviour_policy_driver(q_nxtState, train)\n",
    "    \n",
    "\n",
    "    if self._rl_alg_driver == 'NEURALSARSA':\n",
    "      target = self._target_policy_driver(q_nxtState, next_action)\n",
    "      target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "      \n",
    "      # Optimiser\n",
    "      vob = q_nxtState[target]\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#           self.nqTensor_driver: vob,\n",
    "#           self.rewTensor_driver: r,\n",
    "#           self.disTensor_driver: g,\n",
    "#           self.actionTensor_driver: self._prev_action_driver,\n",
    "#           self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "    elif self._rl_alg_driver == 'DQN':\n",
    "      vob = np.max(q_nxtState)\n",
    "#       cost = self._sess.run(self._cost_driver,{\n",
    "#               self.nqTensor_driver: vob,\n",
    "#               self.rewTensor_driver: r,\n",
    "#               self.disTensor_driver: g,\n",
    "#               self.actionTensor_driver: self._prev_action_driver,\n",
    "#               self.stateTensor_driver: self._s_driver})\n",
    "      if train == True:\n",
    "        self._sess.run(self._opt_driver,{\n",
    "            self.nqTensor_driver: vob,\n",
    "            self.rewTensor_driver: r,\n",
    "            self.disTensor_driver: g,\n",
    "            self.actionTensor_driver: self._prev_action_driver,\n",
    "            self.stateTensor_driver: self._s_driver})\n",
    "        self._replayBuffer_driver.append([self._s_driver, self._prev_action_driver, r, g, vob])\n",
    "        for _ in range(self._num_offline_updates_driver):\n",
    "          replay = self._replayBuffer_driver[np.random.randint(len(self._replayBuffer_driver))]\n",
    "          self._sess.run(self._opt_driver,{\n",
    "              self.nqTensor_driver: replay[4],\n",
    "              self.rewTensor_driver: replay[2],\n",
    "              self.disTensor_driver: replay[3],\n",
    "              self.actionTensor_driver: replay[1],\n",
    "              self.stateTensor_driver: replay[0]})\n",
    "\n",
    "    \n",
    "        \n",
    "    self._s_driver = np.reshape(s, (1,-1))\n",
    "    self._prev_action_driver = next_action\n",
    "    return next_action, self._inventory, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCpz-tYIj1vf"
   },
   "source": [
    "## Agent 0: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_ezyxkw3j1vg"
   },
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "  \n",
    "  This agent returns an action between 0 and 'number_of_arms', \n",
    "  uniformly at random. The 'previous_action' argument of 'step'\n",
    "  is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, number_of_arms):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'random'\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return np.ones((self._number_of_arms))/self._number_of_arms\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiz8-tm5r0XP"
   },
   "source": [
    "## Agent 1: REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TVpQ1fIQr0XQ"
   },
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    " \n",
    "  def __init__(self, number_of_arms, step_size=0.1, baseline=False):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._lr = step_size\n",
    "    self.name = 'reinforce, baseline: {}'.format(baseline)\n",
    "    self._baseline = baseline\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.total_reward = 0;\n",
    "    self.number_rewards = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      self.number_rewards += 1.\n",
    "      self.total_reward += reward\n",
    "      self.action_values[0,previous_action] += reward\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "      self.updatePreferences(previous_action, reward)\n",
    "#    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "#     if unvisited[0].size > 0:\n",
    "#       return unvisited[0][0]\n",
    "#     else:\n",
    "#       return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.number_rewards = 0.\n",
    "    self.total_reward = 0.\n",
    "  \n",
    "  def updatePreferences(self, previous_action, reward):\n",
    "    if not self._baseline: \n",
    "      self.action_preferences[0,previous_action]+=self._lr*reward*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*reward*self.softmax()[i]\n",
    "    else:\n",
    "      self.action_preferences[0,previous_action]+=self._lr*(reward - self.total_reward/self.number_rewards)*(1-self.softmax()[previous_action])\n",
    "      for i in range(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*(reward - self.total_reward/self.number_rewards)*self.softmax()[i]\n",
    "    \n",
    "  def softmax(self):\n",
    "    q = np.sum(np.exp(self.action_preferences),axis=1)\n",
    "    t = np.exp(self.action_preferences)/q\n",
    "    return t.flatten()\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPlTgXlGr0XT"
   },
   "source": [
    "## Agent 2: EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6-ZPCR4tr0XT"
   },
   "outputs": [],
   "source": [
    "class EXP3(object):\n",
    "\n",
    "  def __init__(self, number_of_arms, gamma):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'exp3 Gamma: ' + str(gamma)\n",
    "    \n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.weights = np.ones((1,self._number_of_arms))\n",
    "    \n",
    "    self.time = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      xhat = np.zeros((1, self._number_of_arms))\n",
    "      xhat[0,previous_action] = reward/self.action_values[0,previous_action]\n",
    "      self.weights = self.weights*np.exp(self.gamma*xhat/self._number_of_arms)\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "    self.action_values[0,:] = (1-self.gamma)*(self.weights)/(np.sum(self.weights)) + self.gamma/self._number_of_arms\n",
    "    action = np.random.choice(self._number_of_arms, p=self.action_values[0,:])\n",
    "    self.time += 1.\n",
    "    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "    return unvisited[0][0] if unvisited[0].size > 0 else action\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.action_values[0,:]\n",
    "  \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2, self._number_of_arms))\n",
    "    self.weights = np.ones((1, self._number_of_arms))\n",
    "    self.time = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA9ryUOej1v3"
   },
   "source": [
    "## Agent 7: Task Selection via RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EVhim-3Rj1v4"
   },
   "outputs": [],
   "source": [
    "class NS_DQN(object):\n",
    "  \n",
    "  # Target Network is the same, as C-step is just C=1\n",
    "  \n",
    "  def __init__(self, teacher_student, number_of_features, number_of_hidden, number_of_actions, initial_state, rl_alg='DQN', num_offline_updates=25, step_size=0.01):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    self._prev_action = 0\n",
    "    self._step = step_size\n",
    "    self._num_features = number_of_features\n",
    "    self._num_action = number_of_actions\n",
    "    self._num_hidden = number_of_hidden\n",
    "    self._initial_state = initial_state\n",
    "    self._s = initial_state\n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    self._times_trained = 0\n",
    "    self._replayBuffer = []\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._rl_alg = rl_alg\n",
    "    self._teacher_student = teacher_student\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._inventory = set()\n",
    "    \n",
    "    if self._teacher_student == True:\n",
    "      self.name = 'HYPER '+self._rl_alg\n",
    "    else:\n",
    "      self.name = self._rl_alg\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def resetReplayBuffer(self):\n",
    "    self._replayBuffer = []\n",
    "    \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor = tf.placeholder(tf.float64)\n",
    "    self.disTensor = tf.placeholder(tf.float64)\n",
    "    self.nqTensor = tf.placeholder(tf.float64)\n",
    "    self.actionTensor = tf.placeholder(tf.int32)\n",
    "    self.stateTensor = tf.placeholder(tf.float64, shape=(1,self._num_features))\n",
    "    self._dense_1 = tf.layers.dense(self.stateTensor,\n",
    "                                    self._num_hidden, activation=tf.nn.tanh,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2 = tf.layers.dense(self._dense_1,\n",
    "                                    self._num_action, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q = tf.reshape(self._dense_2, (self._num_action,))    \n",
    "    self._softmx = tf.nn.softmax(self._q)\n",
    "    self._cost = tf.losses.mean_squared_error(self.rewTensor + self.disTensor*self.nqTensor, self._q[self.actionTensor])\n",
    "    self._opt = tf.train.RMSPropOptimizer(self._step).minimize(self._cost) \n",
    "    # HMMM?\n",
    "    self._sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  def _target_policy(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy(self, q, train):\n",
    "    if self._teacher_student == True:   \n",
    "      return epsilon_greedy(q, 0.1)#if train == True else np.random.choice(np.where(np.max(q) == q)[0])\n",
    "    else:\n",
    "      return epsilon_greedy(q, 0.1) if train == True else epsilon_greedy(q, 0.01)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    # softmax\n",
    "    return self._probs\n",
    "  \n",
    "  def q_noProbs(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t = self._sess.run(self._q, {self.stateTensor: obs})\n",
    "    return t\n",
    "  \n",
    "  def q(self, obs):\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    t, probs = self._sess.run([self._q, self._softmx], {self.stateTensor: obs})\n",
    "    return t, probs\n",
    "  \n",
    "  def step(self, r, g, s, item=None, train=True, steps_taken=None):\n",
    "    cost = None\n",
    "\n",
    "    if item != None:\n",
    "      self._inventory.add(item)\n",
    "\n",
    "    qvs, probs = self.q(s)\n",
    "    q_nxtState = np.reshape(qvs, (-1,))\n",
    "    self._probs = probs\n",
    "    next_action = self._behaviour_policy(q_nxtState, train)\n",
    "    \n",
    "    if r != None:\n",
    "      if self._rl_alg == 'NEURALSARSA':\n",
    "        target = self._target_policy(q_nxtState, next_action)\n",
    "        target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "        vob = q_nxtState[target]\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "      elif self._rl_alg == 'DQN':\n",
    "        vob = np.max(q_nxtState)\n",
    "\n",
    "        if train == True:\n",
    "          self._sess.run(self._opt,{\n",
    "              self.nqTensor: vob,\n",
    "              self.rewTensor: r,\n",
    "              self.disTensor: g,\n",
    "              self.actionTensor: self._prev_action,\n",
    "              self.stateTensor: self._s})\n",
    "          self._replayBuffer.append([self._s, self._prev_action, r, g, vob])\n",
    "          for _ in range(self._num_offline_updates):\n",
    "            replay = self._replayBuffer[np.random.randint(len(self._replayBuffer))]\n",
    "            self._sess.run(self._opt,{\n",
    "                self.nqTensor: replay[4],\n",
    "                self.rewTensor: replay[2],\n",
    "                self.disTensor: replay[3],\n",
    "                self.actionTensor: replay[1],\n",
    "                self.stateTensor: replay[0]})\n",
    "\n",
    "    self._s = np.reshape(s, (1,-1))\n",
    "    self._prev_action = next_action\n",
    "    return next_action, self._inventory, cost\n",
    "\n",
    "  def reset(self):\n",
    "    tf.reset_default_graph()\n",
    "    self.handleTF()\n",
    "    self.resetState()\n",
    "    self.resetReplayBuffer()\n",
    "    self._probs = np.ones((1, self._num_action))/(self._num_action*1.)\n",
    "    self._times_trained = 0\n",
    "    self._prev_action = 0\n",
    "    self._inventory = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixZUk41Zj1v6"
   },
   "source": [
    "## Task Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nS1RGyMGj1v7"
   },
   "outputs": [],
   "source": [
    "class TaskSelector(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, reward_signal, number_of_tasks_selection_steps):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._unscaled_env_reward_history = []\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "    self._FIFO_Length = 5\n",
    "    self._tasks_env_buffer = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_buffer_scaled = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_env_buffer_scaled = np.zeros((self._FIFO_Length,len(tasks[0]._goal_loc)))\n",
    "    self._tasks_episodes_completed_train = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._tasks_episodes_completed_test = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._time = 0\n",
    "    self._number_of_tasks_selection_steps = number_of_tasks_selection_steps\n",
    "    \n",
    "    self._train_tasks_times_selected = np.zeros((len(tasks[0]._goal_loc)))\n",
    "    self._test_tasks_times_selected = np.zeros((len(tasks[0]._goal_loc)))\n",
    "    self._train_task_accuracy = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    self._test_task_accuracy = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    \n",
    "    self._tasks_slopes = np.zeros((len(tasks[0]._goal_loc), number_of_tasks_selection_steps))\n",
    "    \n",
    "  def resetReplayBuffer(self):\n",
    "    self._rl_agent.resetReplayBuffer()    \n",
    "  \n",
    "  def step(self, action_task_id):\n",
    "    if np.all(self._train_tasks_times_selected == 0.):\n",
    "      for i in range(len(tasks[0]._goal_loc)):\n",
    "        for j in range(self._FIFO_Length):\n",
    "          reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], i, self._rl_agent, False)\n",
    "          self._tasks_env_buffer[:,i] = np.roll(self._tasks_env_buffer[:,i], 1)\n",
    "          self._tasks_env_buffer[0,i] = reward_after\n",
    "    \n",
    "    \n",
    "    eps = min(1.0, max(0.1, 1.0 - (self._time)/(self._number_of_tasks_selection_steps/8.)))\n",
    "#     print(eps)\n",
    "    self._rl_agent._eps_driver = eps\n",
    "    \n",
    "    \n",
    "    \n",
    "    ep_comp_train = False\n",
    "    if self._reward_signal == 'SPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[action_task_id] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], action_task_id, self._rl_agent, False)\n",
    "      if not ep_comp_test:\n",
    "#         print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[action_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[action_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "       \n",
    "    elif self._reward_signal == 'TPG':\n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[-1] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], -1, self._rl_agent, False)\n",
    "      if not ep_comp_test:\n",
    "        print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True)\n",
    "      \n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[-1, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[-1, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[-1, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected\n",
    "      \n",
    "        \n",
    "    \n",
    "    elif self._reward_signal == 'MPG':\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      \n",
    "      self._train_tasks_times_selected[action_task_id] += 1.\n",
    "      self._test_tasks_times_selected[uniform_sampled_task_id] += 1.\n",
    "      \n",
    "      reward_after, reward_steps_after, ep_comp_test, distanceLeft = run_step(self._tasks[0], uniform_sampled_task_id, self._rl_agent, False)\n",
    "      if not ep_comp_test:\n",
    "        print('Training')\n",
    "        _, _, ep_comp_train, _ = run_step(self._tasks[0], action_task_id, self._rl_agent, True)\n",
    "      \n",
    "      self._tasks_episodes_completed_train[:, self._time] = self._tasks_episodes_completed_train[:, self._time-1]\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[action_task_id, self._time-1]+1 if ep_comp_train == True else self._tasks_episodes_completed_train[action_task_id, self._time-1] \n",
    "      self._train_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_train[:, self._time] / self._train_tasks_times_selected\n",
    "      \n",
    "      self._tasks_episodes_completed_test[:, self._time] = self._tasks_episodes_completed_test[:, self._time-1]\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1]+1 if ep_comp_test == True else self._tasks_episodes_completed_test[uniform_sampled_task_id, self._time-1] \n",
    "      self._test_task_accuracy[:, self._time] = \\\n",
    "      self._tasks_episodes_completed_test[:, self._time] / self._test_tasks_times_selected[:]    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    self._tasks_env_buffer[:,action_task_id] = np.roll(self._tasks_env_buffer[:,action_task_id], 1)\n",
    "    self._tasks_env_buffer[0,action_task_id] = reward_after\n",
    "    X = np.arange(self._tasks_env_buffer.shape[0])\n",
    "    slope, _, _, _, _ = stats.linregress(X, self._tasks_env_buffer[:,action_task_id])\n",
    "    rhat = np.abs(slope)\n",
    "#     rhat = np.std(self._tasks_env_buffer[:,action_task_id])\n",
    "    \n",
    "    for i in range(self._tasks_env_buffer.shape[1]):\n",
    "      slope, _, _, _, _ = stats.linregress(X, self._tasks_env_buffer[:,i])\n",
    "      self._tasks_slopes[i, self._time] = np.abs(slope)\n",
    "#       print(\"Task: \", i, slope)\n",
    "    \n",
    "    self._time += 1\n",
    "\n",
    "#     self._unscaled_env_reward_history.append(distanceLeft)\n",
    "#     temp_history = np.array(sorted(self._unscaled_env_reward_history))\n",
    "#     p_20 = np.percentile(temp_history, 20)\n",
    "#     p_80 = np.percentile(temp_history, 80)        \n",
    "    \n",
    "#     r = None\n",
    "#     if distanceLeft <= p_20:\n",
    "#       r = -1.\n",
    "#     elif distanceLeft > p_80:\n",
    "#       r = 1.\n",
    "#     else:\n",
    "#       r = 2.0 * (distanceLeft - p_20)/(p_80 - p_20) - 1.\n",
    "      \n",
    "#     self._tasks_env_buffer_scaled[:,action_task_id] = np.roll(self._tasks_env_buffer_scaled[:,action_task_id], 1)\n",
    "#     self._tasks_env_buffer_scaled[0,action_task_id] = r\n",
    "\n",
    "    \n",
    "    \n",
    "#     self._unscaled_reward_history.append(rhat)\n",
    "#     temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "#     p_20 = np.percentile(temp_history, 20)\n",
    "#     p_80 = np.percentile(temp_history, 80)        \n",
    "    \n",
    "#     r = None\n",
    "#     if rhat <= p_20:\n",
    "#       r = -1.\n",
    "#     elif rhat > p_80:\n",
    "#       r = 1.\n",
    "#     else:\n",
    "#       r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "      \n",
    "#     self._tasks_buffer_scaled[:,action_task_id] = np.roll(self._tasks_buffer_scaled[:,action_task_id], 1)\n",
    "#     self._tasks_buffer_scaled[0,action_task_id] = r\n",
    "    \n",
    "    print('Task: '+str(action_task_id), rhat, reward_after, self._tasks_env_buffer[:,action_task_id])\n",
    "    \n",
    "    #print reward_steps_after\n",
    "    # Perhaps, plot the variance or something else, because the train==False fucks this plot up\n",
    "    return rhat, reward_after, np.reshape(self._tasks_env_buffer.T,(-1,)), (ep_comp_train or ep_comp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DrXBQ9NZj1v-"
   },
   "outputs": [],
   "source": [
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, title, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(24, 24))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  #print vmin, vmax\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(r\"$v(s), \\mathrm{\" + title + r\"}$\")\n",
    "#   plt.savefig('./action_values_{}'.format(title))\n",
    "#   plt.close()\n",
    "\n",
    "def plot_greedy_policy(grid, title, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid(title)\n",
    "  plt.hold('on')\n",
    "  for i in range(grid._layout.shape[0]):\n",
    "    for j in range(grid._layout.shape[1]):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_slopes(slopes, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Slopes of Tasks; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(slopes.shape[0]):    \n",
    "    plot = plt.semilogy(slopes[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_accuracy(accuracy_train, accuracy_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Train Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_train.shape[0]):    \n",
    "    plot = plt.plot(accuracy_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Average Accuracy of Test Episodes; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(accuracy_test.shape[0]):    \n",
    "    plot = plt.plot(accuracy_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_completed_episodes(completed_train, completed_test, student, teacher, reward_signal):\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Train Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_train.shape[0]):    \n",
    "    plot = plt.plot(completed_train[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Number of Test Episodes Completed; Student: {}, Teacher: {}, Reward Signal: {}'.format(student, teacher, reward_signal))\n",
    "  plt.ylabel('Episodes Completed')\n",
    "  plt.xlabel('Time')\n",
    "  for x in range(completed_test.shape[0]):    \n",
    "    plot = plt.plot(completed_test[x,:], label='Task '+str(x))\n",
    "    #plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QGZIM_Gcj1wA"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  reward_delta_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  episode_dict = {}\n",
    "  \n",
    "  for alg in algs:\n",
    "    print('Running:', alg.name)\n",
    "    reward_dict[alg.name] = []\n",
    "    reward_delta_dict[alg.name] = []\n",
    "    action_dict[alg.name] = []\n",
    "    prob_dict[alg.name] = []\n",
    "    entropy_dict[alg.name] = []\n",
    "    episode_dict[alg.name] = []\n",
    "    \n",
    "    rl_agent = None\n",
    "    qs = None\n",
    "    completed_train_episodes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    completed_test_episodes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    train_task_accuracy = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    test_task_accuracy = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    task_slopes = np.zeros((len(tasks[0]._goal_loc),number_of_steps_of_selecting_tasks))\n",
    "    \n",
    "    for qq in range(repetitions):\n",
    "      print('Rep:', qq)\n",
    "      \n",
    "      if isinstance(alg, NEURAL_CONTROLLER_DRIVER):\n",
    "        alg.reset()\n",
    "        bandit = TaskSelector(alg, tasks, reward_signal, number_of_steps_of_selecting_tasks)\n",
    "\n",
    "        reward_dict[alg.name].append([0.])\n",
    "        episode_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        success_student_episode = False\n",
    "        capability = alg._initial_state_controller\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          print('Steps: ',i)\n",
    "          action = alg.step_controller(reward, 0., capability) if success_student_episode == True else alg.step_controller(reward, 0.98, capability)\n",
    "          # Epsilon Decay on the Teacher? BUT ON EACH STEP( NOT EPISODE )\n",
    "          eps = min(1.0, max(0.1, 1.0 - (i - 0.)/(number_of_steps_of_selecting_tasks/8.)))\n",
    "          alg._eps_controller = eps\n",
    "#             action = alg.step_controller(reward, 0.98, capability)\n",
    "          prob = alg.getProbs()\n",
    "          entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "          \n",
    "          reward, reward_from_environment, capability, success_student_episode = bandit.step(action)\n",
    "          bandit.resetReplayBuffer()\n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          episode_dict[alg.name][-1].append(success_student_episode+episode_dict[alg.name][-1][-1])\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "        \n",
    "        \n",
    "        h, w = tasks[-1]._layout.shape\n",
    "        obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "        if qs is not None:\n",
    "          qs += np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        else:\n",
    "          qs = np.array([[[alg.q_driver(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        \n",
    "      else:\n",
    "        alg.reset()\n",
    "        rl_agent = None\n",
    "        \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          rl_agent = NS_DQN(teacher_student=False,\n",
    "                      number_of_features=(2*vision_size + 1)**2,\n",
    "                      number_of_hidden=hidden_units,\n",
    "                      number_of_actions=4,\n",
    "                      initial_state=tasks[0].get_obs(),\n",
    "                      rl_alg=agent_type_driver,\n",
    "                      step_size=step_size)\n",
    "        elif agent_type_driver == 'Q':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=Q_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        elif agent_type_driver == 'SARSA':\n",
    "          rl_agent = GeneralQ(number_of_states=tasks[0]._layout.size,\n",
    "                  number_of_actions=4,\n",
    "                  initial_state=tasks[0].get_obs(),\n",
    "                  target_policy=SARSA_target_policy,\n",
    "                  behaviour_policy=gen_behaviour_policy,\n",
    "                  double=True)\n",
    "        \n",
    "        bandit = TaskSelector(rl_agent, tasks, reward_signal, number_of_steps_of_selecting_tasks)\n",
    "        \n",
    "        reward_dict[alg.name].append([0.])\n",
    "        episode_dict[alg.name].append([0.])\n",
    "        reward_delta_dict[alg.name].append([])\n",
    "        action_dict[alg.name].append([])\n",
    "        prob_dict[alg.name].append([])\n",
    "        entropy_dict[alg.name].append([])\n",
    "        action = None\n",
    "        reward = None\n",
    "        prob = None\n",
    "        entropy = None\n",
    "        reward_delta = None\n",
    "        success_student_episode = False\n",
    "        capability = None\n",
    "        \n",
    "        if 'HYPER' in alg.name: \n",
    "          capability = alg._initial_state\n",
    "        \n",
    "        for i in range(number_of_steps_of_selecting_tasks):\n",
    "          print('Steps: ',i)\n",
    "          try:\n",
    "            # This is for when the teacher is neural and student not\n",
    "            if 'HYPER' in alg.name:\n",
    "              action, _, _ = alg.step(reward, 0., capability) if success_student_episode == True else alg.step(reward, 0.98, capability)\n",
    "#               action, _, _ = alg.step(reward, 0.98, capability)\n",
    "            else:\n",
    "              action = alg.step(action, reward)\n",
    "            prob = alg.getProbs()\n",
    "            entropy = -1.*np.sum(prob*np.log(prob))\n",
    "          except:\n",
    "            raise ValueError(\n",
    "                \"The step function of algorithm `{}` failed.\\\n",
    "                Perhaps you have a bug, such as a typo.\\\n",
    "                Or, perhaps your value estimates or policy has diverged.\\\n",
    "                (E.g., internal quantities may have become NaNs.)\\\n",
    "                Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "          reward, reward_from_environment, capability, success_student_episode = bandit.step(action)\n",
    "          #print(reward_from_environment)\n",
    "          bandit.resetReplayBuffer()\n",
    "          \n",
    "          reward_dict[alg.name][-1].append(reward_from_environment+reward_dict[alg.name][-1][-1])\n",
    "          episode_dict[alg.name][-1].append(success_student_episode+episode_dict[alg.name][-1][-1])\n",
    "          #print(reward_dict[alg.name][-1])\n",
    "          reward_delta_dict[alg.name][-1].append(reward)\n",
    "          action_dict[alg.name][-1].append(action)\n",
    "          prob_dict[alg.name][-1].append(prob.copy())\n",
    "          entropy_dict[alg.name][-1].append(entropy)\n",
    "          \n",
    "        if agent_type_driver == 'NEURALSARSA' or agent_type_driver == 'DQN':\n",
    "          h, w = tasks[-1]._layout.shape\n",
    "          obs = np.array([[tasks[-1].get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "          if qs is not None:\n",
    "            qs += np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "          else:\n",
    "            qs = np.array([[[rl_agent.q_noProbs(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "        elif agent_type_driver == 'Q' or agent_type_driver == 'SARSA':\n",
    "          if qs is not None:\n",
    "            qs += rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "          else:\n",
    "            qs = rl_agent.q_values.reshape(tasks[-1]._layout.shape + (4,))\n",
    "      #print('')      \n",
    "      \n",
    "#       print(completed_train_episodes.shape, bandit._tasks_episodes_completed_train.shape)\n",
    "      completed_train_episodes += bandit._tasks_episodes_completed_train\n",
    "      completed_test_episodes += bandit._tasks_episodes_completed_test\n",
    "      train_task_accuracy += bandit._train_task_accuracy\n",
    "      test_task_accuracy += bandit._test_task_accuracy\n",
    "      task_slopes += bandit._tasks_slopes\n",
    "    \n",
    "    completed_train_episodes /= repetitions\n",
    "    completed_test_episodes /= repetitions\n",
    "    train_task_accuracy /= repetitions\n",
    "    test_task_accuracy /= repetitions\n",
    "    task_slopes /= repetitions\n",
    "    qs /= repetitions\n",
    "    \n",
    "    if isinstance(alg, NEURAL_CONTROLLER_DRIVER):\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, alg._rl_alg_driver, alg.name, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, alg._rl_alg_driver, alg.name, reward_signal)\n",
    "      plot_task_slopes(task_slopes, alg._rl_alg_driver, alg.name, reward_signal)\n",
    "    else:\n",
    "      plot_completed_episodes(completed_train_episodes, completed_test_episodes, agent_type_driver, alg.name, reward_signal)\n",
    "      plot_task_accuracy(train_task_accuracy, test_task_accuracy, agent_type_driver, alg.name, reward_signal)\n",
    "      plot_task_slopes(task_slopes, agent_type_driver, alg.name, reward_signal)\n",
    "    plot_action_values(qs, alg.name)\n",
    "        \n",
    "  return reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict, episode_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type_driver='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, reward_delta_dict, action_dict, prob_dict, entropy_dict, episode_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, repetitions, vision_size, tabular, agent_type_driver, hidden_units, step_size, reward_signal)\n",
    "  \n",
    "  smoothed_rewards = {}\n",
    "  smoothed_rewards_stds = {}\n",
    "  \n",
    "  smoothed_reward_deltas = {}\n",
    "  smoothed_reward_deltas_stds = {}\n",
    "  \n",
    "  smoothed_actions = {}\n",
    "  \n",
    "  smoothed_probs = {}\n",
    "  \n",
    "  smoothed_entropies = {}\n",
    "  smoothed_entropies_stds = {}\n",
    "  \n",
    "  smoothed_episodes = {}\n",
    "  smoothed_episodes_stds = {}\n",
    "  \n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_rewards_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, rewards in reward_delta_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_reward_deltas[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_reward_deltas_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    \n",
    "  for agent, rewards in episode_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_episodes[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "    smoothed_episodes_stds[agent] = (np.std(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "  \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    smoothed_entropies_stds[agent] = (np.std(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "    \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    \n",
    "    plt.figure(figsize=(44,40))\n",
    "    plt.imshow(smoothed_probs[agent], interpolation=None)\n",
    "    plt.title('Teacher: {}, Student: {}, Reward Signal: {}'.format(agent, agent_type_driver, reward_signal))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "#     plt.savefig('./'+agent + ', Reward Signal: {}; {}'.format(reward_signal, agent_type_driver_driver))\n",
    "#     plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Reward')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    plot = plt.plot(smoothed_rewards[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_rewards[agent].shape[0]), smoothed_rewards[agent]-smoothed_rewards_stds[agent], smoothed_rewards[agent]+smoothed_rewards_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Average Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Delta Average Reward, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Delta')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_reward_deltas[agent] /= repetitions    \n",
    "    plt.plot(smoothed_reward_deltas[agent], label=agent)\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Delta Reward, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Entropy, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_entropies[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_entropies[agent].shape[0]), smoothed_entropies[agent]-smoothed_entropies_stds[agent], smoothed_entropies[agent]+smoothed_entropies_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Maximum Likelihood, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()\n",
    "\n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Cumulative Average Episode Success, Student: {}, Reward Signal: {}'.format(agent_type_driver, reward_signal))\n",
    "  plt.ylabel('Episodes')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_episodes[agent] /= repetitions  \n",
    "    plot = plt.plot(smoothed_episodes[agent], label=agent)\n",
    "    plt.fill_between(np.arange(smoothed_episodes[agent].shape[0]), smoothed_episodes[agent]-smoothed_episodes_stds[agent], smoothed_episodes[agent]+smoothed_episodes_stds[agent], alpha=0.15, facecolor=plot[-1].get_color())\n",
    "  plt.legend(loc='upper right')\n",
    "#   plt.savefig('./Maximum Likelihood, \\t Reward Signal: {}; {}'.format(reward_signal, agent_type_driver))\n",
    "#   plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wTRJuSa_j1wC"
   },
   "outputs": [],
   "source": [
    "def run_step(env, teacher_action, agent, train):     \n",
    "    env.resetState()\n",
    "    agent.resetState()\n",
    "    number_of_steps = env.distanceToGoal(teacher_action)\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "      agent_inventory = agent._inventory\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      agent_inventory = agent._inventory\n",
    "    steps_completed = 0.\n",
    "    total_reward = 0.\n",
    "    while steps_completed != number_of_steps:\n",
    "      reward, discount, next_state, item = env.step(action, teacher_action, agent_inventory)\n",
    "      \n",
    "      if item != None:\n",
    "        agent._inventory.add(item)\n",
    "        \n",
    "      total_reward += reward \n",
    "      \n",
    "      action, agent_inventory, _ = agent.step(reward, discount, next_state, item, train, steps_completed)\n",
    "      if discount == 0.:\n",
    "        print(train, 'EPISODE COMPLETED')\n",
    "        return total_reward, total_reward/steps_completed, True, 0.\n",
    "    \n",
    "      steps_completed += 1.\n",
    "    \n",
    "    mean_reward = total_reward/number_of_steps\n",
    "    \n",
    "#     print(float(env.distanceLeft(teacher_action)))\n",
    "    return total_reward, mean_reward, False, float(env.distanceLeft(teacher_action))\n",
    "  \n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1672I5Z7j1wE"
   },
   "source": [
    "# Reward as Reward Signal for Bandit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK8BVplBj1wE"
   },
   "source": [
    "### NeuralRL/Bandit Controllers with Neural RL Agents\n",
    "#### NeuralRL Controller state input is the buffered reward across all tasks within 5 timesteps\n",
    "#### NeuralRL Controller does not have inventory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2SsYpc2SRPe4"
   },
   "outputs": [],
   "source": [
    "number_of_steps_of_selecting_tasks = 500\n",
    "reps = 3\n",
    "\n",
    "reward_signals=['SPG']\n",
    "\n",
    "\n",
    "hidden_units_controller_net = 30\n",
    "hidden_units_driver_net = 30\n",
    "FIFO_buffer_scores_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 6907
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9174498,
     "status": "ok",
     "timestamp": 1533232808405,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "dDzz94z_iRXz",
    "outputId": "3e9672fa-e83c-4630-bc7c-92e96fe7cbdd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Running:', 'HYPER NEURALSARSA')\n",
      "('Rep:', 0)\n",
      "('Steps: ', 0)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "(False, 'EPISODE COMPLETED')\n",
      "(False, 'EPISODE COMPLETED')\n",
      "(False, 'EPISODE COMPLETED')\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 0', 2.4101609596982602e-12, -440.99999820384141, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 1)\n",
      "('Task: 0', 3.5769703572441362e-07, -440.99999999233609, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 2)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 3', 7.0922379586590983e-05, -441.0, array([-441.   , -441.   , -441.   , -440.999, -441.   ]))\n",
      "('Steps: ', 3)\n",
      "('Task: 3', 0.00018426390194576926, -440.99999999673531, array([-441.   , -441.   , -441.   , -441.   , -440.999]))\n",
      "('Steps: ', 4)\n",
      "('Task: 0', 4.0682229496269429e-07, -440.99999527550852, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 5)\n",
      "('Task: 1', 1.788562826732232e-08, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 6)\n",
      "('Task: 3', 8.3489101757550099e-05, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 7)\n",
      "('Task: 1', 2.6445678713571399e-08, -440.99999999809216, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 8)\n",
      "('Task: 3', 3.920497674698709e-05, -440.99999999999233, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 9)\n",
      "('Task: 3', 3.2416664907941596e-10, -440.99999999999233, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 10)\n",
      "('Task: 0', 5.8042957107318216e-08, -440.99999995823651, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 11)\n",
      "('Task: 2', 85.516376766015611, -440.99990760713905, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 12)\n",
      "('Task: 4', 2.222543571406277e-09, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 13)\n",
      "('Task: 4', 1.9647848148451887e-08, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 14)\n",
      "('Task: 0', 3.5275617165098084e-07, -440.99999998467206, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 15)\n",
      "('Task: 3', 6.4974869928846601e-10, -440.99999999998789, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 16)\n",
      "('Task: 0', 4.67850776431078e-07, -440.9999999770082, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 17)\n",
      "('Task: 1', 8.7225600964302427e-09, -440.99999991042921, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 18)\n",
      "('Task: 0', 9.4524266955886562e-07, -440.99999999233603, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 19)\n",
      "('Task: 0', 3.419745553401299e-09, -440.99999997150326, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 20)\n",
      "('Task: 0', 3.3316521239612486e-07, -440.99999832159847, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 21)\n",
      "('Task: 1', 8.7753960542613637e-09, -440.99999999923688, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 22)\n",
      "('Task: 2', 128.27458292156066, -440.99999646982536, array([-441.   , -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 23)\n",
      "('Task: 3', 3.751665644813329e-12, -440.99999999998346, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 24)\n",
      "('Task: 2', 128.2745924783469, -440.99999982241326, array([-441.   , -441.   , -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 25)\n",
      "('Task: 2', 85.516404466115219, -441.0, array([-441.   , -441.   , -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 26)\n",
      "('Task: 2', 0.060107891146793692, -440.69936638631776, array([-440.699, -441.   , -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 27)\n",
      "('Task: 2', 0.03006267534747167, -440.99999981113575, array([-441.   , -440.699, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 28)\n",
      "('Task: 1', 1.1447127690189519e-10, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 29)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 0', 5.2173139692968111e-07, -440.99999820372, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 30)\n",
      "('Task: 2', 2.1085529056108498e-07, -440.99999886256893, array([-441.   , -441.   , -440.699, -441.   , -441.   ]))\n",
      "('Steps: ', 31)\n",
      "('Task: 2', 0.03006270896520391, -440.99999730670044, array([-441.   , -441.   , -441.   , -440.699, -441.   ]))\n",
      "('Steps: ', 32)\n",
      "('Task: 2', 0.060126472292915879, -441.0, array([-441.   , -441.   , -441.   , -441.   , -440.699]))\n",
      "('Steps: ', 33)\n",
      "('Task: 2', 1.5859058066780563e-08, -440.99999916312493, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 34)\n",
      "('Task: 2', 4.184375370641647e-08, -440.99999772513792, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 35)\n",
      "('Task: 0', 5.3526858891927984e-07, -440.99999819988471, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 36)\n",
      "('Task: 3', 2.390265763096977e-10, -440.99999999880163, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 37)\n",
      "('Task: 0', 8.0048209838423655e-09, -440.99999999233603, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 38)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 3.1117370440369997e-07, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 39)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395160900288, -13.41802377706102, array([ -13.418, -441.   , -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 40)\n",
      "('Task: 4', 1.0544511042098748e-08, -440.99999982124842, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 41)\n",
      "('Task: 3', 1.1886527317983563e-10, -440.99999999999113, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 42)\n",
      "('Task: 2', 42.758197621224639, -440.99999803104021, array([-441.   ,  -13.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 43)\n",
      "('Task: 3', 9.833911462919787e-13, -440.99999999997914, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 44)\n",
      "('Task: 3', 1.2105942914786284e-10, -441.0, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 45)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 5.3097272711966076e-08, -440.99999844413145, array([-441.   , -441.   ,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 46)\n",
      "('Task: 2', 42.758197071845828, -440.99999802569391, array([-441.   , -441.   , -441.   ,  -13.418, -441.   ]))\n",
      "('Steps: ', 47)\n",
      "('Task: 2', 85.516395207559711, -440.99999981753274, array([-441.   , -441.   , -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 48)\n",
      "('Task: 2', 7.5278364874975526e-09, -440.99999730670038, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 49)\n",
      "('Task: 2', 2.357755874982104e-08, -440.99999868574042, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 50)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 2.4339972810594189e-07, -440.99999980858871, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 51)\n",
      "('Task: 2', 5.9196082702328567e-08, -440.99999886256899, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 52)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 3', 2.3967459128471094e-10, -440.99999999999557, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 53)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 5.2082543220421942e-07, -440.99999982241326, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 54)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516394980353425, -13.41802377706102, array([ -13.418, -441.   , -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 55)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459271485634, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 56)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459262163683, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 57)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395209070467, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 58)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 59)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 60)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 61)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 62)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.39999543081737132, -15.418000931147876, array([-15.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 63)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19999771540868566, -13.41802377706102, array([-13.418, -15.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 64)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.418, -13.418, -13.418]))\n",
      "('Steps: ', 65)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19999771540868566, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.418, -13.418]))\n",
      "('Steps: ', 66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task: 3', 0.00037682444256574858, -440.99811587777009, array([-440.998, -441.   , -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 67)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.116399782983507, -440.99999984606541, array([-441.   ,  -13.418,  -13.418,  -13.418,  -15.418]))\n",
      "('Steps: ', 68)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197606900438, -13.41802377706102, array([ -13.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 69)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([ -13.418,  -13.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 70)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197606900438, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 71)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395213800877, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 72)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 73)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.344344271050232, -440.13974513231216, array([-440.14 ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 74)\n",
      "('Task: 2', 128.18856715262669, -440.99999886256893, array([-441.   , -440.14 ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 75)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.2745922981662, -440.99999772513792, array([-441.   , -441.   , -440.14 ,  -13.418,  -13.418]))\n",
      "('Steps: ', 76)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.086025259282575928, -13.41802377706102, array([ -13.418, -441.   , -441.   , -440.14 ,  -13.418]))\n",
      "('Steps: ', 77)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.10254177960101, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   , -440.14 ]))\n",
      "('Steps: ', 78)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459241190928, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 79)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.51639478961539, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 80)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 81)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 82)\n",
      "('Task: 0', 1.5530537211816409e-07, -440.99999820381731, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 83)\n",
      "('Task: 3', 0.00018841323431502134, -440.99999999492252, array([-441.   , -440.998, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 84)\n",
      "('Task: 2', 85.516395130113381, -440.99999942762793, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 85)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459255965124, -440.9999987500338, array([-441.   , -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 86)\n",
      "('Task: 2', 128.27457418790527, -440.99990723010092, array([-441.   , -441.   , -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 87)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 9.2197527010284837e-06, -13.41802377706102, array([ -13.418, -441.   , -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 88)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459262741067, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 89)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27458333989853, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 90)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516376690607984, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 91)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 92)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 93)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 94)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 95)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 96)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 97)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 98)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 99)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 100)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 101)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 102)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 103)\n",
      "('Task: 0', 4.2031729208247271e-10, -440.99999820385528, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 104)\n",
      "('Task: 2', 85.516394767108324, -440.99999761260267, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 105)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197383554169, -13.41802377706102, array([ -13.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 106)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 2.8421709430404009e-15, -13.41802377706102, array([ -13.418,  -13.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 107)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197383554169, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 108)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516394767108338, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 109)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 110)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 111)\n",
      "('Task: 3', 0.0010429273496129099, -440.99478536578846, array([-440.995, -441.   , -440.998, -441.   , -441.   ]))\n",
      "('Steps: ', 112)\n",
      "('Task: 2', 85.516394850795848, -440.99999803104021, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 113)\n",
      "('Task: 2', 128.27459221501331, -440.99999772513792, array([-441.   , -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 114)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.2745915631449, -440.99999461874705, array([-441.   , -441.   , -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 115)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.352861979608278, -440.18233538124895, array([-440.182, -441.   , -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 116)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.598161085184728, -13.41802377706102, array([ -13.418, -440.182, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 117)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.274591873784, -13.41802377706102, array([ -13.418,  -13.418, -440.182, -441.   , -441.   ]))\n",
      "('Steps: ', 118)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.19282532875602, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -440.182, -441.   ]))\n",
      "('Steps: ', 119)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 84.953111789756278, -15.416776432467561, array([ -15.417,  -13.418,  -13.418,  -13.418, -440.182]))\n",
      "('Steps: ', 120)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19987526554065413, -13.41802377706102, array([-13.418, -15.417, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 121)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.417, -13.418, -13.418]))\n",
      "('Steps: ', 122)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19987526554065413, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.417, -13.418]))\n",
      "('Steps: ', 123)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task: 0', 1.7808770280680619e-07, -440.99999820368657, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 124)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.116644486020292, -440.99999886256899, array([-441.   ,  -13.418,  -13.418,  -13.418,  -15.417]))\n",
      "('Steps: ', 125)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197508550801, -13.41802377706102, array([ -13.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 126)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 2.8421709430404009e-15, -13.41802377706102, array([ -13.418,  -13.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 127)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197508550801, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 128)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395017101601, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 129)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 130)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 131)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 132)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 133)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 134)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 135)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 136)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 137)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 138)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 139)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 140)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 141)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -14.41802377706102, array([-14.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 142)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -14.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 143)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -14.418, -13.418, -13.418]))\n",
      "('Steps: ', 144)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 145)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -14.418]))\n",
      "('Steps: ', 146)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 147)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 148)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 149)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 150)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.60000000000000009, -16.41802377706102, array([-16.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 151)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.30000000000000004, -13.41802377706102, array([-13.418, -16.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 152)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -16.418, -13.418, -13.418]))\n",
      "('Steps: ', 153)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.30000000000000004, -13.41802377706102, array([-13.418, -13.418, -13.418, -16.418, -13.418]))\n",
      "('Steps: ', 154)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 82.593734917068048, -429.38669836240121, array([-429.387,  -13.418,  -13.418,  -13.418,  -16.418]))\n",
      "('Steps: ', 155)\n",
      "('Task: 2', 127.11326270312182, -441.0, array([-441.   , -429.387,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 156)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197622293899, -13.41802377706102, array([ -13.418, -441.   , -429.387,  -13.418,  -13.418]))\n",
      "('Steps: ', 157)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 41.396867458534018, -14.41802377706102, array([ -14.418,  -13.418, -441.   , -429.387,  -13.418]))\n",
      "('Steps: ', 158)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 125.85193253936194, -13.41802377706102, array([ -13.418,  -14.418,  -13.418, -441.   , -429.387]))\n",
      "('Steps: ', 159)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.316395244587795, -14.41802377706102, array([ -14.418,  -13.418,  -14.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 160)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -14.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 161)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -14.418, -13.418, -14.418]))\n",
      "('Steps: ', 162)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 163)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.316341926250402, -440.99973340831298, array([-441.   ,  -13.418,  -13.418,  -13.418,  -14.418]))\n",
      "('Steps: ', 164)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758170963125195, -13.41802377706102, array([ -13.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 165)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([ -13.418,  -13.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 166)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758170963125202, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 167)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516341926250391, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 168)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 169)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 170)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 171)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 172)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 173)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 174)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 175)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 176)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -14.41802377706102, array([-14.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 177)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -14.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 178)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -14.418, -13.418, -13.418]))\n",
      "('Steps: ', 179)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 180)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -14.418]))\n",
      "('Steps: ', 181)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.39999938162837484, -15.418020685202894, array([-15.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 182)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19999969081418742, -13.41802377706102, array([-13.418, -15.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 183)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.418, -13.418, -13.418]))\n",
      "('Steps: ', 184)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19999969081418742, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.418, -13.418]))\n",
      "('Steps: ', 185)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.03702385194927018, -15.232901425456543, array([-15.233, -13.418, -13.418, -13.418, -15.418]))\n",
      "('Steps: ', 186)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.18148776483955231, -13.41802377706102, array([-13.418, -15.233, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 187)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.39996623805323001, -15.41785496732717, array([-15.418, -13.418, -15.233, -13.418, -13.418]))\n",
      "('Steps: ', 188)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task: 1', 8.5463057075685362e-09, -440.99999991057524, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 189)\n",
      "('Task: 2', 85.534869173673926, -440.99989287449529, array([-441.   ,  -15.418,  -13.418,  -15.233,  -13.418]))\n",
      "('Steps: ', 190)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 127.91160515624102, -440.9999926579444, array([-441.   , -441.   ,  -15.418,  -13.418,  -15.233]))\n",
      "('Steps: ', 191)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 127.88032759069938, -440.02859288524922, array([-440.029, -441.   , -441.   ,  -15.418,  -13.418]))\n",
      "('Steps: ', 192)\n",
      "('Task: 2', 85.019297931359375, -440.99999461874705, array([-441.   , -440.029, -441.   , -441.   ,  -15.418]))\n",
      "('Steps: ', 193)\n",
      "('Task: 2', 2.1585099983667536e-05, -440.99999981959388, array([-441.   , -441.   , -440.029, -441.   , -441.   ]))\n",
      "('Steps: ', 194)\n",
      "('Task: 2', 0.097141540567423593, -440.99999689360919, array([-441.   , -441.   , -441.   , -440.029, -441.   ]))\n",
      "('Steps: ', 195)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 1', 8.8369688455713916e-09, -440.99999991057518, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 196)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.322113594151432, -13.41802377706102, array([ -13.418, -441.   , -441.   , -441.   , -440.029]))\n",
      "('Steps: ', 197)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459177259047, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 198)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459252016138, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 199)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516394623309637, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 200)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 201)\n",
      "('Task: 1', 2.6676536890590798e-08, -440.9999999105666, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 202)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 4.1863794585486813, -34.349921069804424, array([-34.35 , -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 203)\n",
      "('Task: 2', 87.609584906993987, -440.99999966565929, array([-441.   ,  -34.35 ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 204)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459258238514, -440.99999874468756, array([-441.   , -441.   ,  -34.35 ,  -13.418,  -13.418]))\n",
      "('Steps: ', 205)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 126.1814023937045, -440.9999969081419, array([-441.   , -441.   , -441.   ,  -34.35 ,  -13.418]))\n",
      "('Steps: ', 206)\n",
      "('Task: 4', 1.015640521018213e-07, -440.99999924959292, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 207)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 4.1863797343004139, -13.41802377706102, array([ -13.418, -441.   , -441.   , -441.   ,  -34.35 ]))\n",
      "('Steps: ', 208)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459267448231, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 209)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459230663339, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 210)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516394626216169, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 211)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 212)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 213)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 214)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 215)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 216)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 217)\n",
      "('Task: 1', 2.0070831396878932e-08, -440.99999989965016, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 218)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516356762698848, -440.99980759055524, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 219)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.95817838134942, -14.41802377706102, array([ -14.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 220)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000285, -13.41802377706102, array([ -13.418,  -14.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 221)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758178381349417, -13.41802377706102, array([ -13.418,  -13.418,  -14.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 222)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.616356762698842, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -14.418, -441.   ]))\n",
      "('Steps: ', 223)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -14.418]))\n",
      "('Steps: ', 224)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 225)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.39975053108130826, -15.416776432467561, array([-15.417, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 226)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19987526554065413, -13.41802377706102, array([-13.418, -15.417, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 227)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.417, -13.418, -13.418]))\n",
      "('Steps: ', 228)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19987526554065413, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.417, -13.418]))\n",
      "('Steps: ', 229)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.39975053108130826, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -15.417]))\n",
      "('Steps: ', 230)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 231)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 232)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 233)\n",
      "('Task: 0', 3.5772233104580668e-07, -440.99999820378974, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 234)\n",
      "('Task: 2', 85.516395205687175, -440.99999980549688, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 235)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.2745926199452, -440.99999886256893, array([-441.   , -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 236)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197508550793, -13.41802377706102, array([ -13.418, -441.   , -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 237)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.75819760284358, -13.41802377706102, array([ -13.418,  -13.418, -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 238)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.27459271423797, -13.41802377706102, array([ -13.418,  -13.418,  -13.418, -441.   , -441.   ]))\n",
      "('Steps: ', 239)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395017101601, -13.41802377706102, array([ -13.418,  -13.418,  -13.418,  -13.418, -441.   ]))\n",
      "('Steps: ', 240)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 241)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 242)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 243)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 244)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 246)\n",
      "('Task: 0', 4.8373749450547626e-12, -440.99999820387427, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 247)\n",
      "('Task: 2', 85.516395244587798, -441.0, array([-441.   ,  -13.418,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 248)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758197622293906, -13.41802377706102, array([ -13.418, -441.   ,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 249)\n",
      "('Task: 3', 0.00037209748414852585, -440.99980476856564, array([-441.   , -440.995, -441.   , -440.998, -441.   ]))\n",
      "('Steps: ', 250)\n",
      "('Task: 2', 85.516395244587798, -441.0, array([-441.   ,  -13.418, -441.   ,  -13.418,  -13.418]))\n",
      "('Steps: ', 251)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.516395054705754, -440.99999905058979, array([-441.   , -441.   ,  -13.418, -441.   ,  -13.418]))\n",
      "('Steps: ', 252)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 42.758179116102511, -440.99990794374816, array([-441.   , -441.   , -441.   ,  -13.418, -441.   ]))\n",
      "('Steps: ', 253)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 27.799990794374811, -152.41802377706099, array([-152.418, -441.   , -441.   , -441.   ,  -13.418]))\n",
      "('Steps: ', 254)\n",
      "('Task: 1', 1.0930591543001355e-09, -440.99999991057246, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 255)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 114.37459277194068, -13.41802377706102, array([ -13.418, -152.418, -441.   , -441.   , -441.   ]))\n",
      "('Steps: ', 256)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 128.07458347137447, -14.41802377706102, array([ -14.418,  -13.418, -152.418, -441.   , -441.   ]))\n",
      "('Steps: ', 257)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 99.316376833337429, -13.41802377706102, array([ -13.418,  -14.418,  -13.418, -152.418, -441.   ]))\n",
      "('Steps: ', 258)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 27.799999999999997, -13.41802377706102, array([ -13.418,  -13.418,  -14.418,  -13.418, -152.418]))\n",
      "('Steps: ', 259)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 260)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -14.418]))\n",
      "('Steps: ', 261)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 262)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 263)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 264)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 265)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 266)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 267)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 268)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 269)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 270)\n",
      "('Task: 4', 1.4143323028292799e-07, -440.99999966803739, array([-441., -441., -441., -441., -441.]))\n",
      "('Steps: ', 271)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 272)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -14.41802377706102, array([-14.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 273)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -14.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 274)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -14.418, -13.418, -13.418]))\n",
      "('Steps: ', 275)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -14.418, -13.418]))\n",
      "('Steps: ', 276)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.20000000000000001, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -14.418]))\n",
      "('Steps: ', 277)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 278)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.38637945854868011, -15.34992106980442, array([-15.35 , -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 279)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19318972927434006, -13.41802377706102, array([-13.418, -15.35 , -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 280)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.35 , -13.418, -13.418]))\n",
      "('Steps: ', 281)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19318972927434006, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.35 , -13.418]))\n",
      "('Steps: ', 282)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.38637945854868011, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -15.35 ]))\n",
      "('Steps: ', 283)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 284)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 285)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 286)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.299357055118389, -14.914809052652965, array([-14.915, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 287)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.1496785275591945, -13.41802377706102, array([-13.418, -14.915, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 288)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -14.915, -13.418, -13.418]))\n",
      "('Steps: ', 289)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.23670093098948561, -15.34992106980442, array([-15.35 , -13.418, -13.418, -14.915, -13.418]))\n",
      "('Steps: ', 290)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.10616732584404893, -13.41802377706102, array([-13.418, -15.35 , -13.418, -13.418, -14.915]))\n",
      "('Steps: ', 291)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -15.35 , -13.418, -13.418]))\n",
      "('Steps: ', 292)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.19318972927434006, -13.41802377706102, array([-13.418, -13.418, -13.418, -15.35 , -13.418]))\n",
      "('Steps: ', 293)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.38637945854868011, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -15.35 ]))\n",
      "('Steps: ', 294)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 295)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.0, -13.41802377706102, array([-13.418, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 296)\n",
      "(False, 'EPISODE COMPLETED')\n",
      "('Task: 2', 0.299357055118389, -14.914809052652965, array([-14.915, -13.418, -13.418, -13.418, -13.418]))\n",
      "('Steps: ', 297)\n",
      "('Task: 3', 0.00023874267172345755, -440.99940720430715, array([-440.999, -441.   , -440.995, -441.   , -440.998]))\n",
      "('Steps: ', 298)\n",
      "(True, 'EPISODE COMPLETED')\n",
      "('Task: 2', 85.666073496277576, -440.99999862065289, array([-441.   ,  -14.915,  -13.418,  -13.418,  -13.418]))\n",
      "('Steps: ', 299)\n"
     ]
    }
   ],
   "source": [
    "step_size_controller = 0.01\n",
    "step_size_driver = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "\n",
    "for task in tasks:\n",
    "    task.plot_grid()\n",
    "    \n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "number_of_arms_tasks = len(tasks[0]._goal_loc)\n",
    "\n",
    "\n",
    "agents = [\n",
    "    NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*FIFO_buffer_scores_size,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*FIFO_buffer_scores_size)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'NEURALSARSA',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=0, \n",
    "                              num_offline_updates_driver=20,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "  NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*FIFO_buffer_scores_size,\n",
    "                              (2*vision_size + 1)**2,\n",
    "                              hidden_units_controller_net,\n",
    "                              hidden_units_driver_net,\n",
    "                              number_of_arms_tasks,\n",
    "                              4,\n",
    "                              np.zeros((1,number_of_arms_tasks*FIFO_buffer_scores_size)),\n",
    "                              tasks[0].get_obs(),\n",
    "                              'DQN',\n",
    "                              'DQN',\n",
    "                              num_offline_updates_controller=0, \n",
    "                              num_offline_updates_driver=20,\n",
    "                              step_size_controller=step_size_controller,\n",
    "                              step_size_driver=step_size_driver),\n",
    "#   NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*FIFO_buffer_scores_size,\n",
    "#                               (2*vision_size + 1)**2,\n",
    "#                               hidden_units_controller_net,\n",
    "#                               hidden_units_driver_net,\n",
    "#                               number_of_arms_tasks,\n",
    "#                               4,\n",
    "#                               np.zeros((1,number_of_arms_tasks*FIFO_buffer_scores_size)),\n",
    "#                               tasks[0].get_obs(),\n",
    "#                               'DQN',\n",
    "#                               'NEURALSARSA',\n",
    "#                               num_offline_updates_controller=25, \n",
    "#                               num_offline_updates_driver=25,\n",
    "#                               step_size_controller=step_size_controller,\n",
    "#                               step_size_driver=step_size_driver),\n",
    "#     NEURAL_CONTROLLER_DRIVER(number_of_arms_tasks*FIFO_buffer_scores_size,\n",
    "#                               (2*vision_size + 1)**2,\n",
    "#                               hidden_units_controller_net,\n",
    "#                               hidden_units_driver_net,\n",
    "#                               number_of_arms_tasks,\n",
    "#                               4,\n",
    "#                               np.zeros((1,number_of_arms_tasks*FIFO_buffer_scores_size)),\n",
    "#                               tasks[0].get_obs(),\n",
    "#                               'NEURALSARSA',\n",
    "#                               'NEURALSARSA',\n",
    "#                               num_offline_updates_controller=25, \n",
    "#                               num_offline_updates_driver=25,\n",
    "#                               step_size_controller=step_size_controller,\n",
    "#                               step_size_driver=step_size_driver),\n",
    "    Random(number_of_arms_tasks),\n",
    "]\n",
    "\n",
    "\n",
    "for reward_signal in reward_signals:\n",
    "  train_task_agents(agents,\n",
    "                    number_of_arms_tasks,\n",
    "                    number_of_steps_of_selecting_tasks, \n",
    "                    tasks,\n",
    "                    reward_signal,\n",
    "                    reps,\n",
    "                    vision_size,\n",
    "                    tabular_grid,\n",
    "                    'DQN',\n",
    "                    hidden_units_driver_net)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XBBSjyIFRlD7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Grid_Experiments_SMALL_CIRCLE_LEAN.ipynb",
   "provenance": [
    {
     "file_id": "1a1ONHRz5bcd2rJyLUD53OUMR8npSp0QZ",
     "timestamp": 1522325021849
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1521476023411
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1517660129183
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1517174839485
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1515086437469
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1511098107887
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
