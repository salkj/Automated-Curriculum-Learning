{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYs6LMEbNqoQ"
   },
   "source": [
    "# Hallway Experiments in Curriculum Learning\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "Salkey, Jayson\n",
    "\n",
    "28/06/2018\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v_SYckYfv5G"
   },
   "source": [
    "## Context\n",
    "\n",
    "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
    "\n",
    "## Background reading\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 6-7 + 9-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0tQ4aiAaIu"
   },
   "source": [
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YzYtxi8Wh5SJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDhSYfSDcCC"
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOu9RZY3AkF1"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6EttQGJ1n5Zn"
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  vmin = np.min(values)\n",
    "  vmax = np.max(values)\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "\n",
    "def plot_rewards(xs, rewards, color):\n",
    "  mean = np.mean(rewards, axis=0)\n",
    "  p90 = np.percentile(rewards, 90, axis=0)\n",
    "  p10 = np.percentile(rewards, 10, axis=0)\n",
    "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
    "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
    "  \n",
    "\n",
    "def parameter_study(parameter_values, parameter_name,\n",
    "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
    "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  for rep in range(repetitions):\n",
    "    for i, p in enumerate(parameter_values):\n",
    "      env = env_constructor()\n",
    "      agent = agent_constructor()\n",
    "      if 'eps' in parameter_name:\n",
    "        agent.set_epsilon(p)\n",
    "      elif 'alpha' in parameter_name:\n",
    "        agent._step_size = p\n",
    "      else:\n",
    "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
    "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
    "      agent.set_epsilon(0.)\n",
    "      agent._step_size = 0.\n",
    "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
    "      del env\n",
    "      del agent\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_rewards(parameter_values, mean_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_rewards(parameter_values, greedy_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4v8_c7XqsEo"
   },
   "source": [
    "## Assignment 2 [50pts in total + 10 BONUS pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALrRR76eAd6u"
   },
   "source": [
    "### A grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YP97bVN3NuG8"
   },
   "outputs": [],
   "source": [
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, tabular=True, vision_size=1, discount=0.98, noisy=False):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = np.array([\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1, -5, -1, -1, -1, -1, -6, -1, -1, -1, -1, 10, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "    ])\n",
    "      \n",
    "    \n",
    "    self._start_state = (3, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "  \n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "    \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != -1, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(3, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==10)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==-5)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==-6)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{D}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      return location\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "    \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == -1:  # wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] != 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      reward = float((new_y + new_x)) / float(np.sum(self._layout.shape))\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hallway world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hallway(object):\n",
    "\n",
    "  def __init__(self, goal_loc, goal=5, tabular=True, vision_size=1, discount=0.98, noisy=False):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    \n",
    "    self.goal_loc_r = goal_loc[0]\n",
    "    self.goal_loc_c = goal_loc[1]\n",
    "    \n",
    "#     self._layout = np.array([\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#       [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "#     ])\n",
    "    \n",
    "    self._layout = np.array([\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "    ])\n",
    "    \n",
    "    self._layout[self.goal_loc_r,self.goal_loc_c] = goal\n",
    "    \n",
    "    self._start_state = (3, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "    self._tabular = tabular\n",
    "    self._vision_size = vision_size\n",
    "    self._discount = discount\n",
    "  \n",
    "  def resetState(self):\n",
    "    self._state = self._start_state\n",
    "    \n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != -1, interpolation=\"nearest\", cmap='pink')\n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(3, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    goal_y, goal_x = np.where(self._layout==5)\n",
    "    plt.text(goal_x, goal_y, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return self.get_obs_at(x, y)\n",
    "\n",
    "  def get_obs_at(self, x, y):\n",
    "    if self._tabular:\n",
    "      return y*self._layout.shape[1] + x\n",
    "    else:\n",
    "      v = self._vision_size\n",
    "      location = np.clip(-self._layout[y-v:y+v+1,x-v:x+v+1], 0, 1)\n",
    "      return location\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "    \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    discount = self._discount\n",
    "    if self._layout[new_y, new_x] == -1:  # wall\n",
    "      reward = -1\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] != 0: # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    else:\n",
    "      #reward = float((new_y + new_x)) / float(np.sum(self._layout.shape))\n",
    "      reward = 0\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
    "    \n",
    "    self._state = new_state\n",
    "\n",
    "    return reward, discount, self.get_obs()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaGeLcsvixmt"
   },
   "source": [
    "### The grid\n",
    "\n",
    "The cell below shows the `Grid` environment that we will use. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-1` for bumping into a wall, `+10` for reaching the goal, and `(x + y)/(height + width)` otherwise, which encourages the agent to go right and down.  The episode ends when the agent reaches the goal.  At the end of the left-most two corridors, there are distractor 'goals' (marked `D`) that give a reward of $-5$ and $-6$, and then also terminate the episode.  The discount, on continuing steps, is $\\gamma = 0.98$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1524136431807,
     "user": {
      "displayName": "Jayson Salkey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107749173479931199979"
     },
     "user_tz": -60
    },
    "id": "SlFuWFzIi5uB",
    "outputId": "4a01c9b4-5023-4f96-aa07-19c77587a25b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ecaacd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = Grid()\n",
    "grid.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hallway(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFnCG0ALOEFrAmX8AklPyQzYxF9AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef5f6c710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZwgt4AyhBZwhtIAzfwPOwfJDrBunrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef61e1ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCzhDaAFnCC3gzN8G7fJDY716iQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef5a03610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBZwhtACzvwNPwryQ17Os3UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ecaf93050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWsAZQgs48y93J/JDBIu2VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ecaf295d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDKEFnCG0gDP/Aq9E8kPVHuDVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef0d1b210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAB8RJREFUeJzt3V+IlFUYx/Hfk2ktrVu01jLUZqRgJKVkiF4EqxkImUEK\ndiH9cS+CQEIUb6Q/EGnixV4VQhlEUIiRrOuFXuhFdlHB2sUiQZSlJeWG+WclGilPFztrk86ss9Ps\nnPOc+X5gwX13nj3PHvnt++7M+zAWQhAAP26I3QCAiSG0gDOEFnCG0ALOEFrAGUILOENoE2Vmr5nZ\nhxHXHzGze6t87TkzO9LcjjDmxtgNtCozG5E09iL5LZKKkv4uHXuxdDzai+ghhOnXe0hTGsE1ONNG\nEkKYHkLoCCF0SDoh6YmyYx/H6svMpsRaG7UhtGmw0sfVbjKzD8zsgpkNmdnDVwrMCmb2iZkNm9n3\nZra+6jc3u93MBszsvJl9aWZvlF/emtllM3vJzL6V9G3ZsfvK6veV6r+QNKtRPzgmjtCm7UlJH0m6\nVdKApLclycys9PnXkgqSHpP0spk9XuX7vCNpRNKdkp6X9Jyuvbx9StJCSQ+UPi//+juS/pDUJalX\n0rr/8TPhfyK0afs8hHAwjN4g/qGkh0rHF0qaEUJ4M4TwdwjhR0nvSXrm6m9gZjdIelrSqyGEYgjh\nG0kfVFhrawjhXAihOFZ6Vf0rIYQ/QwjHqtSjSXgiKm2/lv37D0k3l0J0j6S7zOz30tdMo7+AP6vw\nPe6QNEXSz2XHfqrwuJ8rHKtWf0LSo9ftHpOC0Pr0k6TjIYQ5NTz2N0l/Sbpb0nelY90VHlft2eCx\n+m6V/t7V6C8NRMLlsS9jT1Z9JWnEzDab2c1mNsXM5prZI1cXhBAuS/pU0utm1mZm90t6ttYFK9Q/\noNG/iREJoU1Dra95BulKkFZImi/pB0nDkt6V1FGlbr2k2yT9otG/Rz/S6OvC461ffmy9pOml+vdL\nH4jEGIJvPWb2lqSuEMILsXvBxHGmbQFmNsfMHiz9e6FGX7b5NG5XqBdPRLWG6ZI+NrOCpNOSdoQQ\nBiL3hDpxeQw4w+Ux4My4l8dmxmkYiCSEUOl+9Ov/TTuRV9FPNqmmmWtRw/9Rs2vK6yrh8hhwhtAC\nzhBawBlCCzhDaAFnCC3gDKEFnCG0gDOEFnCG0ALOjDvlw73HQDzV7j3mTAs4w8AANZNS08y1cqsp\nr6uEMy3gDKEFnCG0gDOEFnCG0ALOEFrAGUILOENoAWcILeAMoQWcYWAASBQDA0Am3A8MTOQNxMxs\nwmulfGN5yntQb11u+8DAAIC83p+2WCxq3bp1mjlzpi5evKi2tjZt3749dltAQ2V1pu3v71ehUNDW\nrVvV29urs2fPxm4JaLisQnvy5EkdPnxYp06d0rx587R8+fLYLQENl1Voly1bpqGhIXV3d2vx4sVa\ntGhR7JaAhssqtPPnz9fBgwe1atUqDQ0NacuWLbFbAhouq9Du379fS5Ys0Z49e7R7924NDg7Gbglo\nuKxCOzg4qAMHDkiSOjs7NWvWrMgdAY2X1Us+bW1t2rdvnwYGBnT69Glt27YtdktAw2UV2s2bN8du\nAZh0WV0eA62AKR8gUUz5AJlIZspnIhMa0r9TGvVMdlAz+TXNXKvZNbGnfLJ6Igr1GRkZ0aZNmzQ8\nPKzZs2ervb1d586dU19fX+zWUAGXx9CKFSt06dIl7d27Vzt27FBXV5fOnz8fuy1UQWhb3KFDh3Tk\nyBH19vZeObZ27VpNmzYtYlcYD6FtcUePHpWZqVAoXDnW3t6unTt3RuwK4yG0kCRNnTo1dguoEaFt\ncQsWLJAknTlzRhcuXNCGDRvU09OjlStX6tixY5G7QyWEtsUtXbpUPT092rVrlzo6OtTX16fjx49r\nxowZmjt3buz2UAGhhfr7+1UsFrV69Wpt3LhRa9asUWdnZ+y2UMV1b2Pk5gpquLnivzXNurmC2xiB\nTDAwACSKMy2QiWQGBnhbkPprUtyDeuty2wfeFgQAoQW8IbSAM4QWcIbQAs4QWsAZQgs4Q2gBZwgt\n4AyhBZxhYABIFAMDQCYYGKhxnZRrUtyDeuty2wcGBgAQWsAbQgs4Q2gBZwgt4AyhBZwhtIAzhBZw\nhtACzhBawBkGBoBEMTAAZIKBgRrXSbkmxT2oty63fWBgAAChBbwhtIAzhBZwhtACzhBawBlCCzhD\naAFnCC3gDKEFnGFgAEgUAwNAJhgYqHGdlGtS3IN663LbBwYGABBawBtCCzhDaAFnCC3gDKEFnCG0\ngDOEFnCG0ALOEFrAGQYGgEQxMABkgoGBGtdJuSbFPai3Lrd9YGAAAKEFvCG0gDOEFnCG0ALOEFrA\nGUILOENoAWcILeAMoQWcYWAASBQDA0AmGBiocZ2Ua1Lcg3rrctsHBgYAEFrAG0ILOENoAWcILeAM\noQWcIbSAM4QWcIbQAs4QWsAZBgaARDEwAGSCgYEa10m5JsU9qLcut31gYAAAoQW8IbSAM4QWcIbQ\nAs4QWsAZQgs4Q2gBZwgt4AyhBZwhtIAzTPkAiWLKB8iE+ymfFPujprlr5VZTXlcJZ1rAGUILOENo\nAWcILeAMoQWcIbSAM4QWcIbQAs4QWsAZQgs4w8AAkCgGBoBMMDBAzaTUNHOt3GrK6yrhTAs4Q2gB\nZwgt4AyhBZwhtIAzhBZwhtACzhBawBlCCzhDaAFnCC3gDFM+QKKqTfmMG1oA6eHyGHCG0ALOEFrA\nGUILOENoAWf+Aedh8kM4slLLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ecab3ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AyhBZwhtIAzfwMfjfJDb0iqxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef00f5a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgs4Q2gBZ/4GW/vyQ/Ub3/kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef0bdddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADICAYAAAAELGYKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCzhDaAFn/gWUGPJDZp03FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef624f150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "for x in xrange(0,10):\n",
    "  tasks.append(Hallway(goal_loc = (3, 3+x)))\n",
    "  tasks[-1].plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKfA7ifHvO-M"
   },
   "source": [
    "\n",
    "## Implement agents\n",
    "**[10 pts]** In the next code cell, implement an agent that uses **tabular Sarsa** to learn action values.  The agent should act according to an $\\epsilon$-greedy policy with respect to its action values.\n",
    "\n",
    "The agent will be initialized with:\n",
    "```\n",
    "agent = Sarsa(number_of_states=grid._layout.size,\n",
    "              number_of_actions=4,\n",
    "              grid.get_obs())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "u_hLSL8anhsv"
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "#     if double:\n",
    "#       self._q2 = np.zeros((number_of_states, number_of_actions))\n",
    "    self._s = initial_state\n",
    "    self._initial_state = initial_state\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "\n",
    "#     self._double = double\n",
    "    self._last_action = 0\n",
    "  \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "    \n",
    "  def _target_policy(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy(self, q):\n",
    "    return epsilon_greedy(q, 0.1)\n",
    "  \n",
    "  @property\n",
    "  def q_values(self):\n",
    "#     if self._double:\n",
    "#       return (self._q + self._q2)/2\n",
    "#     else:\n",
    "    return self._q\n",
    "\n",
    "  def step(self, r, g, s, train):\n",
    "#     if self._double:\n",
    "#       next_action = self._behaviour_policy(self.q_values[s,:])\n",
    "#       if np.random.random() <= 0.5:\n",
    "#         expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q2[s,:])\n",
    "#         self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "#         #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q2[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "#       else:\n",
    "#         expectation = np.sum(self._target_policy(self._q2[s,:], next_action) * self._q[s,:])\n",
    "#         self._q2[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q2[self._s,self._last_action])   \n",
    "#         #self._q2[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q2[s,:], next_action))] - self._q2[self._s,self._last_action])    \n",
    "#       self._s = s\n",
    "#       self._last_action = next_action\n",
    "#       return self._last_action\n",
    "#     else:\n",
    "    next_action = self._behaviour_policy(self._q[s,:])\n",
    "    \n",
    "    if train == True:\n",
    "      # This is expected sarsa, but still functions as expected.\n",
    "      expectation = np.sum(self._target_policy(self._q[s,:], next_action) * self._q[s,:])\n",
    "      self._q[self._s,self._last_action] += self._step_size*(r + g*expectation - self._q[self._s,self._last_action])\n",
    "      #self._q[self._s,self._last_action] += self._step_size*(r + g*self._q[s,np.argmax(target_policy(self._q[s,:], next_action))] - self._q[self._s,self._last_action])\n",
    "    \n",
    "    self._s = s\n",
    "    self._last_action = next_action\n",
    "    return self._last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMr_z0RZsHNj"
   },
   "source": [
    "**[20 pts]** Implement an agent that uses **neural Sarsa** to learn action values.  The agent should expect a nxn input which it should flatten into a vector, and then pass through a multi-layer perceptron with a single hidden layer with 100 hidden nodes and ReLU activations.  Each weight layer should also have a bias.  Initialize all weights uniformly randomly in $[-0.05, 0.05]$.\n",
    "\n",
    "```\n",
    "NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "            number_of_hidden=100,\n",
    "            number_of_actions=4,\n",
    "            initial_state=grid.get_obs(),\n",
    "            step_size=0.01)\n",
    "```\n",
    "\n",
    "The number `vision_size` will be either 1 or 2 below.  The input vector will be of size $(2v + 1)^2$, which will correspond to a square local view of the grid, centered on the agent, and of size $(2v + 1) \\times (2v + 1)$ (so either 3x3 or 5x5).\n",
    "\n",
    "You are allowed, but not mandated, to use TensorFlow to implement this agent.  (The network is small enough that you can also use numpy, but then you have to implement your own backprop.)  Please document the code clearly, especially on non-trivial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fppAqbUOn8cO"
   },
   "outputs": [],
   "source": [
    "class NeuralSarsa(object):\n",
    "\n",
    "  def __init__(self, number_of_features, number_of_hidden, number_of_actions, initial_state, step_size=0.01):\n",
    "    tf.reset_default_graph()\n",
    "    self._prev_action = 0\n",
    "    self._step = step_size\n",
    "    self._num_features = number_of_features\n",
    "    self._num_action = number_of_actions\n",
    "    self._num_hidden = number_of_hidden\n",
    "    self._initial_state = initial_state\n",
    "    self._s = initial_state\n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    \n",
    "    self.handleTF()\n",
    "  \n",
    "  def resetState(self):\n",
    "    self._s = self._initial_state \n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    \n",
    "  def handleTF(self):\n",
    "    self._sess = tf.Session()\n",
    "    #tf.reset_default_graph()\n",
    "    self.rewTensor = tf.placeholder(tf.float32)\n",
    "    self.disTensor = tf.placeholder(tf.float32)\n",
    "    self.nqTensor = tf.placeholder(tf.float32)\n",
    "    self.actionTensor = tf.placeholder(tf.int32)\n",
    "    self.stateTensor = tf.placeholder(tf.float32, shape=(1,self._num_features))\n",
    "    self._dense_1 = tf.layers.dense(self.stateTensor,\n",
    "                                    self._num_hidden, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._dense_2 = tf.layers.dense(self._dense_1,\n",
    "                                    self._num_action, activation=None,\n",
    "                                    kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05),\n",
    "                                    bias_initializer=tf.random_uniform_initializer(-0.05, 0.05))\n",
    "    self._q = tf.reshape(self._dense_2, (self._num_action,))    \n",
    "    cost = tf.losses.mean_squared_error(self.rewTensor + self.disTensor*self.nqTensor, self._q[self.actionTensor])\n",
    "    self._opt = tf.train.RMSPropOptimizer(self._step).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    self._sess.run(init)\n",
    "\n",
    "  def _target_policy(self, q, a):\n",
    "    return np.eye(len(q))[a]\n",
    " \n",
    "  def _behaviour_policy(self, q):\n",
    "    return epsilon_greedy(q, 0.1)\n",
    "\n",
    "  def q(self, obs):\n",
    "    #print [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    obs = np.reshape(obs,(1,-1))\n",
    "    return self._sess.run(self._q, {self.stateTensor: obs})\n",
    "  \n",
    "  def step(self, r, g, s, train):\n",
    "    # This function should return an action\n",
    "    q_nxtState = np.reshape(self.q(s), (-1,))\n",
    "    next_action = self._behaviour_policy(q_nxtState)\n",
    "    target = self._target_policy(q_nxtState, next_action)\n",
    "    target = np.random.choice(np.where(np.max(target) == target)[0])\n",
    "    \n",
    "    # Optimiser\n",
    "    if train == True:\n",
    "      vob = q_nxtState[target]\n",
    "      self._sess.run(self._opt,{\n",
    "          self.nqTensor: vob,\n",
    "          self.rewTensor: r,\n",
    "          self.disTensor: g,\n",
    "          self.actionTensor: self._prev_action,\n",
    "          self.stateTensor: self._s})\n",
    "    \n",
    "    self._s = np.reshape(self._s, (1,-1))\n",
    "    self._prev_action = next_action\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jZsPzCmDxAh"
   },
   "source": [
    "# Analyse Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQkk8sMxE0N4"
   },
   "source": [
    "### Run the cells below to train the tabular and neural SARSA agents and to generate plots.\n",
    "\n",
    "This trains the agents the Grid problem with an epsilon of 0.1.\n",
    "\n",
    "The plots below will show action values for each of the actions, as well as a state value defined by $v(s) = \\sum_a \\pi(a|s) q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 0: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "  \n",
    "  This agent returns an action between 0 and 'number_of_arms', \n",
    "  uniformly at random. The 'previous_action' argument of 'step'\n",
    "  is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, number_of_arms):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'random'\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return np.ones((self._number_of_arms))/self._number_of_arms\n",
    "  \n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agent 1: REINFORCE agents\n",
    "Implementation of REINFORCE policy-gradient methods\n",
    "\n",
    "The policy should be a softmax on action preferences:\n",
    "$$\\pi(a) = \\frac{\\exp(p(a))}{\\sum_b \\exp(p(b))}\\,.$$\n",
    "\n",
    "The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    " \n",
    "  def __init__(self, number_of_arms, step_size=0.1, baseline=False):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._lr = step_size\n",
    "    self.name = 'reinforce, baseline: {}'.format(baseline)\n",
    "    self._baseline = baseline\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.total_reward = 0;\n",
    "    self.number_rewards = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      self.number_rewards += 1.\n",
    "      self.total_reward += reward\n",
    "      self.action_values[0,previous_action] += reward\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "      self.updatePreferences(previous_action, reward)\n",
    "#    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "#     if unvisited[0].size > 0:\n",
    "#       return unvisited[0][0]\n",
    "#     else:\n",
    "#       return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    return np.random.choice(np.arange(0,self._number_of_arms),p=self.softmax())\n",
    "    \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    self.action_preferences = np.zeros((1,self._number_of_arms))\n",
    "    self.number_rewards = 0.\n",
    "    self.total_reward = 0.\n",
    "  \n",
    "  def updatePreferences(self, previous_action, reward):\n",
    "    if not self._baseline: \n",
    "      self.action_preferences[0,previous_action]+=self._lr*reward*(1-self.softmax()[previous_action])\n",
    "      for i in xrange(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*reward*self.softmax()[i]\n",
    "    else:\n",
    "      self.action_preferences[0,previous_action]+=self._lr*(reward - self.total_reward/self.number_rewards)*(1-self.softmax()[previous_action])\n",
    "      for i in xrange(0,self._number_of_arms):\n",
    "        if i != previous_action:\n",
    "          self.action_preferences[0,i]-=self._lr*(reward - self.total_reward/self.number_rewards)*self.softmax()[i]\n",
    "    \n",
    "  def softmax(self):\n",
    "    q = np.sum(np.exp(self.action_preferences),axis=1)\n",
    "    t = np.exp(self.action_preferences)/q\n",
    "    return t.flatten()\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.softmax()\n",
    "  \n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 2: EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP3(object):\n",
    "\n",
    "  def __init__(self, number_of_arms, gamma):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'exp3 Gamma: ' + str(gamma)\n",
    "    \n",
    "    self.action_values = np.zeros((2,self._number_of_arms))\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.weights = np.ones((1,self._number_of_arms))\n",
    "    \n",
    "    self.time = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      xhat = np.zeros((1, self._number_of_arms))\n",
    "      xhat[0,previous_action] = reward/self.action_values[0,previous_action]\n",
    "      self.weights = self.weights*np.exp(self.gamma*xhat/self._number_of_arms)\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "    self.action_values[0,:] = (1-self.gamma)*(self.weights)/(np.sum(self.weights)) + self.gamma/self._number_of_arms\n",
    "    action = np.random.choice(self._number_of_arms, p=self.action_values[0,:])\n",
    "    self.time += 1.\n",
    "    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "    return unvisited[0][0] if unvisited[0].size > 0 else action\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.action_values[0,:]\n",
    "  \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2, self._number_of_arms))\n",
    "    self.weights = np.ones((1, self._number_of_arms))\n",
    "    self.time = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent 3: EXP3.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP3S(object):\n",
    "\n",
    "  def __init__(self, number_of_arms, gamma, alpha):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = 'exp3s Gamma: ' + str(gamma) + ', Alpha: ' + str(alpha) \n",
    "    \n",
    "    self.action_values = np.zeros((2,number_of_arms))\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.alpha = alpha\n",
    "    self.weights = np.ones((1,number_of_arms))\n",
    "    \n",
    "    self.time = 0.\n",
    "    self.reset()\n",
    "  \n",
    "  def step(self, previous_action, reward):\n",
    "    if previous_action != None:\n",
    "      xhat = np.zeros((1, self._number_of_arms))\n",
    "      xhat[0,previous_action] = reward/self.action_values[0,previous_action]\n",
    "      # Should the added term be using updated weights as we move across arms, or simultaenous updates?\n",
    "      #print 'test'\n",
    "      self.weights = self.weights*np.exp(self.gamma*xhat/self._number_of_arms) + ((np.e*self.alpha)/(self._number_of_arms))*np.sum(self.weights)\n",
    "      self.action_values[1,previous_action] += 1.\n",
    "    self.action_values[0,:] = (1-self.gamma)*(self.weights)/(np.sum(self.weights)) + self.gamma/self._number_of_arms\n",
    "    action = np.random.choice(self._number_of_arms, p=self.action_values[0,:])\n",
    "    self.time += 1.\n",
    "    unvisited = np.where(self.action_values[1,:] == 0.)\n",
    "    return unvisited[0][0] if unvisited[0].size > 0 else action\n",
    "  \n",
    "  def getProbs(self):\n",
    "    return self.action_values[0,:]\n",
    "  \n",
    "  def reset(self):\n",
    "    self.action_values = np.zeros((2, self._number_of_arms))\n",
    "    self.weights = np.ones((1, self._number_of_arms))\n",
    "    self.time = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBandit(object):\n",
    "  \"\"\"An adversarial multi-armed Task bandit.\"\"\"\n",
    "  \n",
    "  \n",
    "  def __init__(self, rl_agent, tasks, number_of_episodes, reward_signal):\n",
    "    self._unscaled_reward_history = []\n",
    "    self._number_of_episodes_per_step = number_of_episodes\n",
    "    self._rl_agent = rl_agent\n",
    "    self._tasks = tasks\n",
    "    self._reward_signal = reward_signal\n",
    "  \n",
    "  def resetEnvRLAgent(self):\n",
    "    #########\n",
    "      # Is this really needed?\n",
    "    for ide,e in enumerate(self._tasks):\n",
    "      self._tasks[ide].resetState()\n",
    "    self._rl_agent.resetState()\n",
    "    #########\n",
    "    \n",
    "  def step(self, action_task_id):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if self._reward_signal == 'PG':\n",
    "      # Prediction Gain\n",
    "      \n",
    "      reward_before, _ = run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, True)\n",
    "      reward_after, _ = run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      \n",
    "      v = reward_after - reward_before\n",
    "    elif self._reward_signal == 'GPG':\n",
    "      # Gradient Prediction Gain\n",
    "      pass\n",
    "    elif self._reward_signal == 'SPG':\n",
    "      # Self Prediction Gain\n",
    "      reward_before, _ = run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, True)\n",
    "      reward_after, _ = run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      \n",
    "      v = reward_after - reward_before\n",
    "    elif self._reward_signal == 'TPG':\n",
    "      # Target Prediction Gain\n",
    "      reward_before, _ = run_episode(self._tasks[-1], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, True)\n",
    "      # On the Last Task\n",
    "      reward_after, _ = run_episode(self._tasks[-1], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      v = reward_after - reward_before\n",
    "    elif self._reward_signal == 'MPG':\n",
    "      # Mean Prediction Gain\n",
    "      # On uniformly selected task\n",
    "      uniform_sampled_task_id = np.random.choice(len(self._tasks))\n",
    "      reward_before, _ = run_episode(self._tasks[uniform_sampled_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      run_episode(self._tasks[action_task_id], self._rl_agent, self._number_of_episodes_per_step, True)\n",
    "      reward_after, _ = run_episode(self._tasks[uniform_sampled_task_id], self._rl_agent, self._number_of_episodes_per_step, False)\n",
    "      v = reward_after - reward_before\n",
    "    elif self._reward_signal == 'VCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'GVCG':\n",
    "      pass\n",
    "    elif self._reward_signal == 'L2G':\n",
    "      pass      \n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    \n",
    "#     print mean_signal\n",
    "    rhat = v/(duration)\n",
    "#     if len(self._unscaled_reward_history) < 2:\n",
    "#       self._unscaled_reward_history.append(rhat)\n",
    "#       return rhat\n",
    "    self._unscaled_reward_history.append(rhat)\n",
    "    temp_history = np.array(sorted(self._unscaled_reward_history))\n",
    "    p_20 = np.percentile(temp_history, 20)\n",
    "    p_80 = np.percentile(temp_history, 80)        \n",
    "    #self._unscaled_reward_history.append(rhat)\n",
    "\n",
    "\n",
    "    if action_task_id < 0 or action_task_id >= len(self._tasks):\n",
    "      raise ValueError('Action {} is out of bounds for a '\n",
    "                       '{}-armed bandit'.format(action_task_id, len(split_train_tasks)))\n",
    "    \n",
    "    r = None\n",
    "    if rhat <= p_20:\n",
    "      r = -1.\n",
    "    elif rhat > p_80:\n",
    "      r = 1.\n",
    "    else:\n",
    "      r = 2.0 * (rhat - p_20)/(p_80 - p_20) - 1.\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algs, tasks, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, repetitions, vision_size, tabular, agent_type, hidden_units, step_size, reward_signal):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  action_dict = {}\n",
    "  prob_dict = {}\n",
    "  entropy_dict = {}\n",
    "  \n",
    "  for alg in algs:\n",
    "    print 'Running:', alg\n",
    "    #bandit = TaskBandit(64, 32, reward_signal, 0.01)\n",
    "    #bandit = TaskBandit(rl_agent, tasks)\n",
    "    reward_dict[alg.name] = []\n",
    "    action_dict[alg.name] = []\n",
    "    prob_dict[alg.name] = []\n",
    "    entropy_dict[alg.name] = []\n",
    "    \n",
    "    accuracies = None\n",
    "    \n",
    "    for _ in range(repetitions):\n",
    "      alg.reset()\n",
    "      \n",
    "      if agent_type != 'neural':\n",
    "        rl_agent = Sarsa(number_of_states=tasks[0]._layout.size,\n",
    "                number_of_actions=4,\n",
    "                initial_state=tasks[0].get_obs())\n",
    "      else:\n",
    "        rl_agent = NeuralSarsa(number_of_features=(2*vision_size + 1)**2,\n",
    "                    number_of_hidden=hidden_units,\n",
    "                    number_of_actions=4,\n",
    "                    initial_state=tasks[0].get_obs(),\n",
    "                    step_size=step_size)\n",
    "      \n",
    "      bandit = TaskBandit(rl_agent, tasks, number_of_episodes_per_step, reward_signal)\n",
    "      \n",
    "      reward_dict[alg.name].append([])\n",
    "      action_dict[alg.name].append([])\n",
    "      prob_dict[alg.name].append([])\n",
    "      entropy_dict[alg.name].append([])\n",
    "      action = None\n",
    "      reward = None\n",
    "      prob = None\n",
    "      entropy = None\n",
    "      \n",
    "      for i in range(number_of_steps_of_selecting_tasks):\n",
    "        try:\n",
    "          action = alg.step(action, reward)\n",
    "          prob = alg.getProbs()\n",
    "          \n",
    "          entropy = -1.0 * np.sum(prob * np.log(prob))\n",
    "          \n",
    "        except:\n",
    "            raise ValueError(\n",
    "              \"The step function of algorithm `{}` failed.\\\n",
    "              Perhaps you have a bug, such as a typo.\\\n",
    "              Or, perhaps your value estimates or policy has diverged.\\\n",
    "              (E.g., internal quantities may have become NaNs.)\\\n",
    "              Try adding print statements to see if you can find a bug.\".format(alg.name))\n",
    "        reward = bandit.step(action)\n",
    "        reward_dict[alg.name][-1].append(reward)\n",
    "        action_dict[alg.name][-1].append(action)\n",
    "        prob_dict[alg.name][-1].append(prob.copy())\n",
    "        entropy_dict[alg.name][-1].append(entropy)\n",
    "      \n",
    "#       if not (accuracies is None):\n",
    "#         accuracies += np.array(bandit.getSPGAccuracyList())\n",
    "#       else:\n",
    "#         accuracies = np.array(bandit.getSPGAccuracyList())\n",
    "      \n",
    "    \n",
    "#     accuracies /= repetitions\n",
    "      \n",
    "#     plt.figure()\n",
    "#     plt.plot(accuracies)\n",
    "#     plt.title('Task Bandit, ' + alg.name + ', Reward Signal: ' + reward_signal)\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Accuracy')\n",
    "  \n",
    "  return reward_dict, action_dict, prob_dict, entropy_dict\n",
    "\n",
    "def train_task_agents(agents, number_of_arms, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, tasks, reward_signal, repetitions=1, vision_size=1, tabular=False, agent_type='norm', hidden_units=100, step_size=0.01):\n",
    "  bandit = None\n",
    "  reward_dict, action_dict, prob_dict, entropy_dict = run_experiment(bandit, agents, tasks, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, repetitions, vision_size, tabular, agent_type, hidden_units, step_size, reward_signal)\n",
    "  \n",
    "  smoothed_rewards = {}\n",
    "  smoothed_actions = {}\n",
    "  smoothed_probs = {}\n",
    "  smoothed_entropies = {}\n",
    "  \n",
    "  agent_set = set()\n",
    "  \n",
    "  for agent, rewards in reward_dict.items():\n",
    "    agent_set.add(agent)\n",
    "    smoothed_rewards[agent] = (np.sum(np.array([np.array(x) for x in rewards]), axis=0)).T\n",
    "      \n",
    "  for agent, probs in prob_dict.items():\n",
    "    smoothed_probs[agent] = (np.sum(np.array([np.array(x) for x in probs]), axis=0)).T\n",
    "\n",
    "  for agent, entropies in entropy_dict.items():\n",
    "    smoothed_entropies[agent] = (np.sum(np.array([np.array(x) for x in entropies]), axis=0)).T\n",
    "  \n",
    "  for agent in agent_set:\n",
    "    smoothed_probs[agent] /= repetitions\n",
    "    smoothed_rewards[agent] /= repetitions    \n",
    "    \n",
    "    plt.figure(figsize=(22,20))\n",
    "    plt.imshow(smoothed_probs[agent])\n",
    "    plt.title(agent + ', Reward Signal: ' + reward_signal)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Task')\n",
    "\n",
    "  \n",
    "  plt.figure(figsize=(12,12))\n",
    "  plt.title('Maximum Likelihood, \\t Reward Signal: ' + reward_signal)\n",
    "  plt.ylabel('Policy Entropy')\n",
    "  plt.xlabel('Time')\n",
    "  for agent in agent_set:\n",
    "    smoothed_entropies[agent] /= repetitions  \n",
    "    plt.plot(smoothed_entropies[agent], label=agent)\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, number_of_episodes, train):\n",
    "    # Mean Reward across all the episodes( aka all steps )\n",
    "    mean_reward = 0.\n",
    "    \n",
    "    # Mean Duration per episode\n",
    "    mean_duration = 0.\n",
    "    \n",
    "    # List of (Total Reward Per Epsiode)/(Duration)\n",
    "    signal_per_episode = np.zeros((1, number_of_episodes))\n",
    "    reward_per_episode = np.zeros((1, number_of_episodes))\n",
    "    duration_per_episode = np.zeros((1, number_of_episodes))\n",
    "    \n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    \n",
    "    episodes_completed = 0\n",
    "    total_reward_per_episode = 0.\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    i = 0.\n",
    "    while episodes_completed != number_of_episodes:\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      total_reward_per_episode += reward\n",
    "      \n",
    "      if discount == 0:\n",
    "        duration = time.time() - start_time\n",
    "        signal_per_episode[0,episodes_completed] = (total_reward_per_episode/duration)\n",
    "        reward_per_episode[0,episodes_completed] = (total_reward_per_episode)\n",
    "        duration_per_episode[0,episodes_completed] = (duration)\n",
    "        \n",
    "        episodes_completed += 1\n",
    "        mean_duration += (duration - mean_duration)/(episodes_completed)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_reward_per_episode = 0.\n",
    "        \n",
    "      action = agent.step(reward, discount, next_state, train)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "      i += 1.\n",
    "    \n",
    "    mean_signal = np.mean(signal_per_episode)\n",
    "    total_reward = np.sum(reward_per_episode)\n",
    "    total_duration = np.sum(duration_per_episode)\n",
    "\n",
    "    return total_reward, total_duration\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=None, vmax=None):\n",
    "  vmin = np.min(values)\n",
    "  vmax = np.max(values)\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(action_values, vmin=None, vmax=None):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "\n",
    "def plot_rewards(xs, rewards, color):\n",
    "  mean = np.mean(rewards, axis=0)\n",
    "  p90 = np.percentile(rewards, 90, axis=0)\n",
    "  p10 = np.percentile(rewards, 10, axis=0)\n",
    "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
    "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
    "  \n",
    "\n",
    "def parameter_study(parameter_values, parameter_name,\n",
    "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
    "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  for rep in range(repetitions):\n",
    "    for i, p in enumerate(parameter_values):\n",
    "      env = env_constructor()\n",
    "      agent = agent_constructor()\n",
    "      if 'eps' in parameter_name:\n",
    "        agent.set_epsilon(p)\n",
    "      elif 'alpha' in parameter_name:\n",
    "        agent._step_size = p\n",
    "      else:\n",
    "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
    "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
    "      agent.set_epsilon(0.)\n",
    "      agent._step_size = 0.\n",
    "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
    "      del env\n",
    "      del agent\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_rewards(parameter_values, mean_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_rewards(parameter_values, greedy_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  # plt.ylim((0, 1.5))\n",
    "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "  \n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: <__main__.Random object at 0x7f1ecaf93590>\n",
      "Running: <__main__.EXP3 object at 0x7f1ecaf93c90>\n",
      "Running: <__main__.EXP3 object at 0x7f1ecaf93a10>\n"
     ]
    }
   ],
   "source": [
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "for x in xrange(0,10):\n",
    "  tasks.append(Hallway(goal_loc = (3, 3+x), goal = (x+1), discount=0.2))\n",
    "\n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "\n",
    "reward_signal = 'SPG'\n",
    "number_of_arms_tasks = len(tasks)\n",
    "number_of_steps_of_selecting_tasks = 350\n",
    "number_of_episodes_per_step = 1\n",
    "reps = 10\n",
    "\n",
    "agents = [\n",
    "      Random(number_of_arms_tasks),\n",
    "#     Greedy(number_of_arms),\n",
    "#     EpsilonGreedy(number_of_arms, 0.1),\n",
    "#     EpsilonGreedy(number_of_arms, 0.01),\n",
    "#     UCB(number_of_arms),\n",
    "      EXP3(number_of_arms_tasks, 0.2),\n",
    "      EXP3(number_of_arms_tasks, 0.7),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.2),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.2),\n",
    "      REINFORCE(number_of_arms_tasks),\n",
    "      REINFORCE(number_of_arms_tasks, baseline=True),\n",
    "]\n",
    "\n",
    "train_task_agents(agents, number_of_arms_tasks, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, tasks, reward_signal, reps)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# # A. Training on the Longest Hallway Only\n",
    "\n",
    "# # B. Manual Curriculum, shortest to longest hallway\n",
    "\n",
    "# # C. Uniform Sampling\n",
    "\n",
    "# # D. Graves Bandit\n",
    "\n",
    "# # E. RL for Curriculum Selection\n",
    "\n",
    "\n",
    "\n",
    "# run_experiment(grid, agent, int(1e5))\n",
    "# q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "# plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: <__main__.Random object at 0x7f1ef13a31d0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-4a6d105b47db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                   \u001b[0magent_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                   \u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                   step_size)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#h, w = grid._layout.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-262-185ff0bc1624>\u001b[0m in \u001b[0;36mtrain_task_agents\u001b[0;34m(agents, number_of_arms, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, tasks, reward_signal, repetitions, vision_size, tabular, agent_type, hidden_units, step_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_task_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_arms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_of_selecting_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_episodes_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvision_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0mreward_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_of_selecting_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_episodes_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvision_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0msmoothed_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-262-185ff0bc1624>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(bandit, algs, tasks, number_of_steps_of_selecting_tasks, number_of_episodes_per_step, repetitions, vision_size, tabular, agent_type, hidden_units, step_size, reward_signal)\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mquantities\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mbecome\u001b[0m \u001b[0mNaNs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m               Try adding print statements to see if you can find a bug.\".format(alg.name))\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mreward_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0maction_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-261-1102e206c64a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_task_id)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward_signal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SPG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# Self Prediction Gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mtotal_reward_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_task_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rl_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_of_episodes_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_task_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rl_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_of_episodes_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mtotal_reward_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_task_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rl_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_of_episodes_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-263-8fb392c8d285>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent, number_of_episodes, train)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtotal_reward_per_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mmean_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-256-7a1ea1cfe97d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, r, g, s, train)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# This function should return an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mq_nxtState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behaviour_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_nxtState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_nxtState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-256-7a1ea1cfe97d>\u001b[0m in \u001b[0;36mq\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#print [n.name for n in tf.get_default_graph().as_graph_def().node]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vision_size = 1\n",
    "tabular_grid = False\n",
    "agent_type = 'neural'\n",
    "hidden_units = 100\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "for x in xrange(0,10):\n",
    "  tasks.append(Hallway(goal_loc = (3, 3+x), goal = (x+1), tabular=tabular_grid, vision_size=vision_size))\n",
    "\n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "reward_signal='SPG'\n",
    "number_of_arms_tasks = len(tasks)\n",
    "number_of_steps_of_selecting_tasks = 350\n",
    "number_of_episodes_per_step = 2\n",
    "reps = 10\n",
    "\n",
    "agents = [\n",
    "      Random(number_of_arms_tasks),\n",
    "#     Greedy(number_of_arms),\n",
    "#     EpsilonGreedy(number_of_arms, 0.1),\n",
    "#     EpsilonGreedy(number_of_arms, 0.01),\n",
    "#     UCB(number_of_arms),\n",
    "      EXP3(number_of_arms_tasks, 0.2),\n",
    "      EXP3(number_of_arms_tasks, 0.7),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.2),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.2),\n",
    "      REINFORCE(number_of_arms_tasks),\n",
    "      REINFORCE(number_of_arms_tasks, baseline=True),\n",
    "]\n",
    "\n",
    "train_task_agents(agents,\n",
    "                  number_of_arms_tasks,\n",
    "                  number_of_steps_of_selecting_tasks,\n",
    "                  number_of_episodes_per_step,\n",
    "                  tasks,\n",
    "                  reward_signal,\n",
    "                  reps,\n",
    "                  vision_size,\n",
    "                  tabular_grid,\n",
    "                  agent_type,\n",
    "                  hidden_units,\n",
    "                  step_size)\n",
    "\n",
    "#h, w = grid._layout.shape\n",
    "#obs = np.array([[grid.get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "#qs = np.array([[[agent.q(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "#plot_action_values(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_size = 2\n",
    "tabular_grid = False\n",
    "agent_type = 'neural'\n",
    "hidden_units = 100\n",
    "step_size = 0.01\n",
    "\n",
    "# goal_loc has format (row, col)\n",
    "tasks = []\n",
    "for x in xrange(0,10):\n",
    "  tasks.append(Hallway(goal_loc = (3, 3+x), goal = (x+1), tabular=tabular_grid, vision_size=vision_size))\n",
    "\n",
    "# Intrinsically Motivated Curriculum Learning\n",
    "reward_signal='SPG'\n",
    "number_of_arms_tasks = len(tasks)\n",
    "number_of_steps_of_selecting_tasks = 350\n",
    "number_of_episodes_per_step = 2\n",
    "reps = 10\n",
    "\n",
    "agents = [\n",
    "      Random(number_of_arms_tasks),\n",
    "#     Greedy(number_of_arms),\n",
    "#     EpsilonGreedy(number_of_arms, 0.1),\n",
    "#     EpsilonGreedy(number_of_arms, 0.01),\n",
    "#     UCB(number_of_arms),\n",
    "      EXP3(number_of_arms_tasks, 0.2),\n",
    "      EXP3(number_of_arms_tasks, 0.7),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.2, 0.2),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.01),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.1),\n",
    "#     EXP3S(number_of_arms_tasks, 0.7, 0.2),\n",
    "      REINFORCE(number_of_arms_tasks),\n",
    "      REINFORCE(number_of_arms_tasks, baseline=True),\n",
    "]\n",
    "\n",
    "train_task_agents(agents,\n",
    "                  number_of_arms_tasks,\n",
    "                  number_of_steps_of_selecting_tasks,\n",
    "                  number_of_episodes_per_step,\n",
    "                  tasks,\n",
    "                  reward_signal,\n",
    "                  reps,\n",
    "                  vision_size,\n",
    "                  tabular_grid,\n",
    "                  agent_type,\n",
    "                  hidden_units,\n",
    "                  step_size)\n",
    "\n",
    "#h, w = grid._layout.shape\n",
    "#obs = np.array([[grid.get_obs_at(x, y) for x in range(2, w-2)] for y in range(2, h-2)])\n",
    "#qs = np.array([[[agent.q(o)[a] for a in range(4)] if o[vision_size,vision_size] == 0 else np.zeros((4,)) for o in ob] for ob in obs])\n",
    "#plot_action_values(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "17045589_RL_hw4.ipynb",
   "provenance": [
    {
     "file_id": "1a1ONHRz5bcd2rJyLUD53OUMR8npSp0QZ",
     "timestamp": 1522325021849
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1521476023411
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1517660129183
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1517174839485
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1515086437469
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1511098107887
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
